---
title: "R Notebook"
output: html_notebook
---
Reasons for the media sources chosen:
News 24 : Recognized by APP Annie (App Annie is the standard in app analytics and app market data) as the most known South African internet media source. (Chinese Academy of Cyberspace Studiespublisher=Springer (15 September 2018). World Internet Development Report 2017: Translated by Peng Ping. p. 203. ISBN 9783662575246. OCLC 1052766508.)

Times Live : Claim to be South Africa's second-biggest news website, published by Arena Holdings (Times Live website). No evidence found to disprove this claim. In top 10 of most visited publication websites for South Africa.

EWN : Not much info but it can be considered to be popular because it originated from 2 radio stations (KFM and CapeTalk), Makes it a popular and people hear about it without actively having to go and search for the news agency themselves. In top 10 of most visited publication websites for South Africa.

eNCA: Not much info but seems popular. Used to E News from TV channel. Might not be as trustworthy as there has been complaints about racism recently. In top 10 of most visited publication websites for South Africa.

SABC: National news company with government ties. Reaches a wide variety of viewers in different languages. The company is both state owned and a public broadcaster company.https://www.businesslive.co.za/bd/national/media/2015-05-20-sabc-both-state-owned-company-and-public-broadcaster-minister-says/


Chapter 10 of the Constitution of the Republic of South Africa, 1996 makes provision for good governance of news media and what they publish.

The other 4 of the 5 is ranked among the top 10 most visited publication sites websites in south africa as found by 
https://mybroadband.co.za/news/internet/358453-here-are-south-africas-most-popular-news-websites.html.
These sites are the first sites on the top 10 list of publication sites that include general news along with news about covid-19 that do not have major historical issues that could bring their credibility in question.
Reasons that other news or publishing sources that are ranked higher were not used is due to the fact that these sources either only focus on specific news like new technology or financial aspects, or these sources are not as popular throughout the whole of South Africa.

Importing Packages
```{r}
install.packages("tidytext")
install.packages("tidyverse")
install.packages("sentimentr")
install.packages("textdata")
install.packages("stringi")
install.packages("rtweet")
install.packages("quanteda")
install.packages("topicmodels")
install.packages("reshape2")
```

Importing Libraries
```{r}
library(tidytext)
library(tidyverse)
library(textdata)
library(sentimentr)
library(stringr)
library(stringi)
library(rtweet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(scales)
library(readr)
library(lubridate)
library(topicmodels)
library(quanteda)
library(reshape2)
library(zoo)
```
Importing data
```{r}
media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
media_agency_df <- media_agency_df[ , colSums(is.na(media_agency_df)) < nrow(media_agency_df)]
media_agency_df <- media_agency_df %>% mutate("X1" = row_number())
```

Wrangeling and cleaning data
```{r}
#Cleaning data
media_agency_df <- media_agency_df %>%
  mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text),
         text = gsub("&amp", "", text),
         text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text),
         text = gsub("@\\w+", "", text),
         text = gsub("[[:punct:]]", "", text),
         text = gsub("[[:digit:]]", "", text),
         text = gsub("http\\w+", "", text),
         text = gsub("[ \t]{2,}", "", text),
         text = gsub("^\\s+|\\s+$", "", text),
         text = gsub("&amp", "", text)) %>%
  mutate(text = str_replace_all(text," "," "),
         text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
         text = str_replace_all(text,"#[a-z,A-Z]*"," "),
         text = str_replace_all(text,"@[a-z,A-Z]*"," "))


#tidying data
tidy_covid_media_df <- media_agency_df %>%
  #filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text)) %>%
  unnest_tokens(word, text, token = "tweets") %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"),
         !(word =="#sabcnews" |
             word =="#enca" |
             word =="https" |
             word =="#dstv403" |
             word =="t.co" |
             word =="rt" |
             word =="amp"))

```
```{r}
#finding the screen names of media angencies
agencies<- media_agency_df %>%
  select(screen_name) 
agencies %>%
  distinct(screen_name)
```

Seniment Analysis with SentimentR
```{r}

mytext<- media_agency_df 

mytext <- get_sentences(mytext$text)
sr_data <-sentiment(mytext)
#write_csv(sr_data,"data_in/sentimentr_data.csv")
```
Adding time data to sentiment df
```{r}
time_data <- media_agency_df %>%
  select(X1,created_at,screen_name)

SentiR_data <-left_join(time_data,sr_data,
                        by=c("X1"="element_id"))
#write_csv(SentiR_data,"data_in/sr_df_time.csv")

```

Modeling the time data
```{r}
#setting the date format
#NB only run date format once
SentiR_data$created_at = ymd_hms(SentiR_data$created_at)
#filtering by news agency
news24<- SentiR_data %>%
  filter(screen_name=="News24")

times_live<-SentiR_data %>%
  filter(screen_name=="TimesLIVE")

dailymaverick<-SentiR_data %>%
  filter(screen_name=="dailymaverick")

sabc<-SentiR_data %>%
  filter(screen_name=="SABCNews")

eNCA<-SentiR_data %>%
  filter(screen_name=="eNCA")
```
Graphs
```{r}
#graph for News24 sentiment over time
ggplot(data=news24, aes(x=created_at, y=rollmean(sentiment, 30,  na.pad=TRUE))) +
       geom_line(color="pink", size=.5)+
  geom_smooth() +
  theme_minimal()+
  facet_wrap(~screen_name) +
  scale_y_continuous(expand = c(0,0), breaks = c(-0.4,-0.2,0,0.2,0.4)) +
  ylim(c(-0.4, 0.4))

```
```{r}
#graph for Times Live sentiment over time
ggplot(data=times_live, aes(x=created_at, y=rollmean(sentiment, 30,  na.pad=TRUE))) +
       geom_line(color="pink", size=.5)+
  geom_smooth() +
  theme_minimal()+
  facet_wrap(~screen_name) +
  scale_y_continuous(expand = c(0,0), breaks = c(-0.4,-0.2,0,0.2,0.4)) +
  ylim(c(-0.4, 0.4))


```
```{r}
#graph for EWN sentiment over time
ggplot(data=dailymaverick, aes(x=created_at, y=rollmean(sentiment, 30,  na.pad=TRUE))) +
       geom_line(color="pink", size=.5)+
  geom_smooth() +
  theme_minimal()+
  facet_wrap(~screen_name) +
  scale_y_continuous(expand = c(0,0), breaks = c(-0.4,-0.2,0,0.2,0.4)) +
  ylim(c(-0.4, 0.4))


```
```{r}
#graph for eNCA sentiment over time
ggplot(data=eNCA, aes(x=created_at, y=rollmean(sentiment, 30,  na.pad=TRUE))) +
       geom_line(color="pink", size=.5)+
  geom_smooth() +
  theme_minimal()+
  facet_wrap(~screen_name) +
  scale_y_continuous(expand = c(0,0), breaks = c(-0.4,-0.2,0,0.2,0.4)) +
  ylim(c(-0.4, 0.4))


```
```{r}
#graph for SABC sentiment over time
ggplot(data=sabc, aes(x=created_at, y=rollmean(sentiment, 30,  na.pad=TRUE))) +
       geom_line(color="pink", size=.5)+
  geom_smooth() +
  theme_minimal()+
  facet_wrap(~screen_name) +
  scale_y_continuous(expand = c(0,0), breaks = c(-0.4,-0.2,0,0.2,0.4)) +
  ylim(c(-0.4, 0.4))


```

#Kyles Code
Topic Modelling
```{r}

tidy_matrix <- tidy_covid_media_df %>% count(screen_name, word) %>% cast_dfm(screen_name, word, n)

 
media_lda <- LDA(tidy_matrix, k = 4, control = list(seed = 1234))
media_lda
media_topics <- tidy(media_lda, matrix = "beta")
media_topics
media_top_terms <- media_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
 

media_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
```
Vader stuff
```{r}
vader_data<- read_csv("data_in/vader_df.csv")

covid_dictionary <- c("herd", "immunity", "incubation", "job", "loss", "Kits", "lockdown", "mask", "N95", "outbreak", "pandemic", "quarantine", "recovery", "sanitiser", "transmission", "Underlying", "conditions", "Ventilators", "WHO", "xenophobia", "youTube", "zoonotic", "stay-at-home", "covid", "coronavirus", "hyrdoxychloroquine", "asymptomatic", "frontline", "virus", "self-isolation", "disinfectant", "shelter-in-place", "masks", "SARS-CoV-2", "ICU", "corona", "reopen", "distancing", "covering", "furlough", "tracer", "easing", "remdesivir", "mail-in", "hornet", "antibody", "in-person", "defund", "racism", "looting", "loot", "reopen", "two-metre", "pandemic", "looter", "distancing", "dexamethasone", "racial", "vaccine",
                      "curfew","johnssons","astrazeneca","hospitals","social-distance",
                      "social-distancing","police","regulations","symptoms","testing",
                      "positive-tests","negative-tests","confirmed-cases","restrictions",
                      "deaths","infected","recoveries","level","jobs","unemployed",
                      "doctors","infections","sanitise","sanitiser","sanitisation",
                      "containment")

```



```{r}
install.packages("XML")
install.packages("RCurl")
install.packages("RColorBrewer")
install.packages("wordcloud")

library(XML)
library(RCurl)
library(RColorBrewer)
library(wordcloud)
```
Setting up Reddit scraper

Kyle's Reddit code that he graciously gave to me :)
I attempted 2 other reddit web scraping methods for scraping comments from post but they were unsuccessful. We settled instead to just focus on the posts themselves about Covid-19 in South Africa.
By scraping the r/southafrica subreddit we managed to find about 250 posts dating back to last year December which would be from when South Africa was entering harder lockdown times again. We expect that due to the fact that this is the post themselves and not the comment sof people that the sentiment would be close to neutral or will have tendencies to return to neutral if it deviates from it. We will be making use of a 7 day rolling average  to see the sentiment of the reddit posts as well a smooth graph to see the tendency of the posts sentiment over time.
```{r}
library(RedditExtractoR)

reddit_Covid_data<-RedditExtractoR::reddit_urls(search_terms = "Covid-19", subreddit = "southafrica",page_threshold = 10)
```
Cleaning Reddit stuff (Made use of Kyle's twitter cleaner)
```{r}
reddit_Covid_data <- reddit_Covid_data %>%
  mutate(title = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", "", title ),
         title  = gsub("&amp", "", title ),
         title  = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", title ),
         title = gsub("@\\w+", "", title ),
         title  = gsub("[[:punct:]]", "", title ),
         title  = gsub("[[:digit:]]", "", title ),
         title  = gsub("http\\w+", "", title ),
         title  = gsub("[ \t]{2,}", "", title ),
         title  = gsub("^\\s+|\\s+$", "", title ),
         title  = gsub("&amp", "", title )) %>%
  mutate(title  = str_replace_all(title ," "," "),
         title  = str_replace_all(title ,"RT @[a-z,A-Z]*: "," "),
         title  = str_replace_all(title ,"#[a-z,A-Z]*"," "),
         title  = str_replace_all(title ,"@[a-z,A-Z]*"," "))
write_csv(reddit_Covid_data,"data_in/reddit_SA.csv")

reddit_Covid_data$date= dmy(reddit_Covid_data$date)
reddit_Covid_data<- reddit_Covid_data %>% mutate("X1"=row_number())
```
Sentiment of reddit
```{r}
sd<- get_sentences(reddit_Covid_data$title)
reddit_sentiment<- sentiment(sd)

reddit_sentiment<- left_join(reddit_Covid_data,reddit_sentiment,
                             by=c("X1"="element_id"))


```

Modeling Reddit sentiment over time
```{r}
ggplot(data=reddit_sentiment, aes(x=date, y=rollmean(sentiment, 30,  na.pad=TRUE))) +
       geom_line(color="pink", size=.5)+
  geom_smooth() +
  theme_minimal()+
  scale_y_continuous(expand = c(0,0), breaks = c(-0.4,-0.2,0,0.2,0.4)) +
  ylim(c(-0.4, 0.4))+
  labs(title = "Sentiment from the titles of Reddit post about Covid in SA", x="Date", 
       y="Sentiment")
```
As can be seen from the graph the sentiment of the posts started out more negative and that would correlated to be around to higher infections per day. The sentiment does have a positive trend towards the end of the wave of infections. Even though there is very slight fluctuations the posts seem mostly neutral.

---
title: "R Notebook"
output: html_notebook
---

What tweets got the most attention.
frequency of tweets posted on a topic


```{r SETUP}
library(rtweet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(tidytext)
library(stringr)
library(scales)
library(readr)
library(lubridate)
library(vader)
library(topicmodels)
library(quanteda)
library(lubridate)
library(zoo)
library(plotly)
library(forcats)
library(igraph)
library(ggraph)
library(widyr)
library(qdapRegex)
library(magick)
```

```{r Fetching Tweets}

comments<- get_reddit(search_terms = "Covid-19", subreddit = "southafrica", page_threshold = 5,
                      sort_by = "new")
write_csv(comments, "data_in/reddit_comments.csv")
```


# Comparing Tweet Archives

Some tweets fetched date further back. The 3200 tweet pull per user causes this. It tells us that agencies like News24 and SABC News Tweet more daily than Daily Maverick.
```{r}
ggplot(media_agency_df, aes(x = created_at, fill = screen_name)) +
  geom_histogram(position = "identity",
                 bins = 20,
                 show.legend = FALSE) +
  facet_wrap( ~ screen_name, ncol = 1)
```

We first clean the original text to remove links, punctuation, digits, links, @'s.
Then we tokenize the Tweets and remove stop words from tidytext and our own stopword dictionary. We also use the "twitter" token to handle any left over @'s and URLS.
```{r Clean up, include=FALSE}
#created if more tweets could be accessed to be more topic specific
# covid_dictionary <- c("herd", "immunity", "incubation", "job", "loss", "Kits", "lockdown", "mask", "N95", "outbreak", "pandemic", "quarantine", "recovery", "sanitiser", "transmission", "Underlying", "conditions", "Ventilators", "WHO", "xenophobia", "youTube", "zoonotic", "stay-at-home", "covid", "coronavirus", "hyrdoxychloroquine", "asymptomatic", "frontline", "virus", "self-isolation", "disinfectant", "shelter-in-place", "masks", "SARS-CoV-2", "ICU", "corona", "reopen", "distancing", "covering", "furlough", "tracer", "easing", "remdesivir", "mail-in", "hornet", "antibody", "in-person", "defund", "racism", "looting", "loot", "reopen", "two-metre", "pandemic", "looter", "distancing", "dexamethasone", "racial", "vaccine", "curfew","johnssons", "astrazeneca", "hospitals", "social-distance", "social-distancing", "police", "regulations", "symptoms", "testing", "positive-tests", "negative-tests" , "confirmed-cases", "restrictions", "deaths", "infected", "recoveries", "level", "jobs", "unemployed", "doctors", "infections", "sanitise", "sanitiser", "sanitisation", "containment")

media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
media_agency_df <-
  media_agency_df[, colSums(is.na(media_agency_df)) < nrow(media_agency_df)]

#JUST tidy
rm_twitter_n_url <-
  rm_(pattern = pastex("@rm_twitter_url", "@rm_url"))
media_agency_df <- media_agency_df %>%
  mutate(
    text = rm_twitter_n_url(text),
    text = gsub("@\\w+", " ", text),
    text = gsub("[[:punct:]]", " ", text),
    text = gsub("[[:digit:]]", " ", text),
    text = gsub("http\\w+", " ", text),
    text = gsub("[ |\t]{2,}", " ", text),
    text = gsub("^ ", " ", text),
    text = gsub(" $", " ", text),
    text = gsub("^BUSINESS", " ", text),
    text = gsub("^World", " ", text)
  ) %>%
  mutate(
    text = str_replace_all(text, " ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " "),
    text = str_replace_all(text, " ", " "),
    text = str_replace_all(text, "RT @[a-z,A-Z]*: ", " "),
    text = str_replace_all(text, "#[a-z,A-Z]*", " "),
    text = str_replace_all(text, "@[a-z,A-Z]*", " ")
  )

#most stop words are filtered based on the media agencies tag at the beginning of each Tweet. eg. WATCH: *headline follows*.
agency_stop_words <-
  tibble(
    word = c(
      "sabcnews",
      "enca",
      "dstv",
      "sabckzn",
      "maverick",
      "opinionista",
      "dm",
      "scorpio",
      "dstv403",
      "itus",
      "rt",
      "amp",
      "tgifood",
      "mamelodi",
      "sundowns",
      "ofmagazineavailable",
      "casablanca",
      "oped",
      "newsdeck",
      "editorial",
      "newflash",
      "southafricanmorning",
      "newslink",
      "encas",
      "southafricatonight",
      "themiddayview",
      "thelead",
      "propertymatters",
      "ba",
      "ka",
      "ya",
      "ga",
      "wa",
      "le",
      "kwa",
      "morninglivesabc",
      "monday",
      "prix",
      "azerbaijan",
      "encasis",
      "encabusiness",
      "encasspeaks",
      "south",
      "africa",
      "pm",
      "sa",
      "pm",
      "encas",
      "iss",
      "icymi",
      "timeslive",
      "fullview",
      "newsbreaksjul",
      "newsbreakjul",
      "sabc",
      "nca",
      "ncas",
      "op",
      "ig",
      "ed",
      "pl",
      "news24",
      "news24s",
      "dm168",
      "siness",
      "usiness",
      "ewsdeck",
      "orld",
      "sunday",
      "friday",
      "saturday",
      "thursday",
      "wednesday",
      "monday",
      "tuesday",
      "pics",
      "live"
    )
  )

#tidy df and unnest
tidy_media_df <- media_agency_df %>%
  #filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
  unnest_tokens(word, text, token = "tweets") %>%
  filter(
    !word %in% stop_words$word,!word %in% agency_stop_words$word, 
    !word %in% negated_words$word2, 
    !word %in% str_remove_all(stop_words$word, "'"),
    str_detect(word, "[a-z]")
  )

```

From our tidied Tweet dataset, we look for the top words that appear. This will give us a good idea of what topics are being discussed the most. We find that COVID-19 has been the main topic of discussion. President appears second as President Ramaphosa of South Africa usually addresses the nation regarding COVID-19 information. Additionally, Zuma also appears as he is mentioned as "former president Zuma" in most articles. Zuma appears more as his recent court avoidance and sentencing is being Tweeted. General words surrounding the COVID-19 topic as it is still the main pressure on the country, especially involving Gauteng's rise in infections. 
```{r top words}
#top words
top_words <- tidy_media_df %>%
  anti_join(stop_words) %>%
  count(word) %>%
  arrange(desc(n))
top_words %>%
  slice(1:20) %>%
  ggplot(aes(reorder(word,-n), n, fill = word)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(
      angle = 60,
      hjust = 1,
      size = 13
    ),
    plot.title = element_text(hjust = 0.5, size = 18)
  ) +
  ylab("Frequency") +
  xlab ("") +
  ggtitle("Most frequent media agency tweets") +
  guides(fill = FALSE)
```

After changing, adding, and removing terms from the filters, we can conclude that the tf-idf anaylsis is not useful for these media agencies. The selected media agencies post tables of covid data with different column names, or use unique names to categorize their Tweets. When tf-idf runs, it increases their term weights as they are "unique" because they are not found among other media agencies. These words are usually not telling. What it can tell us - which segments these media agencies post about the most.
```{r tf-idf frequency per agency}
#tf-idf
tidy_media_tf_idf <- tidy_media_df %>%
  select(screen_name, word) %>%
  group_by(screen_name) %>%
  count(word, screen_name) %>%
  bind_tf_idf(word, screen_name, n)

#calculate a frequency for each agency and word
frequency <- tidy_media_tf_idf %>%
  group_by(screen_name) %>%
  count(word, sort = TRUE) %>%
  left_join(tidy_media_df %>%
              group_by(screen_name) %>%
              summarise(total = n())) %>%
  mutate(freq = n / total)

tidy_media_tf_idf %>%
  group_by(screen_name) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ screen_name, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

By modeling bigrams and trigrams for our dataset gives us a better understanding of what topic is being discussed with each word. We can then also see which sentiments are incorrectly labeled. "not good" gives better context of a negative sentiment, rather than it being incorrectly identified as positive good.
```{r bigrams}
#bigrams
tidy_bigram_df <- media_agency_df %>%
  #filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- tidy_bigram_df %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

#tf-idf bigrams
bigram_tf_idf <- bigrams_united %>%
  count(screen_name, bigram) %>%
  bind_tf_idf(bigram, screen_name, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf %>%
  group_by(screen_name) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ screen_name, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

#weights and graphs
bigram_graph <- bigram_counts %>%
  filter(n > 100) %>%
  graph_from_data_frame()

set.seed(1234)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(
    aes(edge_alpha = n),
    show.legend = FALSE,
    arrow = a,
    end_cap = circle(.07, 'inches')
  ) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name),
                 vjust = 1,
                 hjust = 1,
                 repel = TRUE) +
  theme_void()
```

We give more weight to words that appear more often with the incorrect sentiment. We can now reverse the sentiment of these words once VADER has been run on the dataset.
```{r negation words}
#vader lexicon imported from VADER GitHub
vader_lexicon <- read_csv2("data_in/vader_lexicon.csv") %>%
  rename("word" = TOKEN, "value" = `MEAN-SENTIMENT-RATING`)

#common negation words
negation_words <-
  c(
    "not",
    "no",
    "never",
    "without",
    "no",
    "not",
    "none",
    "no one",
    "nobody",
    "nothing",
    "neither",
    "nowhere",
    "never",
    "doesn’t",
    "isn’t",
    "wasn’t",
    "shouldn’t",
    "wouldn’t",
    "couldn’t",
    "won’t",
    "can’t",
    "don’t"
  )
negated_words <- tidy_bigram_df %>%
  filter(word1 %in% fixed(negation_words, ignore_case = TRUE)) %>%
  inner_join(vader_lexicon, by = c(word2 = "word")) %>%
  mutate(value = as.double(value)) %>%
  count(word1, word2, value, sort = TRUE) %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  mutate(word2 = reorder(word2, contribution))
negated_words %>%
  head(40) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ word1, scales = "free_y") +
  labs(x = "Sentiment value * # number of occurrences",
       y = "Words preceded by negation terms")

```

Trigrams were modeled but were not necessary as bigrams provided enough information
```{r trigrams}
#trigrams
tidy_trigram_df <- media_agency_df %>%
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

trigrams_filtered <- trigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word)

# new trigrams counts:
trigram_counts <- trigrams_filtered %>%
  count(word1, word2, word3, sort = TRUE)

trigrams_united <- trigrams_filtered %>%
  unite(trigram, word1, word2, word3, sep = " ")

#tf-idf trigrams
tidy_trigram_df <- trigrams_united %>%
  count(screen_name, trigram) %>%
  bind_tf_idf(trigram, screen_name, n) %>%
  arrange(desc(tf_idf))

tidy_trigram_df %>%
  group_by(screen_name) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(trigram, tf_idf), fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ screen_name, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

```

The graph below shows which words are most likely to co-occur. We use this to understand the context of words in our topic modelling.
```{r Counting and correlating among sections, warning=FALSE}
media_agency_section_words <- tidy_media_df %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0)

# count words co-occuring within sections
word_pairs <- media_agency_section_words %>%
  pairwise_count(word, section, sort = TRUE)

# we need to filter for at least relatively common words first
word_cors <- media_agency_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

# Correlation of next word
# word_cors %>%
#   filter(item1 == "vaccine")

word_cors %>%
  filter(item1 %in% c("covid", "vaccine", "lockdown", "zuma", "gauteng", "ramaphosa")) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap( ~ item1, scales = "free") +
  coord_flip()

set.seed(1234)
word_cors %>%
  filter(correlation > .65) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation),
                 show.legend = FALSE,
                 edge_width = 3) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

The general sentiment is mostly negative. News articles use negative headlines to get a faster reaction from people when skimming through news.
```{r VADER}
# vader_df <- vader_df(media_agency_df$text)
# write_as_csv(vader_df, "data_in/vader")

vader_df <- read_csv("data_in/vader.csv")
vader_df <- vader_df %>% mutate("X1" = row_number())
media_agency_df <- media_agency_df %>% mutate("X1" = row_number())
media_vader_df <- media_agency_df %>% left_join(vader_df, by = "X1")

#General sentiment over time. Media agencies seem to have an even number of positive and negative tweets per day.
ggplot(
  media_vader_df %>%
    mutate(created_at = as.Date(created_at)) %>%
    group_by(created_at, screen_name) %>%
    mutate(compound = replace(compound, is.na(compound), 0)) %>%
    summarise(sum_compound = sum(as.double(compound))),
  aes(created_at, sum_compound, fill = screen_name)
) +
  geom_bar(stat = "identity") +
  geom_smooth(method = "lm", na.rm = TRUE, se = FALSE) +
  facet_wrap( ~ screen_name, scales = "free_x", ncol = 2) +
  theme_minimal() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_date(date_breaks = "3 day", date_labels = "%m-%d") +
  theme(
    axis.text.x = element_text(
      angle = 90,
      vjust = 0.5,
      hjust = 1
    ),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
  )
# An illegal gold miner who was severely injured in a clash in which four other illegal miners were killed has been charged for their murders Mpumalanga police said on Thursday
# Italy midfielder Jorginho said team glory is more important than any push to win a surprise Ballon d’Or award for the best player in world football after a teammate said he deserved it

ggplot(
  media_vader_df %>%
    mutate(created_at = as.Date(created_at)) %>%
    group_by(created_at) %>%
    filter(screen_name == "eNCA") %>%
    summarise(compound),
  aes(created_at, compound)
) +
  geom_bar(stat = "identity") +
  geom_smooth(method = "lm", na.rm = TRUE) +
  theme_minimal() +
  scale_y_continuous(expand = c(0, 0))

#News sentiment
media_vader_df %>%
  mutate(created_at = as.Date(created_at)) %>%
  group_by(created_at, screen_name) %>%
  summarise(compound) %>%
  arrange(desc(compound))

ggplot(media_vader_df %>% arrange(compound) %>% mutate(X1 = factor(X1, levels = X1)),
       aes(x = X1)) +
  geom_point(mapping =  aes(y = compound)) +
  theme_minimal() +
  scale_y_continuous(expand = c(0, 0))

media_vader_df %>%
  mutate(created_at = as.Date(created_at)) %>%
  group_by(created_at, screen_name) %>%
  mutate(compound = replace(compound, is.na(compound), 0)) %>%
  summarise(sum_compound = sum(as.double(compound)))
```

The plot below maps the average total interactions by day. From the relevant peaks, we extract the top tweet in that day. 
```{r Interactions, echo=FALSE}
# media_vader_df %>% select(favorite_count, retweet_count) %>% mutate(favorite_count + retweet_count)

media_vader_df <- media_vader_df %>%
  mutate(created_at = as.Date(created_at)) %>%
  group_by(created_at) %>%
  mutate(
    retweet_daily = mean(retweet_count),
    favorite_daily = mean(favorite_count),
    total = mean(favorite_count + retweet_count)
  )
# mean for the day
interactions_plot <- ggplotly(
  ggplot(data = media_vader_df, aes(created_at)) +
    geom_ribbon(aes(
      ymin = 0, ymax = total, fill = "Total"
    )) +
    geom_ribbon(aes(
      ymin = 0, ymax = favorite_daily, fill = "Favorites"
    )) +
    geom_ribbon(aes(
      ymin = 0, ymax = retweet_daily, fill = "Retweets"
    )) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      axis.ticks = element_blank(),
      
      legend.justification = c(0, 0),
      legend.position = c(0, 0),
      legend.background = element_blank(),
      legend.key = element_blank(),
      legend.title = element_blank(),
      
      plot.title = element_text(
        size = 14,
        face = "bold",
        margin = margin(0, 0, 10, 0),
        hjust = 0
      ),
      plot.caption = element_text(face = "bold", hjust = 0),
    )
  ,
  tooltip = c("total", "favorite_daily", "retweet_daily", "created_at")
)

# peak tweets by day
# media_vader_df %>%
#   filter(created_at == "2021-05-01" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-05-23" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-05-29" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-02" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-06" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-26" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-07-03" ) %>%
#   slice_max(favorite_count + retweet_count)

# media_vader_df %>%
#   group_by(screen_name) %>%
#   slice_max(favorite_count + retweet_count)

# peak tweets by user
# @eNCA: Call for ministers over 60 to resign.
# @dailymaverick: Floyd Shivambu’s brother quietly pays back Rm admits he received the VBS money gratuitously.
# @TimesLIVE:	Do you approve of Duduzane running for president?
# @News24: Coca-Cola lost $4 billion in market value after Cristiano Ronaldo suggested people drink water instead.
# @SABCNews: BREAKING NEWS: King of Eswatini has fled amid public violence in the country.

image_read("images/interactions_plot.png")
```

Our main topic influence is COVID-19 as this is what currently affects the country the most. 
```{r Topic Modelling}
tidy_matrix <-
  tidy_media_df %>% count(screen_name, word) %>% cast_dfm(screen_name, word, n)

media_lda <- LDA(tidy_matrix, k = 5, control = list(seed = 1234))

media_topics <- tidy(media_lda, matrix = "beta")

media_top_terms <- media_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic,-beta)

media_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  pivot_wider(id_cols = term,
              names_from = topic,
              values_from = beta) %>%
  rename(
    "Legal News" = 2,
    "Former President Zuma facing court" = 3,
    "Nations Address" = 4,
    "Covid & Business World News" = 5,
    "Minister News" = 6
  ) %>%
  pivot_longer(cols = c(2, 3, 4, 5, 6),
               names_to = "topic",
               values_to = "beta") %>%
  drop_na() %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ topic, scales = "free") +
  scale_y_reordered()
```
We determine the correct number of topics in mechanisms like the LDA (Latent Dirichlet Allocation) as they determine the quality of features of the words.

A topic model is a type of statistic model for discovering the abstract "topics" that occur in a collection of documents.

Topic modeling is used to classify and retrieve information. These models capture word correlations in a collection of textual documents with topics. It is very difficult to pick the right number of topics for a specific dataset. It is important to determine the value of 'k'.

The importance of the value of 'k' is to ensure you do not over estimate or underestimate the data. If you over estimate it, you will be left with topics that carry very little meaning and if you under estimate the data, you will lose out on topics that could have been useful to your research.

By looking at the lowest minimum and the highest maximum, we can determine the 'k'. This is how we have interpreted the graphs produced. Griffiths2004 is not informative in this situation and is therefore ignore

This method used has been proved to produce the best results of LDA without having to manually tune the 'k' value.
```{r Gap k justify}
library("ldatuning")
library("topicmodels")

data("AssociatedPress", package = "topicmodels")
dtm <- AssociatedPress[1:10,]

result <- FindTopicsNumber(
  tidy_matrix,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(result)
```

```{r}
library(RedditExtractoR)
RedditExtractoR::reddit_urls(search_terms = "Covid", subreddit = "southafrica")
```






