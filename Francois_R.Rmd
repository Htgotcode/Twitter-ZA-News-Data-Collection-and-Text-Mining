---
title: "R Notebook"
output: html_notebook
---
Reasons for the media sources chosen:
News 24 : Recognized by APP Annie (App Annie is the standard in app analytics and app market data) as the most known South African internet media source. (Chinese Academy of Cyberspace Studiespublisher=Springer (15 September 2018). World Internet Development Report 2017: Translated by Peng Ping. p. 203. ISBN 9783662575246. OCLC 1052766508.)

Times Live : Claim to be South Africa's second-biggest news website, published by Arena Holdings (Times Live website). No evidence found to disprove this claim. In top 10 of most visited publication websites for South Africa.

EWN : Not much info but it can be considered to be popular because it originated from 2 radio stations (KFM and CapeTalk), Makes it a popular and people hear about it without actively having to go and search for the news agency themselves. In top 10 of most visited publication websites for South Africa.

eNCA: Not much info but seems popular. Used to E News from TV channel. Might not be as trustworthy as there has been complaints about racism recently. In top 10 of most visited publication websites for South Africa.

SABC: National news company with government ties. Reaches a wide variety of viewers in different languages. The company is both state owned and a public broadcaster company.https://www.businesslive.co.za/bd/national/media/2015-05-20-sabc-both-state-owned-company-and-public-broadcaster-minister-says/


Chapter 10 of the Constitution of the Republic of South Africa, 1996 makes provision for good governance of news media and what they publish.

The other 4 of the 5 is ranked among the top 10 most visited publication sites websites in south africa as found by 
https://mybroadband.co.za/news/internet/358453-here-are-south-africas-most-popular-news-websites.html.
These sites are the first sites on the top 10 list of publication sites that include general news along with news about covid-19 that do not have major historical issues that could bring their credibility in question.
Reasons that other news or publishing sources that are ranked higher were not used is due to the fact that these sources either only focus on specific news like new technology or financial aspects, or these sources are not as popular throughout the whole of South Africa.

Importing Packages
```{r}
install.packages("tidytext")
install.packages("tidyverse")
install.packages("sentimentr")
install.packages("textdata")
install.packages("stringi")
install.packages("rtweet")
install.packages("quanteda")
install.packages("topicmodels")
install.packages("reshape2")
```

Importing Libraries
```{r}
library(tidytext)
library(tidyverse)
library(textdata)
library(sentimentr)
library(stringr)
library(stringi)
library(rtweet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(scales)
library(readr)
library(lubridate)
library(topicmodels)
library(quanteda)
library(reshape2)
library(zoo)
```
Importing data
```{r}
media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
media_agency_df <- media_agency_df[ , colSums(is.na(media_agency_df)) < nrow(media_agency_df)]
media_agency_df <- media_agency_df %>% mutate("X1" = row_number())
```

Wrangeling and cleaning data
```{r}
#Cleaning data
media_agency_df <- media_agency_df %>%
  mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text),
         text = gsub("&amp", "", text),
         text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text),
         text = gsub("@\\w+", "", text),
         text = gsub("[[:punct:]]", "", text),
         text = gsub("[[:digit:]]", "", text),
         text = gsub("http\\w+", "", text),
         text = gsub("[ \t]{2,}", "", text),
         text = gsub("^\\s+|\\s+$", "", text),
         text = gsub("&amp", "", text)) %>%
  mutate(text = str_replace_all(text," "," "),
         text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
         text = str_replace_all(text,"#[a-z,A-Z]*"," "),
         text = str_replace_all(text,"@[a-z,A-Z]*"," "))


#tidying data
tidy_covid_media_df <- media_agency_df %>%
  #filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text)) %>%
  unnest_tokens(word, text, token = "tweets") %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"),
         !(word =="#sabcnews" |
             word =="#enca" |
             word =="https" |
             word =="#dstv403" |
             word =="t.co" |
             word =="rt" |
             word =="amp"))

```
```{r}
#finding the screen names of media angencies
agencies<- media_agency_df %>%
  select(screen_name) 
agencies %>%
  distinct(screen_name)
```

Seniment Analysis with SentimentR
```{r}

mytext<- media_agency_df 

mytext <- get_sentences(mytext$text)
sr_data <-sentiment(mytext)
#write_csv(sr_data,"data_in/sentimentr_data.csv")
```
Adding time data to sentiment df
```{r}
time_data <- media_agency_df %>%
  select(X1,created_at,screen_name)

SentiR_data <-left_join(time_data,sr_data,
                        by=c("X1"="element_id"))
#write_csv(SentiR_data,"data_in/sr_df_time.csv")

```

Modeling the time data
```{r}
#setting the date format
#NB only run date format once
SentiR_data$created_at = ymd_hms(SentiR_data$created_at)
#filtering by news agency
news24<- SentiR_data %>%
  filter(screen_name=="News24")

times_live<-SentiR_data %>%
  filter(screen_name=="TimesLIVE")

dailymaverick<-SentiR_data %>%
  filter(screen_name=="dailymaverick")

sabc<-SentiR_data %>%
  filter(screen_name=="SABCNews")

eNCA<-SentiR_data %>%
  filter(screen_name=="eNCA")
```
Graphs
```{r}
#graph for News24 sentiment over time
ggplot(data=news24, aes(x=created_at, y=rollmean(sentiment, 30,  na.pad=TRUE))) +
       geom_line(color="pink", size=.5)+
  geom_smooth() +
  theme_minimal()+
  facet_wrap(~screen_name) +
  scale_y_continuous(expand = c(0,0), breaks = c(-0.4,-0.2,0,0.2,0.4)) +
  ylim(c(-0.4, 0.4))

```
```{r}
#graph for Times Live sentiment over time
ggplot(data=times_live, aes(x=created_at, y=rollmean(sentiment, 30,  na.pad=TRUE))) +
       geom_line(color="pink", size=.5)+
  geom_smooth() +
  theme_minimal()+
  facet_wrap(~screen_name) +
  scale_y_continuous(expand = c(0,0), breaks = c(-0.4,-0.2,0,0.2,0.4)) +
  ylim(c(-0.4, 0.4))


```
```{r}
#graph for EWN sentiment over time
ggplot(data=dailymaverick, aes(x=created_at, y=rollmean(sentiment, 30,  na.pad=TRUE))) +
       geom_line(color="pink", size=.5)+
  geom_smooth() +
  theme_minimal()+
  facet_wrap(~screen_name) +
  scale_y_continuous(expand = c(0,0), breaks = c(-0.4,-0.2,0,0.2,0.4)) +
  ylim(c(-0.4, 0.4))


```
```{r}
#graph for eNCA sentiment over time
ggplot(data=eNCA, aes(x=created_at, y=rollmean(sentiment, 30,  na.pad=TRUE))) +
       geom_line(color="pink", size=.5)+
  geom_smooth() +
  theme_minimal()+
  facet_wrap(~screen_name) +
  scale_y_continuous(expand = c(0,0), breaks = c(-0.4,-0.2,0,0.2,0.4)) +
  ylim(c(-0.4, 0.4))


```
```{r}
#graph for SABC sentiment over time
ggplot(data=sabc, aes(x=created_at, y=rollmean(sentiment, 30,  na.pad=TRUE))) +
       geom_line(color="pink", size=.5)+
  geom_smooth() +
  theme_minimal()+
  facet_wrap(~screen_name) +
  scale_y_continuous(expand = c(0,0), breaks = c(-0.4,-0.2,0,0.2,0.4)) +
  ylim(c(-0.4, 0.4))


```

#Kyles Code
Topic Modelling
```{r}

tidy_matrix <- tidy_covid_media_df %>% count(screen_name, word) %>% cast_dfm(screen_name, word, n)

 
media_lda <- LDA(tidy_matrix, k = 4, control = list(seed = 1234))
media_lda
media_topics <- tidy(media_lda, matrix = "beta")
media_topics
media_top_terms <- media_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
 

media_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
```
Vader stuff
```{r}
vader_data<- read_csv("data_in/vader_df.csv")

covid_dictionary <- c("herd", "immunity", "incubation", "job", "loss", "Kits", "lockdown", "mask", "N95", "outbreak", "pandemic", "quarantine", "recovery", "sanitiser", "transmission", "Underlying", "conditions", "Ventilators", "WHO", "xenophobia", "youTube", "zoonotic", "stay-at-home", "covid", "coronavirus", "hyrdoxychloroquine", "asymptomatic", "frontline", "virus", "self-isolation", "disinfectant", "shelter-in-place", "masks", "SARS-CoV-2", "ICU", "corona", "reopen", "distancing", "covering", "furlough", "tracer", "easing", "remdesivir", "mail-in", "hornet", "antibody", "in-person", "defund", "racism", "looting", "loot", "reopen", "two-metre", "pandemic", "looter", "distancing", "dexamethasone", "racial", "vaccine",
                      "curfew","johnssons","astrazeneca","hospitals","social-distance",
                      "social-distancing","police","regulations","symptoms","testing",
                      "positive-tests","negative-tests","confirmed-cases","restrictions",
                      "deaths","infected","recoveries","level","jobs","unemployed",
                      "doctors","infections","sanitise","sanitiser","sanitisation",
                      "containment")

```



```{r}
install.packages("XML")
install.packages("RCurl")
install.packages("RColorBrewer")
install.packages("wordcloud")

library(XML)
library(RCurl)
library(RColorBrewer)
library(wordcloud)
```
Setting up Reddit scraper

Kyle's Reddit code that he graciously gave to me :)
I attempted 2 other reddit web scraping methods for scraping comments from post but they were unsuccessful. We settled instead to just focus on the posts themselves about Covid-19 in South Africa.
By scraping the r/southafrica subreddit we managed to find about 250 posts dating back to last year December which would be from when South Africa was entering harder lockdown times again. We expect that due to the fact that this is the post themselves and not the comment sof people that the sentiment would be close to neutral or will have tendencies to return to neutral if it deviates from it. We will be making use of a 7 day rolling average  to see the sentiment of the reddit posts as well a smooth graph to see the tendency of the posts sentiment over time.
```{r}
library(RedditExtractoR)

reddit_Covid_data<-RedditExtractoR::reddit_urls(search_terms = "Covid-19", subreddit = "southafrica",page_threshold = 10)
```
Cleaning Reddit stuff (Made use of Kyle's twitter cleaner)
```{r}
reddit_Covid_data <- reddit_Covid_data %>%
  mutate(title = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", "", title ),
         title  = gsub("&amp", "", title ),
         title  = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", title ),
         title = gsub("@\\w+", "", title ),
         title  = gsub("[[:punct:]]", "", title ),
         title  = gsub("[[:digit:]]", "", title ),
         title  = gsub("http\\w+", "", title ),
         title  = gsub("[ \t]{2,}", "", title ),
         title  = gsub("^\\s+|\\s+$", "", title ),
         title  = gsub("&amp", "", title )) %>%
  mutate(title  = str_replace_all(title ," "," "),
         title  = str_replace_all(title ,"RT @[a-z,A-Z]*: "," "),
         title  = str_replace_all(title ,"#[a-z,A-Z]*"," "),
         title  = str_replace_all(title ,"@[a-z,A-Z]*"," "))
write_csv(reddit_Covid_data,"data_in/reddit_SA.csv")

reddit_Covid_data$date= dmy(reddit_Covid_data$date)
reddit_Covid_data<- reddit_Covid_data %>% mutate("X1"=row_number())
```
Sentiment of reddit
```{r}
sd<- get_sentences(reddit_Covid_data$title)
reddit_sentiment<- sentiment(sd)

reddit_sentiment<- left_join(reddit_Covid_data,reddit_sentiment,
                             by=c("X1"="element_id"))


```

Modeling Reddit sentiment over time
```{r}
ggplot(data=reddit_sentiment, aes(x=date, y=rollmean(sentiment, 30,  na.pad=TRUE))) +
       geom_line(color="pink", size=.5)+
  geom_smooth() +
  theme_minimal()+
  scale_y_continuous(expand = c(0,0), breaks = c(-0.4,-0.2,0,0.2,0.4)) +
  ylim(c(-0.4, 0.4))+
  labs(title = "Sentiment from the titles of Reddit post about Covid in SA", x="Date", 
       y="Sentiment")
```
As can be seen from the graph the sentiment of the posts started out more negative and that would correlated to be around to higher infections per day. The sentiment does have a positive trend towards the end of the wave of infections. Even though there is very slight fluctuations the posts seem mostly neutral.

```{r}
comments<- get_reddit(search_terms = "Covid-19", subreddit = "southafrica", page_threshold = 1,
                      sort_by = "comments")
```




Trying Reddit comments
```{r}
# A script to scrape the top comments from the top posts of Reddit or a specified 
# subreddit and create a word frequency table from them. It also can plot a 
# word cloud from the most common words on the page. 

# Notes:
#
# This won't work on NSFW subreddits that require you to click that you're 18 
# or older. I haven't looked at how to code that yet.


# Overview of the function:  
# 1. Get the selected page (front or subreddit)
# 2. Build the links to all of the comments
# 3. Scrape each comments page (this step can take a while, 10 to 40 seconds)
# 4. Clean it up
# 5. Get the word frequency 
# 6. Plot a word cloud
# 7. Return a word frequency table


redditScrape <- function(subred = c('nameOfSubred', 'allTop'), time = c('day', 'week', 'month', 'year'), plotCloud = TRUE, saveText = FALSE, myDirectory = "/choose/a/directory") {


	#######################################################
	# 0. Load the required packages.  And check a few items
	
	require(XML)
	require(RCurl)
	require(RColorBrewer) 
	require(wordcloud)
	
	# if more than one time, apply function to each time frame separately
	if (length(time) > 1) { 
	  return(lapply(time, function(i) 
	    redditScrape(subred=subred, time=i, saveText=saveText, myDirectory=myDirectory)))
	}
	

	#######################################################
	# 1. Make the url, get the page. 

	if (subred == 'allTop') {
		url <- paste('http://www.reddit.com/top/?sort=top&t=', time, sep = "")
	} else {	
		url <- paste("http://www.reddit.com/r/", subred, "/top/?sort=top&t=", time, sep = "")
	}
	
	doc <- htmlParse(url)

	#######################################################
	# 2. Get the links that go to comment sections of the posts

	links <- xpathSApply(doc, "//a/@href")
	comments <- grep("comments", links)
	comLinks <- links[comments]
	comments <- grep('reddit.com', comLinks, fixed=TRUE)
	comLinks <- comLinks[comments]


	#######################################################
	#  3. Scrape the pages
	#  This will scrape a page and put it in to 
	#  an R list object 

	textList <- as.list(rep(as.character(""), length(comLinks))) 
	docs <- getURL(comLinks)
	for (i in 1:length(docs)) {
		textList[[i]] <- htmlParse(docs[i], asText = TRUE)
		textList[[i]] <- xpathSApply(textList[[i]], "//p", xmlValue)
	}
		
	#######################################################
	#  4. Clean up the text.

	# Remove the submitted lines and lines at the end of each page
	for (i in 1:length(textList)) {
		submitLine <- grep("submitted [0-9]", textList[[i]]) 
		textList[[i]] <- textList[[i]][{(submitLine[1] + 1):(length(textList[[i]])-10)}]
	}
	
	# Removing lines capturing user and points, etc.
	# Yes, there could be fewer grep calls, but this made it 
	# easier to keep track of what was going on.
	textList <- lapply(textList, function(i){
            		grep('points 1 minute ago', i) -> nameLines1
            		grep('points [0-9] minutes ago', i) -> nameLines2
            		grep('points [0-9][0-9] minutes ago', i) -> nameLines3
            		grep("points 1 hour ago", i) -> nameLines4
            		grep("points [0-9] hours ago", i) -> nameLines5
            		grep("points [0-9][0-9] hours ago", i) -> nameLines6
            		grep('points 1 day ago', i) -> nameLines7
            		grep('points [0-9] days ago', i) -> nameLines8
            		grep('points [0-9][0-9] days ago', i) -> nameLines9
            		grep('points 1 month ago', i) -> nameLines10
            		grep('points [0-9] months ago', i) -> nameLines11
            		grep('points [0-9][0-9] months ago', i) -> nameLines12
            		allLines <- c(nameLines1, nameLines2, nameLines3, nameLines4, 
            			nameLines5, nameLines6, nameLines7, nameLines8, nameLines9, 
            			nameLines10, nameLines11, nameLines12)
            		temp <- i[-allLines]
            		temp <- temp[temp!=""]
            		tolower(temp)
            	  })
  
	# Let's simplify our list. Could have been done earlier, but so it goes. 
	allText <- unlist(textList)

	# Remove the punctuation, links, etc.
	allText <- gsub("https?://[[:alnum:][:punct:]]+", "", allText)
	allText <- gsub("[,.!?\"]", "", allText)
	allText <- strsplit(allText, "\\W+", perl=TRUE)
	allText <- unlist(allText)

	# Remove frequent words and orphans of contractions (that sounds 
	# sadder than it is).
	frequentWords <- c("the", "be", "been", "to", "of", "and", "a", "in", 
	"that", "have", "i", "it", "for", "not", "on", "with", "he", "as", "you", 
	"do", "at", "this", "but", "his", "by", "from", "they", "we", "say", "her", 
	"she", "or", "an", "will", "my", "one", "all", "would", "there", "their", 
	"what", "so", "up", "out", "if", "about", "who", "get", "which", "go", 
	"me", "when", "make", "can", "like", "time", "no", "just", "him", "know", 
	"take", "people", "into", "year", "your", "good", "some", "could", "them", 
	"see", "other", "than", "then", "now", "look", "only", "come", "its", 
	"over", "think", "also", "back", "after", "use", "two", "how", "our", 
	"work", "first", "well", "way", "even", "new", "want", "because", "any", 
	"these", "give", "day", "most", "us", 'is', 'are', 'was', 'were', 'i', 's', 
	'was', 'don', 'aren', 'points1', 'point', 't', 'm', 'points0', '10', '1', 
	're', 'll', 'd', '2', '3', '4', '5', '6', '7', '8', '9', 'doesn','d', 've', 
	'r', 'has', 'had', 'been', 'being', '0', 'more', 'really', 'isn', 'very', 
	'am', 'didn', 'wouldn', '', 'points', 'point', 'months', 'ago', 'deleted', 
	'much')

	for (i in 1:length(frequentWords)) { 
		allText <- allText[allText!=frequentWords[i]]
	}

	# Save the file to your drive. This way you can drop it into
	# Wordle.net or use it other places. 
	if (saveText == TRUE) {
		curWD <- getwd() 
		setwd(myDirectory)
		filename <- paste("Reddit Comments Postscrub ", subred, " ", time, " ", 
			Sys.time(),".txt", sep = "") 
		write.table(allText, file = filename, row.names=F, col.names=F, append=T)
		# save(allText, file = filename)
		textListBackup <- textList
		setwd(curWD)
	}

	#######################################################
	#  5. Word frequency table

	textTable <- table(allText)
	textTable <- sort(textTable, decreasing = TRUE)

	#######################################################
	#  6. Plot word cloud

	if (plotCloud == TRUE) {
		# This is a nice option.  Just use a portion of the 0-1 for color
		rainbow(30,s=.8,v=.6,start=.5,end=1,alpha=1) -> pal
		wordcloud(names(textTable[1:200]), textTable[1:200], scale = c(4,.5), max.words = 200, colors = pal)
	}

	#######################################################
	#  7. Return the text table

	textTable
}

```

