---
title: "R Notebook"
output: html_notebook
---

```{r SETUP}
library(rtweet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(tidytext)
library(stringr)
library(scales)
library(readr)
library(lubridate)
library(vader)
library(topicmodels)
library(quanteda)
library(lubridate)
library(zoo)
library(plotly)
library(forcats)
library(igraph)
library(ggraph)
library(widyr)
library(qdapRegex)
library(magick)
```

```{r Fetching Posts}
library(RedditExtractoR)
reddit_posts <- RedditExtractoR::reddit_urls(search_terms = "Covid", subreddit = "southafrica", page_threshold = 10)
write_as_csv(reddit_posts, "data_in/reddit_posts")
```

```{r Clean up, include=FALSE}
#created if more posts could be accessed to be more topic specific
# covid_dictionary <- c("herd", "immunity", "incubation", "job", "loss", "Kits", "lockdown", "mask", "N95", "outbreak", "pandemic", "quarantine", "recovery", "sanitiser", "transmission", "Underlying", "conditions", "Ventilators", "WHO", "xenophobia", "youTube", "zoonotic", "stay-at-home", "covid", "coronavirus", "hyrdoxychloroquine", "asymptomatic", "frontline", "virus", "self-isolation", "disinfectant", "shelter-in-place", "masks", "SARS-CoV-2", "ICU", "corona", "reopen", "distancing", "covering", "furlough", "tracer", "easing", "remdesivir", "mail-in", "hornet", "antibody", "in-person", "defund", "racism", "looting", "loot", "reopen", "two-metre", "pandemic", "looter", "distancing", "dexamethasone", "racial", "vaccine", "curfew","johnssons", "astrazeneca", "hospitals", "social-distance", "social-distancing", "police", "regulations", "symptoms", "testing", "positive-tests", "negative-tests" , "confirmed-cases", "restrictions", "deaths", "infected", "recoveries", "level", "jobs", "unemployed", "doctors", "infections", "sanitise", "sanitiser", "sanitisation", "containment")

reddit_posts_df <- read_csv("data_in/reddit_posts.csv")
reddit_posts_df <-
  reddit_posts_df[, colSums(is.na(reddit_posts_df)) < nrow(reddit_posts_df)]

#JUST tidy
rm_reddit_n_url <-
  rm_(pattern = pastex("@rm_twitter_url", "@rm_url"))
reddit_posts_df <- reddit_posts_df %>%
  mutate(
    title = rm_reddit_n_url(title),
    title = gsub("@\\w+", " ", title),
    title = gsub("[[:punct:]]", " ", title),
    title = gsub("[[:digit:]]", " ", title),
    title = gsub("http\\w+", " ", title),
    title = gsub("[ |\t]{2,}", " ", title),
    title = gsub("^ ", " ", title),
    title = gsub(" $", " ", title),
    title = gsub("^BUSINESS", " ", title),
    title = gsub("^World", " ", title)
  ) %>%
  mutate(
    title = str_replace_all(title, " ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " "),
    title = str_replace_all(title, " ", " "),
    title = str_replace_all(title, "RT @[a-z,A-Z]*: ", " "),
    title = str_replace_all(title, "#[a-z,A-Z]*", " "),
    title = str_replace_all(title, "@[a-z,A-Z]*", " ")
  )

#most stop words are filtered based on the media agencies tag at the beginning of each Tweet. eg. WATCH: *headline follows*.
reddit_stop_words <-
  tibble(
    word = c(
      "sabcnews",
      "enca",
      "dstv",
      "sabckzn",
      "maverick",
      "opinionista",
      "dm",
      "scorpio",
      "dstv403",
      "itus",
      "rt",
      "amp",
      "tgifood",
      "mamelodi",
      "sundowns",
      "ofmagazineavailable",
      "casablanca",
      "oped",
      "newsdeck",
      "editorial",
      "newflash",
      "southafricanmorning",
      "newslink",
      "encas",
      "southafricatonight",
      "themiddayview",
      "thelead",
      "propertymatters",
      "ba",
      "ka",
      "ya",
      "ga",
      "wa",
      "le",
      "kwa",
      "morninglivesabc",
      "monday",
      "prix",
      "azerbaijan",
      "encasis",
      "encabusiness",
      "encasspeaks",
      "south",
      "africa",
      "pm",
      "sa",
      "pm",
      "encas",
      "iss",
      "icymi",
      "timeslive",
      "fullview",
      "newsbreaksjul",
      "newsbreakjul",
      "sabc",
      "nca",
      "ncas",
      "op",
      "ig",
      "ed",
      "pl",
      "news24",
      "news24s",
      "dm168",
      "siness",
      "usiness",
      "ewsdeck",
      "orld",
      "sunday",
      "friday",
      "saturday",
      "thursday",
      "wednesday",
      "monday",
      "tuesday",
      "pics",
      "live"
    )
  )

#tidy df and unnest
reddit_media_df <- reddit_posts_df %>%
  #filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
  unnest_tokens(word, title, token = "tweets") %>%
  filter(
    !word %in% stop_words$word,!word %in% reddit_stop_words$word, 
    !word %in% reddit_negated_words$word2, 
    !word %in% str_remove_all(stop_words$word, "'"),
    str_detect(word, "[a-z]")
  )

```

```{r top words}
#top words
top_words_reddit <- reddit_media_df %>%
  anti_join(stop_words) %>%
  count(word) %>%
  arrange(desc(n))
top_words_reddit %>%
  slice(1:20) %>%
  ggplot(aes(reorder(word,-n), n, fill = word)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(
      angle = 60,
      hjust = 1,
      size = 13
    ),
    plot.title = element_text(hjust = 0.5, size = 18)
  ) +
  ylab("Frequency") +
  xlab ("") +
  ggtitle("Most frequent reddit posts") +
  guides(fill = FALSE)
```

```{r bigrams}
#bigrams
reddit_bigram_df <- reddit_posts_df %>%
  #filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
  filter(!str_detect(title, "^RT")) %>%
  mutate(title = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", title)) %>%
  unnest_tokens(bigram, title, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

reddit_bigrams_filtered <- reddit_bigram_df %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
reddit_bigram_counts <- reddit_bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

reddit_bigrams_united <- reddit_bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

#weights and graphs
reddit_bigram_graph <- reddit_bigram_counts %>%
  filter(n > 100) %>%
  graph_from_data_frame()

```

```{r negation words}
#vader lexicon imported from VADER GitHub
reddit_vader_lexicon <- read_csv2("data_in/vader_lexicon.csv") %>%
  rename("word" = TOKEN, "value" = `MEAN-SENTIMENT-RATING`)

#common negation words
negation_words <-
  c(
    "not",
    "no",
    "never",
    "without",
    "no",
    "not",
    "none",
    "no one",
    "nobody",
    "nothing",
    "neither",
    "nowhere",
    "never",
    "doesn’t",
    "isn’t",
    "wasn’t",
    "shouldn’t",
    "wouldn’t",
    "couldn’t",
    "won’t",
    "can’t",
    "don’t"
  )
reddit_negated_words <- reddit_bigram_df %>%
  filter(word1 %in% fixed(negation_words, ignore_case = TRUE)) %>%
  inner_join(reddit_vader_lexicon, by = c(word2 = "word")) %>%
  mutate(value = as.double(value)) %>%
  count(word1, word2, value, sort = TRUE) %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  mutate(word2 = reorder(word2, contribution))
reddit_negated_words %>%
  head(40) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ word1, scales = "free_y") +
  labs(x = "Sentiment value * # number of occurrences",
       y = "Words preceded by negation terms")

```

```{r Counting and correlating among sections, warning=FALSE}
reddit_section_words <- reddit_media_df %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0)

# count words co-occuring within sections
reddit_word_pairs <- reddit_section_words %>%
  pairwise_count(word, section, sort = TRUE)

# we need to filter for at least relatively common words first
reddit_word_cors <- reddit_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

reddit_word_cors %>%
  slice_max(correlation, n = 6) %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap( ~ item1, scales = "free") +
  coord_flip()

set.seed(1234)
reddit_word_cors %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation),
                 show.legend = FALSE,
                 edge_width = 3) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

```{r VADER}
# vader_df <- vader_df(reddit_posts_df$title)
# write_as_csv(vader_df, "data_in/vader_reddit_posts")

vader_df <- read_csv("data_in/vader_reddit_posts.csv")
vader_df <- vader_df %>% mutate("X1" = row_number())
reddit_posts_df <- reddit_posts_df %>% mutate("X1" = row_number())
reddit_vader_df <- reddit_posts_df %>% left_join(vader_df, by = "X1")

#General sentiment over time. Media agencies seem to have an even number of positive and negative tweets per day.
ggplot(
  reddit_vader_df %>%
    mutate(date = as.Date(date)) %>%
    group_by(date) %>%
    mutate(compound = replace(compound, is.na(compound), 0)) %>%
    summarise(sum_compound = sum(as.double(compound))),
  aes(date, sum_compound)
) +
  geom_bar(stat = "identity") +
  geom_smooth(method = "lm", na.rm = TRUE, se = FALSE) +
  theme_minimal() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_date(date_breaks = "3 day", date_labels = "%m-%d") +
  theme(
    axis.text.x = element_text(
      angle = 90,
      vjust = 0.5,
      hjust = 1
    ),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
  )
# An illegal gold miner who was severely injured in a clash in which four other illegal miners were killed has been charged for their murders Mpumalanga police said on Thursday
# Italy midfielder Jorginho said team glory is more important than any push to win a surprise Ballon d’Or award for the best player in world football after a teammate said he deserved it

ggplot(
  reddit_vader_df %>%
    mutate(date = as.Date(date)) %>%
    group_by(date) %>%
    summarise(compound),
  aes(date, compound)
) +
  geom_bar(stat = "identity") +
  geom_smooth(method = "lm", na.rm = TRUE) +
  theme_minimal() +
  scale_y_continuous(expand = c(0, 0))

#News sentiment
reddit_vader_df %>%
  mutate(date = as.Date(date)) %>%
  group_by(date) %>%
  summarise(compound) %>%
  arrange(desc(compound))

ggplot(reddit_vader_df %>% arrange(compound) %>% mutate(X1 = factor(X1, levels = X1)),
       aes(x = X1)) +
  geom_point(mapping =  aes(y = compound)) +
  theme_minimal() +
  scale_y_continuous(expand = c(0, 0))

reddit_vader_df %>%
  mutate(date = as.Date(date)) %>%
  group_by(date) %>%
  mutate(compound = replace(compound, is.na(compound), 0)) %>%
  summarise(sum_compound = sum(as.double(compound)))
```

```{r Interactions, echo=FALSE}
# reddit_vader_df %>% select(favorite_count, retweet_count) %>% mutate(favorite_count + retweet_count)

reddit_vader_df <- reddit_vader_df %>%
  mutate(date = as.Date(date, format = "%d-%m-%y")) %>%
  group_by(date) %>%
  mutate(
    num_comments = mean(num_comments)
  )
# mean for the day
reddit_interactions_plot <- ggplotly(
  ggplot(data = reddit_vader_df, aes(date)) +
    geom_ribbon(aes(
      ymin = 0, ymax = num_comments, fill = "num_comments"
    )) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      axis.ticks = element_blank(),
      
      legend.justification = c(0, 0),
      legend.position = c(0, 0),
      legend.background = element_blank(),
      legend.key = element_blank(),
      legend.title = element_blank(),
      
      plot.title = element_text(
        size = 14,
        face = "bold",
        margin = margin(0, 0, 10, 0),
        hjust = 0
      ),
      plot.caption = element_text(face = "bold", hjust = 0),
    )
)

reddit_interactions_plot
```

```{r Topic Modelling}
reddit_matrix <-
  reddit_media_df %>%  count(subreddit, word) %>% cast_dfm(subreddit, word, n)

reddit_media_lda <- LDA(reddit_matrix, k = 3, control = list(seed = 1234))

reddit_media_topics <- tidy(reddit_media_lda, matrix = "beta")

reddit_media_top_terms <- reddit_media_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic,-beta)

reddit_media_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  pivot_wider(id_cols = term,
              names_from = topic,
              values_from = beta) %>%
  rename(
    "topic 1" = 2,
    "topic 2" = 3,
    "topic 3" = 4
  ) %>%
  pivot_longer(cols = c(2, 3, 4),
               names_to = "topic",
               values_to = "beta") %>%
  drop_na() %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ topic, scales = "free") +
  scale_y_reordered()
```

```{r Gap k justify}
library("ldatuning")
library("topicmodels")

data("AssociatedPress", package = "topicmodels")
dtm <- AssociatedPress[1:10,]

result <- FindTopicsNumber(
  reddit_matrix,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(result)
```
