---
title: "R Notebook"
output: html_notebook
---

# Additional Requirements

We choose Reddit as our other data source. Facebook was considered but API needing proof of identity with an ID document seemed excessive. The 'ReddicommentractoR' package is used to extract comment and post data. Search term 'Covid-19' is used. Other terms were not used as their post dates went further than 6 months.

```{r Fetch post and comments, eval=FALSE, include=FALSE}
reddit_comments_df <-
  get_reddit(
    search_terms = "Covid-19",
    subreddit = "southafrica",
    cn_threshold = 3,
    page_threshold = 10,
    sort_by = "new",
    wait_time = 2
  )

write_csv(reddit_comments_df, "data_in/reddit_comments.csv")
```

# Comparing Comments From r/southafrica Covid-19 Posts.

We clean the data as we did before. Controversial and foul language is left in to not affect sentiment. The worst langauge is usually moderated within the Reddit communities before it is seen by thg public.

```{r Reddit Clean up, warning=FALSE, include=FALSE}
reddit_comments_df <- read_csv("data_in/reddit_comments.csv")
reddit_comments_df <-
  reddit_comments_df[, colSums(is.na(reddit_comments_df)) < nrow(reddit_comments_df)] %>% 
  mutate(comm_date = dmy(comm_date), post_date = dmy(post_date))

reddit_comments_df_clean <- reddit_comments_df %>%
  mutate(
    comment = tolower(comment), 
    comment = str_replace_all(comment, "-", " "),
    comment = str_replace_all(comment, "'", ""),
    comment = str_replace_all(comment, " ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " "),
    comment = str_replace_all(comment, " ", " "),
    comment = str_replace_all(comment, "#[a-z,A-Z]*", " "),
    comment = str_replace_all(comment, "@[a-z,A-Z]*", " "),
  ) %>% 
  mutate(
    comment = rm_twitter_n_url(comment),
    comment = gsub("&amp", "", comment),
    comment = gsub("J&amp;J", " ", comment),
    comment = gsub("@\\w+", " ", comment),
    comment = gsub("[[:punct:]]", "", comment),
    comment = gsub("[[:digit:]]", " ", comment),
    comment = gsub("http\\w+", " ", comment),
    comment = gsub("[ |\t]{2,}", " ", comment),
    comment = gsub("^ ", "", comment),
    comment = gsub(" $", "", comment),
  )
```


```{r Reddit bigrams, warning=FALSE, include=FALSE}
#bigrams
tidy_bigram_df <- reddit_comments_df_clean %>%
  unnest_tokens(bigram, comment, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- tidy_bigram_df %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

#tf-idf bigrams
bigram_tf_idf <- bigrams_united %>%
  count(title, bigram) %>%
  bind_tf_idf(bigram, title, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf %>%
  group_by(title) %>%
  slice_max(tf_idf, n = 20) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ title, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL) +
  theme_bw() 

# weights and graphs
# bigram_graph <- bigram_counts %>%
#   filter(n > 100) %>%
#   graph_from_data_frame()
# 
# set.seed(1234)
# a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
# ggraph(bigram_graph, layout = "fr") +
#   geom_edge_link(
#     aes(edge_alpha = n),
#     show.legend = FALSE,
#     arrow = a,
#     end_cap = circle(.07, 'inches')
#   ) +
#   geom_node_point(color = "lightblue", size = 5) +
#   geom_node_comment(aes(label = name),
#                  vjust = 1,
#                  hjust = 1,
#                  repel = TRUE) +
#   theme_void()
```


```{r Reddit negation words, warning=FALSE, include=FALSE}
#vader lexicon imported from VADER GitHub
vader_lexicon <- read_csv2("data_in/vader_lexicon.csv") %>%
  rename("word" = TOKEN, "value" = `MEAN-SENTIMENT-RATING`)

#common negation words
negation_words <-
  c(
    "not",
    "no",
    "never",
    "without",
    "no",
    "not",
    "none",
    "no one",
    "nobody",
    "nothing",
    "neither",
    "nowhere",
    "never",
    "doesn’t",
    "isn’t",
    "wasn’t",
    "shouldn’t",
    "wouldn’t",
    "couldn’t",
    "won’t",
    "can’t",
    "don’t"
  )
negated_words <- tidy_bigram_df %>%
  filter(word1 %in% fixed(negation_words, ignore_case = TRUE)) %>%
  inner_join(vader_lexicon, by = c(word2 = "word")) %>%
  mutate(value = as.double(value)) %>%
  count(word1, word2, value, sort = TRUE) %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  mutate(word2 = reorder(word2, contribution))
negated_words %>%
  head(40) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ word1, scales = "free_y") +
  labs(x = "Sentiment value * # number of occurrences",
       y = "Words preceded by negation terms",
       title = "Negation Words") +
  theme_bw() 

```


```{r Reddit Tidy, warning=FALSE, include=FALSE}
#tidy df and unnest
tidy_reddit_df <- reddit_comments_df_clean %>%
  unnest_tokens(word, comment, token = "words") %>%
  filter(
    !word %in% stop_words$word,
    !word %in% negated_words$word2, 
    !word %in% "pm", 
    str_detect(word, "[a-z]")
  ) %>% 
  mutate(post_date = as.Date(post_date))

```

From our tidied Reddit dataset, we look for the top words that appear. This will give us a good idea of what topics are being discussed the most. We find that with COVID-19 comments being pulled, the comments discuss people's general interaction witht he virus. The comments come from regular people and this shows what topics are discussed among people.

```{r Reddit top words, echo=FALSE, warning=FALSE}
#top words
top_words <- tidy_reddit_df %>%
  anti_join(stop_words, by = 'word') %>%
  count(word) %>%
  arrange(desc(n))
top_words %>%
  slice(1:20) %>%
  ggplot(aes(reorder(word,-n), n, fill = word)) +
  geom_bar(stat = "identity")  +
  theme_bw() +
  theme(
    axis.text.x = element_text(
      angle = 60,
      hjust = 1,
      size = 13
    )
  ) +
  ylab("Frequency") +
  xlab ("") +
  ggtitle("Most Frequent Reddit Comment Words") +
  guides(fill = FALSE)
```

A tf-idf is then modelled to determine which words are the most important per media agency. We also model the word importance by week and determine which media agency has the most unique topics of the week. Daily Maverick is domanting the first four weeks as they are the only user with Tweets from that time.

```{r Reddit tf-idf, echo=FALSE, warning=FALSE, fig.height = 15, fig.width = 15}
#tf-idf by user
tidy_reddit_tf_idf <- tidy_reddit_df %>%
  count(word, title) %>%
  bind_tf_idf(word, title, n) %>% 
    group_by(title) %>%
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, tf_idf)) 

tidy_reddit_tf_idf %>%
  group_by(title) %>%
  slice_max(tf_idf, n = 5) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ title, ncol = 2, scales = "free_y") +
  labs(x = "tf-idf", y = NULL) +
  theme_bw() +
  labs(
    title = "Highest tf-idf from Title"
  )
```


```{r Reddit tf-idf week, echo=FALSE, warning=FALSE, fig.height = 8, fig.width = 13}
#tf-idf week
tidy_reddit_tf_idf_date <- tidy_reddit_df %>%
  select(post_date, title, word) %>%
  mutate(post_date = as.Date(post_date)) %>% 
  group_by(post_date, title) %>%
  count(word, post_date) %>%
  bind_tf_idf(word, post_date, n)

tidy_reddit_tf_idf_date %>%
  mutate(week = week(as.Date(post_date))) %>% 
  group_by(week) %>%
  slice_max(tf_idf, n = 7) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title)) +
  geom_col(show.legend = TRUE) + 
  facet_wrap(~ week, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL) +
  theme_bw() +
  scale_fill_discrete(name = "Screen Name") +
  labs(
    title = "Highest tf-idf by Title in Each Week"
  )
```

The graph below shows which words are most likely to co-occur. We use this to understand the concomment of words in our topic modeling. The words shown are popular words in our dataset. 

```{r Reddit Counting and correlating among sections, echo=FALSE, warning=FALSE}
media_agency_section_words <- tidy_reddit_df %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0)

# count words co-occuring within sections
word_pairs <- media_agency_section_words %>%
  pairwise_count(word, section, sort = TRUE)

# we need to filter for at least relatively common words first
word_cors <- media_agency_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

# Correlation of next word
# word_cors %>%
#   filter(item1 == "vaccine")

word_cors %>%
  filter(item1 %in% c("covid", "vaccine", "lockdown", "gauteng", "ramaphosa", "police")) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation, fill = item1)) +
  geom_bar(stat = "identity") +
  facet_wrap( ~ item1, scales = "free") +
  coord_flip() +
  theme_bw() +
  xlab("Correlation Strength") +
  ylab("Preceding Word") +
  labs(title = "Words that Precede Popular Terms") +
  theme(legend.justification = c(-1,-2))

# modeled correlation map but not used.
# set.seed(1234)
# word_cors %>%
#   filter(correlation > .65) %>%
#   graph_from_data_frame() %>%
#   ggraph(layout = "fr") +
#   geom_edge_link(aes(edge_alpha = correlation),
#                  show.legend = FALSE,
#                  edge_width = 3) +
#   geom_node_point(color = "lightblue", size = 5) +
#   geom_node_comment(aes(label = name), repel = TRUE) +
#   theme_void()
```

# Sentiment Analysis

```{r Reddit VADER write, eval=FALSE, warning=FALSE, include=FALSE}
vader_df <- vader_df(reddit_comments_df_clean$comment)
write_as_csv(vader_df, "data_in/vader_reddit_comments")
```


```{r Reddit VADER setup, warning=FALSE, include=FALSE}
vader_df <- read_csv("data_in/vader_reddit_comments.csv")
vader_df <- vader_df %>% mutate("X1" = row_number())
reddit_comments_df_clean <- reddit_comments_df_clean %>% mutate("X1" = row_number())
reddit_vader_df <- reddit_comments_df_clean %>% left_join(vader_df, by = "X1") %>%
  mutate(compound = replace(compound, is.na(compound), 0))

```

## Sentiment over time

Reddit sentiment is sporadic. Everyone has their own opinion on a post title whether it is a positive or negative title.

```{r Reddit VADER over time, echo=FALSE, fig.width=15, warning=FALSE}
#Sentiment over time
ggplot(
  reddit_vader_df %>%
    mutate(post_date = as.Date(post_date)) %>%
    group_by(post_date) %>%
    summarise(sum_compound = sum(as.double(compound))),
  aes(post_date, sum_compound)
) +
  geom_line(lwd = 1.5, colour = "red") +
  geom_smooth(method = "lm", formula = "y ~ x",na.rm = TRUE, se = FALSE, linetype = "dashed") +
  theme_bw() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_date(date_breaks = "3 day", date_labels = "%m-%d", expand = c(0, 0)) +
  ylab("Total Sentiment") +
  xlab("Date") +
  labs(
    title = "Sentiment Over Time by Media Agency",
  ) +
  theme(
    axis.text.x = element_text(
      angle = 90,
      vjust = 0.5,
      hjust = 1
    ),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.background = element_blank(),
    legend.title = element_blank(),
    legend.text = element_blank(),
    legend.key = element_blank(),
    legend.justification = c(-2,-2)
  )
```

### Most Negative Comment

Lambpanties: it does but sheesh the poor people my dad rents property to a restuarant owner and the poor guy hasnt been able to pay his rent about or different months now one month my parents even pitched in to pay his staffs wages there is no way the poor guy is not going to go under from this cherry from hell on top he has covid right now to boot	

```{r Reddit VADER most negative, eval=FALSE, include=FALSE}
head(reddit_vader_df %>% select(comment, compound) %>% 
  arrange(compound), 1)
```

### Most Positive Comment

babufrikhasaposse: but arts a strong driver of an improved society you dont want to live in a society without arts and demanding other people meet a standard you set while pretending they dont contribute positively to society is a bit absurd not everyone is interested in science and the aim of education isnt economic value exclusively even if everything you said is true that doesnt make the comment i replied to mot just an asshole thing to say

```{r Reddit VADER most positive, eval=FALSE, include=FALSE}
head(reddit_vader_df %>% select(comment, compound) %>% 
  arrange(desc(compound)),1)
```

\newpage

# Interations

### Highest upvoted comment from each post (Upvotes > 10 to remove noise)

```{r Reddit Interactions Highest, echo=FALSE, fig.height=12, fig.width=20, warning=FALSE}
reddit_comments_df %>% group_by(title) %>% slice_max(order_by = comment_score) %>% select(title, user, comment, comment_score) %>% filter(comment_score > 10)
```

### Controversial Comments

```{r Reddit Interactions Controversial, echo=FALSE, warning=FALSE, fig.height=12, fig.width=20}
reddit_vader_df %>% group_by(title) %>% select(title, user, text, comment_score, controversiality) %>% filter(controversiality == 1)
```

\newpage

# Topic Modelling

## Gap k justification

Our result is 9-14 topics. From this we found 12 topics to be accurate.

```{r Reddit tidy matrix, warning = FALSE, include = FALSE}
tidy_matrix <-
tidy_reddit_df %>% count(title, word) %>% cast_dfm(title, word, n)
```


```{r Reddit Gap k justify, eval=FALSE, warning=FALSE, include=FALSE}
data("AssociatedPress", package = "topicmodels")
dtm <- AssociatedPress[1:10, ]

result <- FindTopicsNumber(
  tidy_matrix,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(result)
```

## Topics found

### Beta

Our main topic influence is COVID-19, as this is what currently affects the country the most. Our words are mapped to a beta which shows the amount the word appears per topic. 

```{r Reddit Topic Modelling Beta, echo=FALSE, warning=FALSE, fig.height=12, fig.width=15}
reddit_lda <- LDA(tidy_matrix, k = 12, control = list(seed = 1234))


reddit_topics_beta <- tidy(reddit_lda, matrix = "beta")

reddit_top_terms <- reddit_topics_beta %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>%
  ungroup() %>%
  arrange(topic, -beta)

reddit_top_terms <- reddit_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  pivot_wider(id_cols = term,
              names_from = topic,
              values_from = beta) %>%
    rename(
      "Russian Vaccine Data" = 2,
      "Religious People on Vaccination" = 3,
      "Lockdown Discussions" = 4,
      "Covid Deaths" = 5,
      "Time Based Discussion in Lockdown" = 6,
      "Vaccine Discussions" = 7,
      "Sale of Alcohol and the Truama it Causes" = 8,
      "Zuma's Prison Time " = 9,
      "Gauteng in Need of More Hospital Equipment" = 10,
      "People's Interaction with Covid Laws" = 11,
      "People Wearing Masks" = 12,
      "Covid Reported Death Data" = 13
    ) %>%
    pivot_longer(
      cols = c(-1),
      names_to = "topic",
      values_to = "beta"
    ) %>%
    drop_na()

ggplot(reddit_top_terms, aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ topic, scales = "free_y", ncol = 2) +
  scale_y_reordered() +
  labs(title = "Modeled Topics (All Tweets)")
```