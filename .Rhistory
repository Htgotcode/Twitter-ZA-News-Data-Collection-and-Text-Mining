count(word1, word2, word3, sort = TRUE)
# new trigrams counts:
trigram_counts <- trigrams_filtered %>%
count(word1, word2, word3, sort = TRUE)
trigrams_united <- trigrams_filtered %>%
unite(trigram, word1, word2, word3, sep = " ")
#tf-idf trigrams
tidy_trigram_df <- trigrams_united %>%
count(screen_name, trigram) %>%
bind_tf_idf(trigram, screen_name, n) %>%
arrange(desc(tf_idf))
tidy_trigram_df %>%
group_by(screen_name) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(trigram, tf_idf), fill = screen_name)) +
geom_col(show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
tidy_trigram_df %>%
group_by(screen_name) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(trigram, tf_idf), fill = screen_name)) +
geom_col(show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
media_agency_section_words <- media_agency_df %>%
mutate(section = row_number() %/% 10) %>%
filter(section > 0) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words$word)
media_agency_section_words
install.packages("widyr")
library(widyr)
word_pairs <- media_agency_section_words %>%
pairwise_count(word, section, sort = TRUE)
word_pairs <- media_agency_section_words %>%
pairwise_count(word, section, sort = TRUE)
word_pairs
# we need to filter for at least relatively common words first
word_cors <- media_agency_section_words %>%
group_by(word) %>%
filter(n() >= 20) %>%
pairwise_cor(word, section, sort = TRUE)
# we need to filter for at least relatively common words first
word_cors <- media_agency_section_words %>%
group_by(word) %>%
filter(n() >= 20) %>%
pairwise_cor(word, section, sort = TRUE)
word_cors
word_cors %>%
filter(item1 == "covid")
word_cors %>%
filter(item1 == "vaccine")
w
word_cors %>%
filter(item1 %in% c("covid", "vaccine", "lockdown", "guateng")) %>%
group_by(item1) %>%
slice_max(correlation, n = 6) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
set.seed(1234)
word_cors %>%
filter(correlation > .15) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors
word_cors %>%
filter(correlation > .9) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors %>%
filter(correlation > .5) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors %>%
filter(correlation > .6) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors %>%
filter(correlation > .7) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
plot <- media_vader_df %>% mutate(created_at = as.Date(created_at)) %>%  group_by(created_at) %>% mutate(retweet_daily = mean(retweet_count), favorite_daily = mean(favorite_count), total = mean(favorite_count + retweet_count)) %>%
ggplot(aes(created_at)) +
geom_ribbon(aes(ymin = 0, ymax = total, fill = "Total")) +
geom_ribbon(aes(ymin = 0, ymax = favorite_daily, fill = "Favorites")) +
geom_ribbon(aes(ymin = 0, ymax = retweet_daily, fill = "Retweets"))
ggplotly(plot)
ggplot(media_agency_df, aes(x = created_at, fill = screen_name)) +
geom_histogram(position = "identity", bins = 20, show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 1)
library(rtweet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(tidytext)
library(stringr)
library(scales)
library(readr)
library(lubridate)
library(vader)
library(topicmodels)
library(quanteda)
library(lubridate)
library(zoo)
library(plotly)
library(forcats)
library(igraph)
library(ggraph)
library(widyr)
ggplot(media_agency_df, aes(x = created_at, fill = screen_name)) +
geom_histogram(position = "identity", bins = 20, show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 1)
covid_dictionary <- c("herd", "immunity", "incubation", "job", "loss", "Kits", "lockdown", "mask", "N95", "outbreak", "pandemic", "quarantine", "recovery", "sanitiser", "transmission", "Underlying", "conditions", "Ventilators", "WHO", "xenophobia", "youTube", "zoonotic", "stay-at-home", "covid", "coronavirus", "hyrdoxychloroquine", "asymptomatic", "frontline", "virus", "self-isolation", "disinfectant", "shelter-in-place", "masks", "SARS-CoV-2", "ICU", "corona", "reopen", "distancing", "covering", "furlough", "tracer", "easing", "remdesivir", "mail-in", "hornet", "antibody", "in-person", "defund", "racism", "looting", "loot", "reopen", "two-metre", "pandemic", "looter", "distancing", "dexamethasone", "racial", "vaccine", "curfew","johnssons", "astrazeneca", "hospitals", "social-distance", "social-distancing", "police", "regulations", "symptoms", "testing", "positive-tests", "negative-tests" , "confirmed-cases", "restrictions", "deaths", "infected", "recoveries", "level", "jobs", "unemployed", "doctors", "infections", "sanitise", "sanitiser", "sanitisation", "containment")
covid_dictionary <- c("herd", "immunity", "incubation", "job", "loss", "Kits", "lockdown", "mask", "N95", "outbreak", "pandemic", "quarantine", "recovery", "sanitiser", "transmission", "Underlying", "conditions", "Ventilators", "WHO", "xenophobia", "youTube", "zoonotic", "stay-at-home", "covid", "coronavirus", "hyrdoxychloroquine", "asymptomatic", "frontline", "virus", "self-isolation", "disinfectant", "shelter-in-place", "masks", "SARS-CoV-2", "ICU", "corona", "reopen", "distancing", "covering", "furlough", "tracer", "easing", "remdesivir", "mail-in", "hornet", "antibody", "in-person", "defund", "racism", "looting", "loot", "reopen", "two-metre", "pandemic", "looter", "distancing", "dexamethasone", "racial", "vaccine", "curfew","johnssons", "astrazeneca", "hospitals", "social-distance", "social-distancing", "police", "regulations", "symptoms", "testing", "positive-tests", "negative-tests" , "confirmed-cases", "restrictions", "deaths", "infected", "recoveries", "level", "jobs", "unemployed", "doctors", "infections", "sanitise", "sanitiser", "sanitisation", "containment")
#JUST tidy
media_agency_df <- media_agency_df %>%
mutate(text = gsub("?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text),
text = gsub("&amp", "", text),
text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text),
text = gsub("@\\w+", "", text),
text = gsub("[[:punct:]]", "", text),
text = gsub("[[:digit:]]", "", text),
text = gsub("http\\w+", "", text),
text = gsub("[ \t]{2,}", "", text),
text = gsub("^\\s+|\\s+$", "", text),
text = gsub("enca", "", text),
text = gsub("dstv", "", text),
text = gsub("sabcnews", "", text),
text = gsub("&amp", "", text)) %>%
mutate(text = str_replace_all(text," "," "),
text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
text = str_replace_all(text,"#[a-z,A-Z]*"," "),
text = str_replace_all(text,"@[a-z,A-Z]*"," ")) %>%
filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE)))
View(media_agency_df)
#JUST tidy
media_agency_df <- media_agency_df %>%
mutate(text = gsub("?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text),
text = gsub("&amp", "", text),
text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text),
text = gsub("@\\w+", "", text),
text = gsub("[[:punct:]]", "", text),
text = gsub("[[:digit:]]", "", text),
text = gsub("http\\w+", "", text),
text = gsub("[ \t]{2,}", "", text),
text = gsub("^\\s+|\\s+$", "", text),
text = gsub("enca", "", text),
text = gsub("dstv", "", text),
text = gsub("sabcnews", "", text),
text = gsub("&amp", "", text)) %>%
mutate(text = str_replace_all(text," "," "),
text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
text = str_replace_all(text,"#[a-z,A-Z]*"," "),
text = str_replace_all(text,"@[a-z,A-Z]*"," ")) %>%
filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE)))
media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
media_agency_df <- media_agency_df[ , colSums(is.na(media_agency_df)) < nrow(media_agency_df)]
#JUST tidy
media_agency_df <- media_agency_df %>%
mutate(text = gsub("?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text),
text = gsub("&amp", "", text),
text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text),
text = gsub("@\\w+", "", text),
text = gsub("[[:punct:]]", "", text),
text = gsub("[[:digit:]]", "", text),
text = gsub("http\\w+", "", text),
text = gsub("[ \t]{2,}", "", text),
text = gsub("^\\s+|\\s+$", "", text),
text = gsub("enca", "", text),
text = gsub("dstv", "", text),
text = gsub("sabcnews", "", text),
text = gsub("&amp", "", text)) %>%
mutate(text = str_replace_all(text," "," "),
text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
text = str_replace_all(text,"#[a-z,A-Z]*"," "),
text = str_replace_all(text,"@[a-z,A-Z]*"," ")) %>%
filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE)))
#JUST tidy
media_agency_df <- media_agency_df %>%
mutate(text = gsub("?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text),
text = gsub("&amp", "", text),
text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text),
text = gsub("@\\w+", "", text),
text = gsub("[[:punct:]]", "", text),
text = gsub("[[:digit:]]", "", text),
text = gsub("http\\w+", "", text),
text = gsub("[ \t]{2,}", "", text),
text = gsub("^\\s+|\\s+$", "", text),
text = gsub("&amp", "", text)) %>%
mutate(text = str_replace_all(text," "," "),
text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
text = str_replace_all(text,"#[a-z,A-Z]*"," "),
text = str_replace_all(text,"@[a-z,A-Z]*"," ")) %>%
filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE)))
media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
media_agency_df <- media_agency_df[ , colSums(is.na(media_agency_df)) < nrow(media_agency_df)]
#JUST tidy
media_agency_df <- media_agency_df %>%
mutate(text = gsub("?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text),
text = gsub("&amp", "", text),
text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text),
text = gsub("@\\w+", "", text),
text = gsub("[[:punct:]]", "", text),
text = gsub("[[:digit:]]", "", text),
text = gsub("http\\w+", "", text),
text = gsub("[ \t]{2,}", "", text),
text = gsub("^\\s+|\\s+$", "", text),
text = gsub("&amp", "", text)) %>%
mutate(text = str_replace_all(text," "," "),
text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
text = str_replace_all(text,"#[a-z,A-Z]*"," "),
text = str_replace_all(text,"@[a-z,A-Z]*"," "))
#JUST tidy
media_agency_df <- media_agency_df %>%
mutate(text = gsub("?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text),
text = gsub("&amp", "", text),
text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text),
text = gsub("@\\w+", "", text),
text = gsub("[[:punct:]]", "", text),
text = gsub("[[:digit:]]", "", text),
text = gsub("http\\w+", "", text),
text = gsub("[ \t]{2,}", "", text),
text = gsub("^\\s+|\\s+$", "", text),
text = gsub("&amp", "", text)) %>%
mutate(text = str_replace_all(text," "," "),
text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
text = str_replace_all(text,"#[a-z,A-Z]*"," "),
text = str_replace_all(text,"@[a-z,A-Z]*"," "))
?stop_words
?unnest_tokens
top_words <- tidy_media_df %>%
anti_join(stop_words) %>%
count(word) %>%
arrange(desc(n))
top_words %>%
slice(1:20) %>%
ggplot(aes(reorder(word, -n), n, fill = word)) +
geom_bar(stat="identity") +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 60, hjust = 1, size = 13),
plot.title = element_text(hjust = 0.5, size = 18)
) +
ylab("Frequency") +
xlab ("") +
ggtitle("Most frequent media agency tweets") +
guides(fill=FALSE)
media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
library(rtweet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(tidytext)
library(stringr)
library(scales)
library(readr)
library(lubridate)
library(vader)
library(topicmodels)
library(quanteda)
library(lubridate)
library(zoo)
library(plotly)
library(forcats)
library(igraph)
library(ggraph)
library(widyr)
media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
media_agency_df <- media_agency_df[ , colSums(is.na(media_agency_df)) < nrow(media_agency_df)]
#bigrams
tidy_bigram_df <- media_agency_df %>%
#filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
filter(!str_detect(text, "^RT")) %>%
mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
bigrams_separated <- tidy_bigram_df %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# new bigram counts:
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
#tf-idf bigrams
bigram_tf_idf <- bigrams_united %>%
count(screen_name, bigram) %>%
bind_tf_idf(bigram, screen_name, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf %>%
group_by(screen_name) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = screen_name)) +
geom_col(show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
#weights and graphs
bigram_graph <- bigram_counts %>%
filter(n > 40) %>%
graph_from_data_frame()
set.seed(1234)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
#bigrams
tidy_bigram_df <- media_agency_df %>%
#filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
filter(!str_detect(text, "^RT")) %>%
mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# new bigram counts:
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
#tf-idf bigrams
bigram_tf_idf <- bigrams_united %>%
count(screen_name, bigram) %>%
bind_tf_idf(bigram, screen_name, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf %>%
group_by(screen_name) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = screen_name)) +
geom_col(show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
#weights and graphs
bigram_graph <- bigram_counts %>%
filter(n > 40) %>%
graph_from_data_frame()
set.seed(1234)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
#bigrams
tidy_bigram_df <- media_agency_df %>%
#filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
filter(!str_detect(text, "^RT")) %>%
mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ")
library(rtweet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(tidytext)
library(stringr)
library(scales)
library(readr)
library(lubridate)
library(vader)
library(topicmodels)
library(quanteda)
library(lubridate)
library(zoo)
library(plotly)
library(forcats)
library(igraph)
library(ggraph)
library(widyr)
origop <- options("httr_oauth_cache")
options(httr_oauth_cache = TRUE)
api_key <- "5PgtS7ljq5ZbBoXnemU5qHe62"
api_secret <- "M44LeduQ4zoyDxQIkAFjeIJrpDhWnb5xASDvhahTlrAvOhN7fx"
access_token <- "743029724750942208-JLEp26XrjwvQ1CPJUXwvdUMLka82cgx"
access_secret <- "XRMeMBaOgQy2BC1Bd9iJARfMIyK40VKyII1ZRcf9nS0qd"
token <- create_token(
app = "KyleResearchApp",
consumer_key = api_key,
consumer_secret = api_secret,
access_token = access_token,
access_secret = access_secret
)
#bigrams
tidy_bigram_df <- media_agency_df %>%
#filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
filter(!str_detect(text, "^RT")) %>%
mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# new bigram counts:
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
#tf-idf bigrams
bigram_tf_idf <- bigrams_united %>%
count(screen_name, bigram) %>%
bind_tf_idf(bigram, screen_name, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf %>%
group_by(screen_name) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = screen_name)) +
geom_col(show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
#weights and graphs
bigram_graph <- bigram_counts %>%
filter(n > 40) %>%
graph_from_data_frame()
set.seed(1234)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1, repel = TRUE) +
theme_void()
#weights and graphs
bigram_graph <- bigram_counts %>%
filter(n > 50) %>%
graph_from_data_frame()
set.seed(1234)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1, repel = TRUE) +
theme_void()
#weights and graphs
bigram_graph <- bigram_counts %>%
filter(n > 100) %>%
graph_from_data_frame()
set.seed(1234)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1, repel = TRUE) +
theme_void()
AFINN <- get_sentiments("afinn")
not_words <- bigrams_separated %>%
filter(word1 == "not") %>%
inner_join(AFINN, by = c(word2 = "word")) %>%
count(word2, value, sort = TRUE)
not_words %>%
mutate(contribution = n * value) %>%
arrange(desc(abs(contribution))) %>%
head(20) %>%
mutate(word2 = reorder(word2, contribution)) %>%
ggplot(aes(n * value, word2, fill = n * value > 0)) +
geom_col(show.legend = FALSE) +
labs(x = "Sentiment value * number of occurrences",
y = "Words preceded by \"not\"")
#not words
VADER <- get_sentiments("VADER")
#not words
VADER <- vader::get_vader()
#not words
VADER <- vader::get_vader()
not_words <- bigrams_separated %>%
filter(word1 == "not") %>%
inner_join(VADER, by = c(word2 = "word")) %>%
count(word2, value, sort = TRUE)
install.packages("readtext")
library(readtext)
readtext(file = "data_in/vader_lexicon.txt")
#not words
#TOKEN, MEAN-SENTIMENT-RATING, STANDARD DEVIATION, and RAW-HUMAN-SENTIMENT-RATINGS
vader_lexicon <- readtext(file = "data_in/vader_lexicon.txt")
View(vader_lexicon)
read_csv("data_in/vader_lexicon.txt", col_names = c("TOKEN", "MEAN-SENTIMENT-RATING", "STANDARD DEVIATION", "RAW-HUMAN-SENTIMENT-RATINGS"))
