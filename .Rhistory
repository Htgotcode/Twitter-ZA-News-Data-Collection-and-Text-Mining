scale_x_date(date_breaks = "3 day", date_labels = "%m-%d") +
theme(
axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
panel.grid.minor.y = element_blank(),
panel.grid.minor.x = element_blank(),
)
#General sentiment over time. Media agencies seem to have an even number of positive and negative tweets per day.
ggplot(media_vader_df %>%
mutate(created_at = as.Date(created_at)) %>%
group_by(created_at, screen_name) %>%
mutate(compound = replace(compound, is.na(compound), 0)) %>%
summarise(sum_compound = sum(as.double(compound))), aes(created_at, sum_compound, fill = screen_name)) +
geom_bar(stat = "identity")+
geom_smooth(method = "lm", na.rm = TRUE, se = FALSE) +
facet_wrap(~screen_name, scales = "free_x", ncol = 2) +
theme_minimal()+
scale_y_continuous(expand = c(0,0)) +
scale_x_date(date_breaks = "3 day", date_labels = "%m-%d") +
theme(
axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
panel.grid.minor.y = element_blank(),
panel.grid.minor.x = element_blank(),
)
#tidy df and unnest
tidy_media_df <- media_agency_df %>%
#filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word,
!word %in% agency_stop_words$word,
!word %in% negated_words$word2,
!word %in% str_remove_all(stop_words$word, "'"),
str_detect(word, "[a-z]"))
vader_df <- vader_df(media_agency_df$text)
write_as_csv(vader_df, "data_in/vader")
vader_df <- read_csv("data_in/vader.csv")
vader_df <- vader_df %>% mutate("X1" = row_number())
media_agency_df <- media_agency_df %>% mutate("X1" = row_number())
media_vader_df <- media_agency_df %>% left_join(vader_df, by = "X1")
#General sentiment over time. Media agencies seem to have an even number of positive and negative tweets per day.
ggplot(media_vader_df %>%
mutate(created_at = as.Date(created_at)) %>%
group_by(created_at, screen_name) %>%
mutate(compound = replace(compound, is.na(compound), 0)) %>%
summarise(sum_compound = sum(as.double(compound))), aes(created_at, sum_compound, fill = screen_name)) +
geom_bar(stat = "identity")+
geom_smooth(method = "lm", na.rm = TRUE, se = FALSE) +
facet_wrap(~screen_name, scales = "free_x", ncol = 2) +
theme_minimal()+
scale_y_continuous(expand = c(0,0)) +
scale_x_date(date_breaks = "3 day", date_labels = "%m-%d") +
theme(
axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
panel.grid.minor.y = element_blank(),
panel.grid.minor.x = element_blank(),
)
#General sentiment over time. Media agencies seem to have an even number of positive and negative tweets per day.
ggplot(media_vader_df %>%
mutate(created_at = as.Date(created_at)) %>%
group_by(created_at, screen_name) %>%
mutate(compound = replace(compound, is.na(compound), 0)) %>%
summarise(sum_compound = sum(as.double(compound))), aes(created_at, sum_compound, fill = screen_name)) +
geom_bar(stat = "identity")+
geom_smooth(method = "lm", na.rm = TRUE, se = FALSE) +
facet_wrap(~screen_name, scales = "free_x", ncol = 2) +
theme_minimal()+
scale_y_continuous(expand = c(0,0)) +
scale_x_date(date_breaks = "3 day", date_labels = "%m-%d") +
theme(
axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
panel.grid.minor.y = element_blank(),
panel.grid.minor.x = element_blank(),
)
#General sentiment over time. Media agencies seem to have an even number of positive and negative tweets per day.
ggplot(media_vader_df %>%
mutate(created_at = as.Date(created_at)) %>%
group_by(created_at, screen_name) %>%
mutate(compound = replace(compound, is.na(compound), 0)) %>%
summarise(sum_compound = sum(as.double(compound))), aes(created_at, sum_compound, fill = screen_name)) +
geom_bar(stat = "identity")+
geom_smooth(method = "lm", na.rm = TRUE, se = FALSE) +
facet_wrap(~screen_name, scales = "free_x", ncol = 2) +
theme_minimal()+
scale_y_continuous(expand = c(0,0)) +
scale_x_date(date_breaks = "3 day", date_labels = "%m-%d") +
theme(
axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
panel.grid.minor.y = element_blank(),
panel.grid.minor.x = element_blank(),
)
media_agency_section_words <- media_agency_df %>%
mutate(section = row_number() %/% 10) %>%
filter(section > 0) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words$word)
# count words co-occuring within sections
word_pairs <- media_agency_section_words %>%
pairwise_count(word, section, sort = TRUE)
# we need to filter for at least relatively common words first
word_cors <- media_agency_section_words %>%
group_by(word) %>%
filter(n() >= 20) %>%
pairwise_cor(word, section, sort = TRUE)
# Correlation of next word
# word_cors %>%
#   filter(item1 == "vaccine")
word_cors %>%
filter(item1 %in% c("covid", "vaccine", "lockdown", "guateng")) %>%
group_by(item1) %>%
slice_max(correlation, n = 6) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
set.seed(1234)
word_cors %>%
filter(correlation > .7) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE, edge_width = 3) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
# count words co-occuring within sections
word_pairs <- tidy_media_df %>%
pairwise_count(word, section, sort = TRUE)
media_agency_section_words <- tidy_media_df %>%
mutate(section = row_number() %/% 10) %>%
filter(section > 0)
media_agency_section_words <- tidy_media_df %>%
mutate(section = row_number() %/% 10) %>%
filter(section > 0)
# count words co-occuring within sections
word_pairs <- media_agency_section_words %>%
pairwise_count(word, section, sort = TRUE)
# we need to filter for at least relatively common words first
word_cors <- media_agency_section_words %>%
group_by(word) %>%
filter(n() >= 20) %>%
pairwise_cor(word, section, sort = TRUE)
# Correlation of next word
# word_cors %>%
#   filter(item1 == "vaccine")
word_cors %>%
filter(item1 %in% c("covid", "vaccine", "lockdown", "guateng")) %>%
group_by(item1) %>%
slice_max(correlation, n = 6) %>%
ungroup() %>%
>>>>>>> Stashed changes
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
set.seed(1234)
word_cors %>%
filter(correlation > .7) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE, edge_width = 3) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors %>%
filter(correlation > .6) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE, edge_width = 3) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors %>%
filter(correlation > .65) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE, edge_width = 3) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors %>%
filter(item1 %in% c("covid", "vaccine", "lockdown", "guateng")) %>%
group_by(item1) %>%
slice_max(correlation, n = 6) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
word_cors %>%
filter(item1 %in% c("covid", "vaccine", "guateng", "guateng")) %>%
group_by(item1) %>%
slice_max(correlation, n = 6) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
lockdown
word_cors %>%
filter(item1 %in% c("covid", "vaccine", "lockdown", "guateng")) %>%
group_by(item1) %>%
slice_max(correlation, n = 6) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
View(tidy_media_df)
word_cors %>%
filter(item1 %in% c("covid", "vaccine", "lockdown", "ZUma")) %>%
group_by(item1) %>%
slice_max(correlation, n = 6) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
word_cors %>%
filter(item1 %in% c("covid", "vaccine", "lockdown", "zuma")) %>%
group_by(item1) %>%
slice_max(correlation, n = 6) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
word_cors %>%
filter(item1 %in% c("covid", "vaccine", "lockdown", "zuma", "gauteng")) %>%
group_by(item1) %>%
slice_max(correlation, n = 6) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
word_cors %>%
filter(item1 %in% c("covid", "vaccine", "lockdown", "zuma", "gauteng", "ramaphosa")) %>%
group_by(item1) %>%
slice_max(correlation, n = 6) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
library(rtweet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(tidytext)
library(stringr)
library(scales)
library(readr)
library(lubridate)
library(vader)
library(topicmodels)
library(quanteda)
library(lubridate)
library(zoo)
library(plotly)
library(forcats)
library(igraph)
library(ggraph)
library(widyr)
library(qdapRegex)
origop <- options("httr_oauth_cache")
options(httr_oauth_cache = TRUE)
api_key <- "5PgtS7ljq5ZbBoXnemU5qHe62"
api_secret <- "M44LeduQ4zoyDxQIkAFjeIJrpDhWnb5xASDvhahTlrAvOhN7fx"
access_token <- "743029724750942208-JLEp26XrjwvQ1CPJUXwvdUMLka82cgx"
access_secret <- "XRMeMBaOgQy2BC1Bd9iJARfMIyK40VKyII1ZRcf9nS0qd"
token <- create_token(
app = "KyleResearchApp",
consumer_key = api_key,
consumer_secret = api_secret,
access_token = access_token,
access_secret = access_secret
)
# vader_df <- vader_df(media_agency_df$text)
# write_as_csv(vader_df, "data_in/vader")
vader_df <- read_csv("data_in/vader.csv")
vader_df <- vader_df %>% mutate("X1" = row_number())
media_agency_df <- media_agency_df %>% mutate("X1" = row_number())
#created if more tweets could be accessed to be more topic specific
# covid_dictionary <- c("herd", "immunity", "incubation", "job", "loss", "Kits", "lockdown", "mask", "N95", "outbreak", "pandemic", "quarantine", "recovery", "sanitiser", "transmission", "Underlying", "conditions", "Ventilators", "WHO", "xenophobia", "youTube", "zoonotic", "stay-at-home", "covid", "coronavirus", "hyrdoxychloroquine", "asymptomatic", "frontline", "virus", "self-isolation", "disinfectant", "shelter-in-place", "masks", "SARS-CoV-2", "ICU", "corona", "reopen", "distancing", "covering", "furlough", "tracer", "easing", "remdesivir", "mail-in", "hornet", "antibody", "in-person", "defund", "racism", "looting", "loot", "reopen", "two-metre", "pandemic", "looter", "distancing", "dexamethasone", "racial", "vaccine", "curfew","johnssons", "astrazeneca", "hospitals", "social-distance", "social-distancing", "police", "regulations", "symptoms", "testing", "positive-tests", "negative-tests" , "confirmed-cases", "restrictions", "deaths", "infected", "recoveries", "level", "jobs", "unemployed", "doctors", "infections", "sanitise", "sanitiser", "sanitisation", "containment")
media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
media_agency_df <- media_agency_df[ , colSums(is.na(media_agency_df)) < nrow(media_agency_df)]
#JUST tidy
rm_twitter_n_url <- rm_(pattern=pastex("@rm_twitter_url", "@rm_url"))
media_agency_df <- media_agency_df %>%
mutate(text = rm_twitter_n_url(text),
text = gsub("rt", " ", text),
text = gsub("@\\w+", " ", text),
text = gsub("[[:punct:]]", " ", text),
text = gsub("[[:digit:]]", " ", text),
text = gsub("http\\w+", " ", text),
text = gsub("[ |\t]{2,}", " ", text),
text = gsub("^ ", " ", text),
text = gsub(" $", " ", text)) %>%
mutate(text = str_replace_all(text," "," "),
text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
text = str_replace_all(text,"#[a-z,A-Z]*"," "),
text = str_replace_all(text,"@[a-z,A-Z]*"," "))
#most stop words are filtered based on the media agencies tag at the beginning of each Tweet. eg. WATCH: *headline follows*.
agency_stop_words <- tibble(word = c("sabcnews", "enca", "dstv", "sabckzn", "maverick", "opinionista", "dm", "scorpio", "dstv403", "itus", "rt", "amp", "tgifood", "mamelodi", "sundowns", "ofmagazineavailable", "casablanca", "oped", "newsdeck", "editorial", "newflash", "southafricanmorning", "newslink", "encas", "southafricatonight", "themiddayview", "thelead", "propertymatters", "ba", "ka", "ya", "ga", "wa", "le", "kwa", "morninglivesabc", "monday", "prix", "azerbaijan", "encasis", "encabusiness", "encasspeaks", "south", "africa", "pm", "sa","pm", "encas", "iss", "icymi", "timeslive", "fullview", "newsbreaksjul", "newsbreakjul", "sabc", "nca", "ncas", "op", "ig", "pl", "news24", "news24s", "dm168", "pics"))
#tidy df and unnest
tidy_media_df <- media_agency_df %>%
#filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word,
!word %in% agency_stop_words$word,
!word %in% negated_words$word2,
!word %in% str_remove_all(stop_words$word, "'"),
str_detect(word, "[a-z]"))
#vader lexicon imported from VADER GitHub
vader_lexicon <- read_csv2("data_in/vader_lexicon.csv") %>%
rename("word" = TOKEN, "value" = `MEAN-SENTIMENT-RATING`)
#common negation words
negation_words <- c("not", "no", "never", "without", "no", "not", "none", "no one", "nobody", "nothing", "neither", "nowhere", "never", "doesn’t", "isn’t", "wasn’t", "shouldn’t", "wouldn’t", "couldn’t", "won’t", "can’t", "don’t")
negated_words <- bigrams_separated %>%
filter(word1 %in% fixed(negation_words, ignore_case = TRUE)) %>%
inner_join(vader_lexicon, by = c(word2 = "word")) %>%
mutate(value = as.double(value)) %>%
count(word1, word2, value, sort = TRUE) %>%
mutate(contribution = n * value) %>%
arrange(desc(abs(contribution))) %>%
mutate(word2 = reorder(word2, contribution))
#bigrams
tidy_bigram_df <- media_agency_df %>%
#filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
filter(!str_detect(text, "^RT")) %>%
mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
#bigrams
tidy_bigram_df <- media_agency_df %>%
#filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
filter(!str_detect(text, "^RT")) %>%
mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- tidy_bigram_df %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# new bigram counts:
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
#tf-idf bigrams
bigram_tf_idf <- bigrams_united %>%
count(screen_name, bigram) %>%
bind_tf_idf(bigram, screen_name, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf %>%
group_by(screen_name) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = screen_name)) +
geom_col(show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
#weights and graphs
bigram_graph <- bigram_counts %>%
filter(n > 100) %>%
graph_from_data_frame()
set.seed(1234)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1, repel = TRUE) +
theme_void()
#vader lexicon imported from VADER GitHub
vader_lexicon <- read_csv2("data_in/vader_lexicon.csv") %>%
rename("word" = TOKEN, "value" = `MEAN-SENTIMENT-RATING`)
#common negation words
negation_words <- c("not", "no", "never", "without", "no", "not", "none", "no one", "nobody", "nothing", "neither", "nowhere", "never", "doesn’t", "isn’t", "wasn’t", "shouldn’t", "wouldn’t", "couldn’t", "won’t", "can’t", "don’t")
negated_words <- bigrams_separated %>%
filter(word1 %in% fixed(negation_words, ignore_case = TRUE)) %>%
inner_join(vader_lexicon, by = c(word2 = "word")) %>%
mutate(value = as.double(value)) %>%
count(word1, word2, value, sort = TRUE) %>%
mutate(contribution = n * value) %>%
arrange(desc(abs(contribution))) %>%
mutate(word2 = reorder(word2, contribution))
#vader lexicon imported from VADER GitHub
vader_lexicon <- read_csv2("data_in/vader_lexicon.csv") %>%
rename("word" = TOKEN, "value" = `MEAN-SENTIMENT-RATING`)
#common negation words
negation_words <- c("not", "no", "never", "without", "no", "not", "none", "no one", "nobody", "nothing", "neither", "nowhere", "never", "doesn’t", "isn’t", "wasn’t", "shouldn’t", "wouldn’t", "couldn’t", "won’t", "can’t", "don’t")
negated_words <- tidy_bigram_df %>%
filter(word1 %in% fixed(negation_words, ignore_case = TRUE)) %>%
inner_join(vader_lexicon, by = c(word2 = "word")) %>%
mutate(value = as.double(value)) %>%
count(word1, word2, value, sort = TRUE) %>%
mutate(contribution = n * value) %>%
arrange(desc(abs(contribution))) %>%
mutate(word2 = reorder(word2, contribution))
negated_words %>%
head(40) %>%
ggplot(aes(n * value, word2, fill = n * value > 0)) +
geom_col(show.legend = FALSE) +
facet_wrap(~word1,scales = "free_y") +
labs(x = "Sentiment value * # number of occurrences",
y = "Words preceded by negation terms")
#created if more tweets could be accessed to be more topic specific
# covid_dictionary <- c("herd", "immunity", "incubation", "job", "loss", "Kits", "lockdown", "mask", "N95", "outbreak", "pandemic", "quarantine", "recovery", "sanitiser", "transmission", "Underlying", "conditions", "Ventilators", "WHO", "xenophobia", "youTube", "zoonotic", "stay-at-home", "covid", "coronavirus", "hyrdoxychloroquine", "asymptomatic", "frontline", "virus", "self-isolation", "disinfectant", "shelter-in-place", "masks", "SARS-CoV-2", "ICU", "corona", "reopen", "distancing", "covering", "furlough", "tracer", "easing", "remdesivir", "mail-in", "hornet", "antibody", "in-person", "defund", "racism", "looting", "loot", "reopen", "two-metre", "pandemic", "looter", "distancing", "dexamethasone", "racial", "vaccine", "curfew","johnssons", "astrazeneca", "hospitals", "social-distance", "social-distancing", "police", "regulations", "symptoms", "testing", "positive-tests", "negative-tests" , "confirmed-cases", "restrictions", "deaths", "infected", "recoveries", "level", "jobs", "unemployed", "doctors", "infections", "sanitise", "sanitiser", "sanitisation", "containment")
media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
media_agency_df <- media_agency_df[ , colSums(is.na(media_agency_df)) < nrow(media_agency_df)]
#JUST tidy
rm_twitter_n_url <- rm_(pattern=pastex("@rm_twitter_url", "@rm_url"))
media_agency_df <- media_agency_df %>%
mutate(text = rm_twitter_n_url(text),
text = gsub("rt", " ", text),
text = gsub("@\\w+", " ", text),
text = gsub("[[:punct:]]", " ", text),
text = gsub("[[:digit:]]", " ", text),
text = gsub("http\\w+", " ", text),
text = gsub("[ |\t]{2,}", " ", text),
text = gsub("^ ", " ", text),
text = gsub(" $", " ", text)) %>%
mutate(text = str_replace_all(text," "," "),
text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
text = str_replace_all(text,"#[a-z,A-Z]*"," "),
text = str_replace_all(text,"@[a-z,A-Z]*"," "))
#most stop words are filtered based on the media agencies tag at the beginning of each Tweet. eg. WATCH: *headline follows*.
agency_stop_words <- tibble(word = c("sabcnews", "enca", "dstv", "sabckzn", "maverick", "opinionista", "dm", "scorpio", "dstv403", "itus", "rt", "amp", "tgifood", "mamelodi", "sundowns", "ofmagazineavailable", "casablanca", "oped", "newsdeck", "editorial", "newflash", "southafricanmorning", "newslink", "encas", "southafricatonight", "themiddayview", "thelead", "propertymatters", "ba", "ka", "ya", "ga", "wa", "le", "kwa", "morninglivesabc", "monday", "prix", "azerbaijan", "encasis", "encabusiness", "encasspeaks", "south", "africa", "pm", "sa","pm", "encas", "iss", "icymi", "timeslive", "fullview", "newsbreaksjul", "newsbreakjul", "sabc", "nca", "ncas", "op", "ig", "pl", "news24", "news24s", "dm168", "pics"))
#tidy df and unnest
tidy_media_df <- media_agency_df %>%
#filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word,
!word %in% agency_stop_words$word,
!word %in% negated_words$word2,
!word %in% str_remove_all(stop_words$word, "'"),
str_detect(word, "[a-z]"))
#bigrams
tidy_bigram_df <- media_agency_df %>%
#filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
filter(!str_detect(text, "^RT")) %>%
mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- tidy_bigram_df %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# new bigram counts:
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
#tf-idf bigrams
bigram_tf_idf <- bigrams_united %>%
count(screen_name, bigram) %>%
bind_tf_idf(bigram, screen_name, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf %>%
group_by(screen_name) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = screen_name)) +
geom_col(show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
#weights and graphs
bigram_graph <- bigram_counts %>%
filter(n > 100) %>%
graph_from_data_frame()
set.seed(1234)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1, repel = TRUE) +
theme_void()
#vader lexicon imported from VADER GitHub
vader_lexicon <- read_csv2("data_in/vader_lexicon.csv") %>%
rename("word" = TOKEN, "value" = `MEAN-SENTIMENT-RATING`)
#common negation words
negation_words <- c("not", "no", "never", "without", "no", "not", "none", "no one", "nobody", "nothing", "neither", "nowhere", "never", "doesn’t", "isn’t", "wasn’t", "shouldn’t", "wouldn’t", "couldn’t", "won’t", "can’t", "don’t")
negated_words <- tidy_bigram_df %>%
filter(word1 %in% fixed(negation_words, ignore_case = TRUE)) %>%
inner_join(vader_lexicon, by = c(word2 = "word")) %>%
mutate(value = as.double(value)) %>%
count(word1, word2, value, sort = TRUE) %>%
mutate(contribution = n * value) %>%
arrange(desc(abs(contribution))) %>%
mutate(word2 = reorder(word2, contribution))
negated_words %>%
head(40) %>%
ggplot(aes(n * value, word2, fill = n * value > 0)) +
geom_col(show.legend = FALSE) +
facet_wrap(~word1,scales = "free_y") +
labs(x = "Sentiment value * # number of occurrences",
y = "Words preceded by negation terms")
