labs(x = "tf-idf", y = NULL)
bigrams_separated %>%
filter(word1 == "not") %>%
count(word1, word2, sort = TRUE)
#trigrams
tidy_trigram_df <- media_agency_df %>%
unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word,
!word3 %in% stop_words$word) %>%
count(word1, word2, word3, sort = TRUE) %>%
group_by(screen_name) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(trigram, tf_idf), fill = screen_name)) +
geom_col(show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
r
# new trigrams counts:
trigram_counts <- tidy_trigram_df %>%
count(word1, word2, word3, sort = TRUE)
trigrams_united <- tidy_trigram_df %>%
unite(trigram, word1, word2, word3, sep = " ")
#tf-idf trigrams
tidy_trigram_df <- trigrams_united %>%
count(screen_name, trigram) %>%
bind_tf_idf(trigram, screen_name, n) %>%
arrange(desc(tf_idf))
#tf-idf trigrams
tidy_trigram_df <- trigrams_united %>%
count(screen_name, trigram) %>%
bind_tf_idf(trigram, screen_name, n) %>%
arrange(desc(tf_idf))
# new trigrams counts:
trigram_counts <- tidy_trigram_df %>%
count(word1, word2, word3, sort = TRUE)
# new trigrams counts:
trigram_counts <- tidy_trigram_df %>%
count(word1, word2, word3, sort = TRUE)
trigrams_united <- tidy_trigram_df %>%
unite(trigram, word1, word2, word3, sep = " ")
#tf-idf trigrams
tidy_trigram_df <- trigrams_united %>%
count(screen_name, trigram) %>%
bind_tf_idf(trigram, screen_name, n) %>%
arrange(desc(tf_idf))
tidy_trigram_df %>%
group_by(screen_name) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(trigram, tf_idf), fill = screen_name)) +
geom_col(show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
#trigrams
tidy_trigram_df <- media_agency_df %>%
unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word,
!word3 %in% stop_words$word) %>%
count(word1, word2, word3, sort = TRUE)
#trigrams
tidy_trigram_df <- media_agency_df %>%
unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word,
!word3 %in% stop_words$word) %>%
count(word1, word2, word3, sort = TRUE)
# new trigrams counts:
trigram_counts <- tidy_trigram_df %>%
count(word1, word2, word3, sort = TRUE)
View(tidy_trigram_df)
#trigrams
tidy_trigram_df <- media_agency_df %>%
unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word,
!word3 %in% stop_words$word) %>%
count(word1, word2, word3, sort = TRUE)
View(tidy_trigram_df)
View(tidy_bigram_df)
#bigrams
tidy_bigram_df <- media_agency_df %>%
#filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
filter(!str_detect(text, "^RT")) %>%
mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
#trigrams
tidy_trigram_df <- media_agency_df %>%
filter(!str_detect(text, "^RT")) %>%
mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word,
!word3 %in% stop_words$word) %>%
count(word1, word2, word3, sort = TRUE)
#trigrams
tidy_trigram_df <- media_agency_df %>%
select(everything()) %>%
filter(!str_detect(text, "^RT")) %>%
mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word,
!word3 %in% stop_words$word) %>%
count(word1, word2, word3, sort = TRUE)
#trigrams
tidy_trigram_df <- media_agency_df %>%
filter(!str_detect(text, "^RT")) %>%
mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
trigrams_separated <- tidy_trigram_df %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ")
#trigrams
tidy_trigram_df <- media_agency_df %>%
filter(!str_detect(text, "^RT")) %>%
mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
unnest_tokens(trigram, text, token = "ngrams", n = 3)
trigrams_separated <- tidy_trigram_df %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ")
trigrams_separated <- tidy_trigram_df %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ")
trigrams_filtered <- trigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word3 %in% stop_words$word)
# new trigrams counts:
trigram_counts <- tidy_trigram_df %>%
count(word1, word2, word3, sort = TRUE)
# new trigrams counts:
trigram_counts <- trigrams_filtered %>%
count(word1, word2, word3, sort = TRUE)
trigrams_united <- trigrams_filtered %>%
unite(trigram, word1, word2, word3, sep = " ")
#tf-idf trigrams
tidy_trigram_df <- trigrams_united %>%
count(screen_name, trigram) %>%
bind_tf_idf(trigram, screen_name, n) %>%
arrange(desc(tf_idf))
tidy_trigram_df %>%
group_by(screen_name) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(trigram, tf_idf), fill = screen_name)) +
geom_col(show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
tidy_trigram_df %>%
group_by(screen_name) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(trigram, tf_idf), fill = screen_name)) +
geom_col(show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
media_agency_section_words <- media_agency_df %>%
mutate(section = row_number() %/% 10) %>%
filter(section > 0) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words$word)
media_agency_section_words
install.packages("widyr")
library(widyr)
word_pairs <- media_agency_section_words %>%
pairwise_count(word, section, sort = TRUE)
word_pairs <- media_agency_section_words %>%
pairwise_count(word, section, sort = TRUE)
word_pairs
# we need to filter for at least relatively common words first
word_cors <- media_agency_section_words %>%
group_by(word) %>%
filter(n() >= 20) %>%
pairwise_cor(word, section, sort = TRUE)
# we need to filter for at least relatively common words first
word_cors <- media_agency_section_words %>%
group_by(word) %>%
filter(n() >= 20) %>%
pairwise_cor(word, section, sort = TRUE)
word_cors
word_cors %>%
filter(item1 == "covid")
word_cors %>%
filter(item1 == "vaccine")
w
word_cors %>%
filter(item1 %in% c("covid", "vaccine", "lockdown", "guateng")) %>%
group_by(item1) %>%
slice_max(correlation, n = 6) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip()
set.seed(1234)
word_cors %>%
filter(correlation > .15) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors
word_cors %>%
filter(correlation > .9) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors %>%
filter(correlation > .5) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors %>%
filter(correlation > .6) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors %>%
filter(correlation > .7) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
plot <- media_vader_df %>% mutate(created_at = as.Date(created_at)) %>%  group_by(created_at) %>% mutate(retweet_daily = mean(retweet_count), favorite_daily = mean(favorite_count), total = mean(favorite_count + retweet_count)) %>%
ggplot(aes(created_at)) +
geom_ribbon(aes(ymin = 0, ymax = total, fill = "Total")) +
geom_ribbon(aes(ymin = 0, ymax = favorite_daily, fill = "Favorites")) +
geom_ribbon(aes(ymin = 0, ymax = retweet_daily, fill = "Retweets"))
ggplotly(plot)
ggplot(media_agency_df, aes(x = created_at, fill = screen_name)) +
geom_histogram(position = "identity", bins = 20, show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 1)
library(rtweet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(tidytext)
library(stringr)
library(scales)
library(readr)
library(lubridate)
library(vader)
library(topicmodels)
library(quanteda)
library(lubridate)
library(zoo)
library(plotly)
library(forcats)
library(igraph)
library(ggraph)
library(widyr)
ggplot(media_agency_df, aes(x = created_at, fill = screen_name)) +
geom_histogram(position = "identity", bins = 20, show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 1)
covid_dictionary <- c("herd", "immunity", "incubation", "job", "loss", "Kits", "lockdown", "mask", "N95", "outbreak", "pandemic", "quarantine", "recovery", "sanitiser", "transmission", "Underlying", "conditions", "Ventilators", "WHO", "xenophobia", "youTube", "zoonotic", "stay-at-home", "covid", "coronavirus", "hyrdoxychloroquine", "asymptomatic", "frontline", "virus", "self-isolation", "disinfectant", "shelter-in-place", "masks", "SARS-CoV-2", "ICU", "corona", "reopen", "distancing", "covering", "furlough", "tracer", "easing", "remdesivir", "mail-in", "hornet", "antibody", "in-person", "defund", "racism", "looting", "loot", "reopen", "two-metre", "pandemic", "looter", "distancing", "dexamethasone", "racial", "vaccine", "curfew","johnssons", "astrazeneca", "hospitals", "social-distance", "social-distancing", "police", "regulations", "symptoms", "testing", "positive-tests", "negative-tests" , "confirmed-cases", "restrictions", "deaths", "infected", "recoveries", "level", "jobs", "unemployed", "doctors", "infections", "sanitise", "sanitiser", "sanitisation", "containment")
covid_dictionary <- c("herd", "immunity", "incubation", "job", "loss", "Kits", "lockdown", "mask", "N95", "outbreak", "pandemic", "quarantine", "recovery", "sanitiser", "transmission", "Underlying", "conditions", "Ventilators", "WHO", "xenophobia", "youTube", "zoonotic", "stay-at-home", "covid", "coronavirus", "hyrdoxychloroquine", "asymptomatic", "frontline", "virus", "self-isolation", "disinfectant", "shelter-in-place", "masks", "SARS-CoV-2", "ICU", "corona", "reopen", "distancing", "covering", "furlough", "tracer", "easing", "remdesivir", "mail-in", "hornet", "antibody", "in-person", "defund", "racism", "looting", "loot", "reopen", "two-metre", "pandemic", "looter", "distancing", "dexamethasone", "racial", "vaccine", "curfew","johnssons", "astrazeneca", "hospitals", "social-distance", "social-distancing", "police", "regulations", "symptoms", "testing", "positive-tests", "negative-tests" , "confirmed-cases", "restrictions", "deaths", "infected", "recoveries", "level", "jobs", "unemployed", "doctors", "infections", "sanitise", "sanitiser", "sanitisation", "containment")
#JUST tidy
media_agency_df <- media_agency_df %>%
mutate(text = gsub("?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text),
text = gsub("&amp", "", text),
text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text),
text = gsub("@\\w+", "", text),
text = gsub("[[:punct:]]", "", text),
text = gsub("[[:digit:]]", "", text),
text = gsub("http\\w+", "", text),
text = gsub("[ \t]{2,}", "", text),
text = gsub("^\\s+|\\s+$", "", text),
text = gsub("enca", "", text),
text = gsub("dstv", "", text),
text = gsub("sabcnews", "", text),
text = gsub("&amp", "", text)) %>%
mutate(text = str_replace_all(text," "," "),
text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
text = str_replace_all(text,"#[a-z,A-Z]*"," "),
text = str_replace_all(text,"@[a-z,A-Z]*"," ")) %>%
filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE)))
View(media_agency_df)
#JUST tidy
media_agency_df <- media_agency_df %>%
mutate(text = gsub("?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text),
text = gsub("&amp", "", text),
text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text),
text = gsub("@\\w+", "", text),
text = gsub("[[:punct:]]", "", text),
text = gsub("[[:digit:]]", "", text),
text = gsub("http\\w+", "", text),
text = gsub("[ \t]{2,}", "", text),
text = gsub("^\\s+|\\s+$", "", text),
text = gsub("enca", "", text),
text = gsub("dstv", "", text),
text = gsub("sabcnews", "", text),
text = gsub("&amp", "", text)) %>%
mutate(text = str_replace_all(text," "," "),
text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
text = str_replace_all(text,"#[a-z,A-Z]*"," "),
text = str_replace_all(text,"@[a-z,A-Z]*"," ")) %>%
filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE)))
media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
media_agency_df <- media_agency_df[ , colSums(is.na(media_agency_df)) < nrow(media_agency_df)]
#JUST tidy
media_agency_df <- media_agency_df %>%
mutate(text = gsub("?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text),
text = gsub("&amp", "", text),
text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text),
text = gsub("@\\w+", "", text),
text = gsub("[[:punct:]]", "", text),
text = gsub("[[:digit:]]", "", text),
text = gsub("http\\w+", "", text),
text = gsub("[ \t]{2,}", "", text),
text = gsub("^\\s+|\\s+$", "", text),
text = gsub("enca", "", text),
text = gsub("dstv", "", text),
text = gsub("sabcnews", "", text),
text = gsub("&amp", "", text)) %>%
mutate(text = str_replace_all(text," "," "),
text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
text = str_replace_all(text,"#[a-z,A-Z]*"," "),
text = str_replace_all(text,"@[a-z,A-Z]*"," ")) %>%
filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE)))
#JUST tidy
media_agency_df <- media_agency_df %>%
mutate(text = gsub("?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text),
text = gsub("&amp", "", text),
text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text),
text = gsub("@\\w+", "", text),
text = gsub("[[:punct:]]", "", text),
text = gsub("[[:digit:]]", "", text),
text = gsub("http\\w+", "", text),
text = gsub("[ \t]{2,}", "", text),
text = gsub("^\\s+|\\s+$", "", text),
text = gsub("&amp", "", text)) %>%
mutate(text = str_replace_all(text," "," "),
text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
text = str_replace_all(text,"#[a-z,A-Z]*"," "),
text = str_replace_all(text,"@[a-z,A-Z]*"," ")) %>%
filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE)))
media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
media_agency_df <- media_agency_df[ , colSums(is.na(media_agency_df)) < nrow(media_agency_df)]
#JUST tidy
media_agency_df <- media_agency_df %>%
mutate(text = gsub("?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text),
text = gsub("&amp", "", text),
text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text),
text = gsub("@\\w+", "", text),
text = gsub("[[:punct:]]", "", text),
text = gsub("[[:digit:]]", "", text),
text = gsub("http\\w+", "", text),
text = gsub("[ \t]{2,}", "", text),
text = gsub("^\\s+|\\s+$", "", text),
text = gsub("&amp", "", text)) %>%
mutate(text = str_replace_all(text," "," "),
text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
text = str_replace_all(text,"#[a-z,A-Z]*"," "),
text = str_replace_all(text,"@[a-z,A-Z]*"," "))
#JUST tidy
media_agency_df <- media_agency_df %>%
mutate(text = gsub("?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text),
text = gsub("&amp", "", text),
text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text),
text = gsub("@\\w+", "", text),
text = gsub("[[:punct:]]", "", text),
text = gsub("[[:digit:]]", "", text),
text = gsub("http\\w+", "", text),
text = gsub("[ \t]{2,}", "", text),
text = gsub("^\\s+|\\s+$", "", text),
text = gsub("&amp", "", text)) %>%
mutate(text = str_replace_all(text," "," "),
text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
text = str_replace_all(text,"#[a-z,A-Z]*"," "),
text = str_replace_all(text,"@[a-z,A-Z]*"," "))
?stop_words
?unnest_tokens
top_words <- tidy_media_df %>%
anti_join(stop_words) %>%
count(word) %>%
arrange(desc(n))
top_words %>%
slice(1:20) %>%
ggplot(aes(reorder(word, -n), n, fill = word)) +
geom_bar(stat="identity") +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 60, hjust = 1, size = 13),
plot.title = element_text(hjust = 0.5, size = 18)
) +
ylab("Frequency") +
xlab ("") +
ggtitle("Most frequent media agency tweets") +
guides(fill=FALSE)
media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
library(rtweet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(tidytext)
library(stringr)
library(scales)
library(readr)
library(lubridate)
library(vader)
library(topicmodels)
library(quanteda)
library(lubridate)
library(zoo)
library(plotly)
library(forcats)
library(igraph)
library(ggraph)
library(widyr)
media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
media_agency_df <- media_agency_df[ , colSums(is.na(media_agency_df)) < nrow(media_agency_df)]
#bigrams
tidy_bigram_df <- media_agency_df %>%
#filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
filter(!str_detect(text, "^RT")) %>%
mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
bigrams_separated <- tidy_bigram_df %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# new bigram counts:
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
#tf-idf bigrams
bigram_tf_idf <- bigrams_united %>%
count(screen_name, bigram) %>%
bind_tf_idf(bigram, screen_name, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf %>%
group_by(screen_name) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = screen_name)) +
geom_col(show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
#weights and graphs
bigram_graph <- bigram_counts %>%
filter(n > 40) %>%
graph_from_data_frame()
set.seed(1234)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
#bigrams
tidy_bigram_df <- media_agency_df %>%
#filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
filter(!str_detect(text, "^RT")) %>%
mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# new bigram counts:
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
#tf-idf bigrams
bigram_tf_idf <- bigrams_united %>%
count(screen_name, bigram) %>%
bind_tf_idf(bigram, screen_name, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf %>%
group_by(screen_name) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = screen_name)) +
geom_col(show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
#weights and graphs
bigram_graph <- bigram_counts %>%
filter(n > 40) %>%
graph_from_data_frame()
set.seed(1234)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
