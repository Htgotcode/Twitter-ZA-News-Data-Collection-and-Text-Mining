---
title: "R Notebook"
output: html_notebook
---

What tweets got the most attention.
frequency of tweets posted on a topic


```{r}
library(rtweet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(tidytext)
library(stringr)
library(scales)
library(readr)
library(lubridate)
library(vader)
library(topicmodels)
library(quanteda)
library(lubridate)
library(zoo)
library(plotly)
library(forcats)
library(igraph)
library(ggraph)
library(widyr)

origop <- options("httr_oauth_cache")
options(httr_oauth_cache = TRUE)

api_key <- "5PgtS7ljq5ZbBoXnemU5qHe62"
api_secret <- "M44LeduQ4zoyDxQIkAFjeIJrpDhWnb5xASDvhahTlrAvOhN7fx"
access_token <- "743029724750942208-JLEp26XrjwvQ1CPJUXwvdUMLka82cgx"
access_secret <- "XRMeMBaOgQy2BC1Bd9iJARfMIyK40VKyII1ZRcf9nS0qd"
token <- create_token(
  app = "KyleResearchApp",
  consumer_key = api_key,
  consumer_secret = api_secret,
  access_token = access_token,
  access_secret = access_secret
)
```

```{r Fetching Tweets}
media_agency_df <- get_timeline("News24", n = 3200)
media_agency_df <- media_agency_df %>% 
  bind_rows(get_timeline("eNCA", n = 3200)) %>% 
  bind_rows(get_timeline("TimesLIVE", n = 3200)) %>% 
  bind_rows(get_timeline("SABCNews", n = 3200)) %>% 
  bind_rows(get_timeline("dailymaverick", n = 3200))

write_as_csv(media_agency_df, "data_in/media_agency_tweets")
media_agency_df <- read_csv("data_in/media_agency_tweets.csv")

media_agency_df <- media_agency_df[ , colSums(is.na(media_agency_df)) < nrow(media_agency_df)]
```


# Comparing Tweet Archives

Some tweets fetched date further back. The 3200 tweet pull per user causes this. It tells us that agencies like News24 and SABC News Tweet more daily than Daily Maverick.
```{r}
ggplot(media_agency_df, aes(x = created_at, fill = screen_name)) +
  geom_histogram(position = "identity", bins = 20, show.legend = FALSE) +
  facet_wrap(~screen_name, ncol = 1)
```

We first clean the original text to remove links, punctuation, digits, links, @'s.
Then we tokenize the Tweets and remove stop words from tidytext and our own stopword dictionary. We also use the "twitter" token to handle any left over @'s and URLS.
```{r Clean up}

covid_dictionary <- c("herd", "immunity", "incubation", "job", "loss", "Kits", "lockdown", "mask", "N95", "outbreak", "pandemic", "quarantine", "recovery", "sanitiser", "transmission", "Underlying", "conditions", "Ventilators", "WHO", "xenophobia", "youTube", "zoonotic", "stay-at-home", "covid", "coronavirus", "hyrdoxychloroquine", "asymptomatic", "frontline", "virus", "self-isolation", "disinfectant", "shelter-in-place", "masks", "SARS-CoV-2", "ICU", "corona", "reopen", "distancing", "covering", "furlough", "tracer", "easing", "remdesivir", "mail-in", "hornet", "antibody", "in-person", "defund", "racism", "looting", "loot", "reopen", "two-metre", "pandemic", "looter", "distancing", "dexamethasone", "racial", "vaccine", "curfew","johnssons", "astrazeneca", "hospitals", "social-distance", "social-distancing", "police", "regulations", "symptoms", "testing", "positive-tests", "negative-tests" , "confirmed-cases", "restrictions", "deaths", "infected", "recoveries", "level", "jobs", "unemployed", "doctors", "infections", "sanitise", "sanitiser", "sanitisation", "containment")

#JUST tidy
media_agency_df <- media_agency_df %>% 
  mutate(text = gsub("?(f|ht)tp(s?)://(.*)[.][a-z]+", "", text),
         text = gsub("&amp", "", text),
         text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text),
         text = gsub("@\\w+", "", text),
         text = gsub("[[:punct:]]", "", text),
         text = gsub("[[:digit:]]", "", text),
         text = gsub("[ \t]{2,}", "", text),
         text = gsub("^\\s+|\\s+$", "", text),
         text = gsub("&amp", "", text)) %>% 
  mutate(text = str_replace_all(text," "," "),
         text = str_replace_all(text,"RT @[a-z,A-Z]*: "," "),
         text = str_replace_all(text,"#[a-z,A-Z]*"," "),
         text = str_replace_all(text,"@[a-z,A-Z]*"," "))  
#  filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) 

mystopwords <- tibble(word = c("sabcnews", "enca", "https", "dstv", "sabckzn", "maverick", "opinion", "opinionista", "dm", "scorpio", "anc", "south", "africa", "dstv403", "t.co", "itus", "rt", "amp", "tgifood", "mamelodi", "sundowns", "ofmagazineavailable", "casablanca", "oped", "newsdeck", "editorial", "newflash", "southafricanmorning", "newslink", "encas", "southafricatonight", "themiddayview", "thelead", "propertymatters", "ba", "ka", "ya", "ga", "wa", "le", "kwa", "morninglivesabc", "davis", "monday", "livezuma", "prix", "azerbaijan", "iss", "assassin", "preprison", "diaries", "encasis", "moyane", "encabusiness","withon", "boresetse", "encasspeaks", "buisana", "sponsoredthe", "jub", "liesl"))  

#tidy df and unnest
tidy_media_df <- media_agency_df %>% 
  #filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
  filter(!str_detect(text, "^RT")) %>%
  unnest_tokens(word, text, token = "tweets") %>% 
  filter(!word %in% stop_words$word,
         !word %in% mystopwords$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"))

```

From our tidyed Tweet dataset, we look for the top words that appear. This will give us a good idea of what topics are being discussed the most. We find that COVID-19 has been the main topic of discussion. President appears second as President Ramaphosa of South Africa usually adresses the nation regarding COVID-19 information. Additionally, Zuma also appears as he is mentioned as "former president Zuma" in most articles. Zuma appears more as his recent court avoidance and sentencing is being Tweeted. General words surrounding the COVID-19 topic as it is still the main pressure on the country, espicially involving Gauteng's rise in infections. 
```{r top words}
#top words
top_words <- tidy_media_df %>% 
  anti_join(stop_words) %>% 
  count(word) %>% 
  arrange(desc(n))
top_words %>%
  slice(1:20) %>%
  ggplot(aes(reorder(word, -n), n, fill = word)) +
  geom_bar(stat="identity") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 60, hjust = 1, size = 13),
    plot.title = element_text(hjust = 0.5, size = 18)
    ) +
  ylab("Frequency") +
  xlab ("") +
  ggtitle("Most frequent media agency tweets") +
  guides(fill=FALSE)
```

gh
```{r tf-idf frequency per agency}
#tf-idf
tidy_media_tf_idf <- tidy_media_df %>% 
  select(screen_name, word) %>% 
  count(word, screen_name) %>% 
  bind_tf_idf(word, screen_name, n) 

#calculate a frequency for each agency and word
frequency <- tidy_media_tf_idf %>% 
  group_by(screen_name) %>% 
  count(word, sort = TRUE) %>% 
  left_join(tidy_media_df %>% 
              group_by(screen_name) %>% 
              summarise(total = n())) %>%
  mutate(freq = n/total)


tidy_media_tf_idf %>%
  group_by(screen_name) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~screen_name, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

By modelling bigrams and trigrams for our dataset gives us a better understanding of what topic is being discussed with each word. 
```{r bigrams}
#bigrams
tidy_bigram_df <- media_agency_df %>% 
  #filter(str_detect(text, fixed(covid_dictionary, ignore_case = TRUE))) %>%
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

#tf-idf bigrams
bigram_tf_idf <- bigrams_united %>%
  count(screen_name, bigram) %>%
  bind_tf_idf(bigram, screen_name, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf %>%
  group_by(screen_name) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~screen_name, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

#weights and graphs
bigram_graph <- bigram_counts %>%
  filter(n > 40) %>%
  graph_from_data_frame()

set.seed(1234)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```


```{r not words}
#not words
AFINN <- get_sentiments("afinn")
not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE)
not_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * number of occurrences",
       y = "Words preceded by \"not\"")

negation_words <- c("not", "no", "never", "without")
negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)
negated_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~word1) +
  labs(x = "Sentiment value * # number of occurrences",
       y = "Words preceded by negation terms")
```


```{r trigrams}
#trigrams
tidy_trigram_df <- media_agency_df %>%
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

trigrams_filtered <- trigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word3 %in% stop_words$word) 

# new trigrams counts:
trigram_counts <- trigrams_filtered %>% 
  count(word1, word2, word3, sort = TRUE)

trigrams_united <- trigrams_filtered %>%
  unite(trigram, word1, word2, word3, sep = " ")

#tf-idf trigrams
tidy_trigram_df <- trigrams_united %>%
  count(screen_name, trigram) %>%
  bind_tf_idf(trigram, screen_name, n) %>%
  arrange(desc(tf_idf))

tidy_trigram_df %>%
  group_by(screen_name) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(trigram, tf_idf), fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~screen_name, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

```

```{r Counting and correlating among sections}
media_agency_section_words <- media_agency_df %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0) %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word)

media_agency_section_words

# count words co-occuring within sections
word_pairs <- media_agency_section_words %>%
  pairwise_count(word, section, sort = TRUE)

word_pairs

# we need to filter for at least relatively common words first
word_cors <- media_agency_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

word_cors
# Correlation of next word
word_cors %>%
  filter(item1 == "vaccine")

word_cors %>%
  filter(item1 %in% c("covid", "vaccine", "lockdown", "guateng")) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()

set.seed(1234)
word_cors %>%
  filter(correlation > .7) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

```{r VADER}
vader_df <- vader_df(media_agency_df$text)
write_as_csv(vader_df, "data_in/vader")

vader_df <- read_csv("data_in/vader.csv")
vader_df <- vader_df %>% mutate("X1" = row_number())
media_agency_df <- media_agency_df %>% mutate("X1" = row_number())
media_vader_df <- media_agency_df %>% left_join(vader_df, by = "X1")

#sentiment
ggplot(data=media_vader_df, aes(x=created_at, y=rollmean(compound, 30,  na.pad=TRUE))) +
       geom_line(color="pink", size=.5)+
  geom_smooth() +
  theme_minimal()+
  facet_wrap(~screen_name) +
  scale_y_continuous(expand = c(0,0), breaks = c(-0.4,-0.2,0,0.2,0.4)) +
  ylim(c(-0.4, 0.4))

media_vader_df %>% filter(compound != 0)%>% mutate(created_at = as.Date(created_at)) %>%  group_by(created_at) %>% mutate(compound_daily = mean(compound), pos_daily = mean(pos), neg_daily = mean(neg), neu_daily = mean(neu))

#News sentiment increased over time, probably based on less restrictions and vaccines
media_vader_df %>% filter(compound != 0) %>% mutate(created_at = as.Date(created_at)) %>%  group_by(created_at) %>% mutate(compound_daily = mean(compound), pos_daily = mean(pos), neg_daily = mean(neg), neu_daily = mean(neu)) %>% 
  ggplot(aes(created_at)) +
  geom_line(aes(y = compound_daily, colour = "Compound")) +
  geom_smooth(aes(y = compound_daily), formula = y ~ x)

#sentiment
ggplot(data=media_vader_df, aes(x=created_at, fill = screen_name)) +
  stat_smooth(media_vader_df %>% filter(screen_name == "News24"), mapping =  aes(y = compound))+
  stat_smooth(media_vader_df %>% filter(screen_name == "dailymaverick"), mapping =  aes(y = compound))+
  stat_smooth(media_vader_df %>% filter(screen_name == "eNCA"), mapping =  aes(y = compound))+
  stat_smooth(media_vader_df %>% filter(screen_name == "SABCNews"), mapping =  aes(y = compound))+
  stat_smooth(media_vader_df %>% filter(screen_name == "TimesLIVE"), mapping =  aes(y = compound))+
  theme_minimal()+
  scale_y_continuous(expand = c(0,0), breaks = c(-0.1, 0)) 

ggplot(media_vader_df %>% arrange(compound) %>% mutate(X1 = factor(X1, levels = X1)), aes(x=X1)) +
  geom_point(mapping =  aes(y = compound)) +
  theme_minimal()+
  scale_y_continuous(expand = c(0,0)) 


```

```{r Interactions}
media_vader_df %>% select(favorite_count, retweet_count) %>% mutate(favorite_count + retweet_count)

plot <- media_vader_df %>% mutate(created_at = as.Date(created_at)) %>%  group_by(created_at) %>% mutate(retweet_daily = mean(retweet_count), favorite_daily = mean(favorite_count), total = mean(favorite_count + retweet_count)) %>% 
  ggplot(aes(created_at)) +
  geom_ribbon(aes(ymin = 0, ymax = total, fill = "Total")) +
  geom_ribbon(aes(ymin = 0, ymax = favorite_daily, fill = "Favorites")) +
  geom_ribbon(aes(ymin = 0, ymax = retweet_daily, fill = "Retweets"))
  
ggplotly(plot)


media_vader_df %>% 
   filter(screen_name == "dailymaverick") %>% 
  slice_max(favorite_count + retweet_count) 

class(media_vader_df$created_at)

# top tweets
# eNCA Call for ministers over 60 to resign https://t.co/wlEjJGEpuk
# SCORPIO\r\nFloyd Shivambu’s brother quietly pays back Rm admits he received the VBS money gratuitouslyLWeOmNWp
# TimesLIVE	Do you approve of Duduzane running for president? https://t.co/hCDVQGHRWy
# News24 Coca-Cola lost $4 billion in market value after Cristiano Ronaldo suggested people drink water instead | @BISouthAfrica
# SABCNews BREAKING NEWS: King of Eswatini has fled amid public violence in the country https://t.co/1jv8vVCw9d
```

```{r Topic Modelling}
tidy_matrix <- tidy_covid_media_df %>% count(screen_name, word) %>% cast_dfm(screen_name, word, n)

media_lda <- LDA(tidy_matrix, k = 5, control = list(seed = 1234))

media_topics <- tidy(media_lda, matrix = "beta")

media_top_terms <- media_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

media_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  pivot_wider(id_cols = term, names_from = topic, values_from = beta) %>%
  rename("Nations Address" = 2, "Getting vaccinated for third wave" = 3, "Announcement of vaccine" = 4, "Covid-19: South Africa update" = 5, "Vaccination distributions" = 6) %>%
  pivot_longer(cols = c(2,3,4,5,6), names_to = "topic", values_to = "beta") %>%
  drop_na() %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```{r Gap k justify}

install.packages("ldatuning")


install.packages("devtools")
devtools::install_github("nikita-moor/ldatuning")


library("ldatuning")


library("topicmodels")
data("AssociatedPress", package="topicmodels")
dtm <- AssociatedPress[1:10, ]


result <- FindTopicsNumber(
  tidy_matrix,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)


##  fit models... done.
##  calculate metrics:
##   Griffiths2004... done.
##   CaoJuan2009... done.
##   Arun2010... done.
##   Deveaud2014... done.


FindTopicsNumber_plot(result)
```

