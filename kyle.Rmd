---
title: "Assignment 2: South African Media Agency Twitter: Sentiment & Topic Modeling"
output: pdf_document
---
# Background

South Africa has seen an increase in COVID-19 recently and that has been labelled the third wave. The statistic of a third wave has moved the country into another level 4 lockdown. The news on statistics and general COVID-19 related happenings in South Africa are usually reported by their media agencies. The media agencies use many platforms online to repost their articles such as websites and mobile apps. Twitter is a platform where they can post their headline with a link to the article. Using sentiment analysis and topic modelling, we will compare how sentiment and topics have changed over time and how they compare to other media agencies.

# Analysis Process

Twitter posts will be extracted using the “rtweet” package for the programming language “R”. By using the get_timeline method in the package we can extract the latest 3200 Tweets from a specific user without premium. In this case, the users will be a selection of top South African media agencies. 3200 Tweets will give us 2 months’ worth of data per media agency. Relevant COVID-19 Tweets will be extracted from that data. We use VADER to conduct sentiment analysis on the Tweets extracted. We choose VADER over sentimentR, as VADER has been academically proven to provide a more accurate sentiment on Tweets. VADER will give us a positive, negative, neutral, and compound metric on the Tweet. Compound is the Tweets overall sentiment. We then conducted topic modeling using LDA. 
Most of our process comes from working through [Text Mining with R](https://www.tidytextmining.com/)

## 

News24: Recognized by APP Annie (App Annie is the standard in app analytics and app market data) as the most known South African internet media source.
Times Live: Claim to be South Africa's second-biggest news website, published by Arena Holdings (Times Live website). No evidence found to disprove this claim. In top 10 of most visited publication websites for South Africa.
Daily Maverick: Boasts free, fair, and fearless reporting. 
eNCA: In top 10 of most visited publication websites for South Africa.
SABC News: National news company with government ties. Reaches a wide variety of viewers in different languages. The company is both state owned and a public broadcaster company.
\newpage

```{r SETUP, warning=FALSE, include=FALSE}
library(rtweet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(tidytext)
library(stringr)
library(scales)
library(readr)
library(lubridate)
library(vader)
library(quanteda)
library(lubridate)
library(zoo)
library(plotly)
library(forcats)
library(igraph)
library(ggraph)
library(widyr)
library(qdapRegex)
library(png)
library(lubridate)
library(forecast)
library(ldatuning)
library(topicmodels)
library(lda)
library(stm)
library(RedditExtractoR)
```

```{r Fetching Tweets, eval=FALSE, warning=FALSE, include=FALSE}
media_agency_df <- get_timeline("News24", n = 3200)
media_agency_df <- media_agency_df %>%
  bind_rows(get_timeline("eNCA", n = 3200)) %>%
  bind_rows(get_timeline("TimesLIVE", n = 3200)) %>%
  bind_rows(get_timeline("SABCNews", n = 3200)) %>%
  bind_rows(get_timeline("dailymaverick", n = 3200))

write_as_csv(media_agency_df, "data_in/media_agency_tweets")
```

# Comparing Tweets from South African Media Agencies.

We first clean the original Tweets to remove unique News based language. Media agencies often lead their Tweet about an article with the category it belongs to, eg. OPINION, BUSINESS, WATCH.
```{r Clean up, warning=FALSE, include=FALSE}
# created if more Tweets could be accessed to be more covid topic specific.
# covid_dictionary <- c("herd", "immunity", "incubation", "job", "loss", "Kits", "lockdown", "mask", "N95", "outbreak", "pandemic", "quarantine", "recovery", "sanitiser", "transmission", "Underlying", "conditions", "Ventilators", "WHO", "xenophobia", "youTube", "zoonotic", "stay-at-home", "covid", "coronavirus", "hyrdoxychloroquine", "asymptomatic", "frontline", "virus", "self-isolation", "disinfectant", "shelter-in-place", "masks", "SARS-CoV-2", "ICU", "corona", "reopen", "distancing", "covering", "furlough", "tracer", "easing", "remdesivir", "mail-in", "hornet", "antibody", "in-person", "defund", "racism", "looting", "loot", "reopen", "two-metre", "pandemic", "looter", "distancing", "dexamethasone", "racial", "vaccine", "curfew","johnssons", "astrazeneca", "hospitals", "social-distance", "social-distancing", "police", "regulations", "symptoms", "testing", "positive-tests", "negative-tests" , "confirmed-cases", "restrictions", "deaths", "infected", "recoveries", "level", "jobs", "unemployed", "doctors", "infections", "sanitise", "sanitiser", "sanitisation", "containment")

media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
media_agency_df <-
  media_agency_df[, colSums(is.na(media_agency_df)) < nrow(media_agency_df)] %>%
  select(status_id, created_at, screen_name, text, lang, favorite_count, retweet_count)

#Tweet based cleaning. Customized to fit our media sources.
rm_twitter_n_url <-
  rm_(pattern = pastex("@rm_twitter_url", "@rm_url"))
media_agency_df_clean <- media_agency_df %>%
  filter(lang == "en") %>% 
  mutate(
    #Remove news categories that appear before ':' at the start of a Tweet
    text = str_replace_all(text, "(.*?)([:upper:])(?=:)", ""),
    text = str_replace_all(text, "(.*?)([:upper:])\\s(?=|)", ""),
    text = str_replace_all(text, "^[:upper:][:alpha:]{1,}(?=:)", ""),
    text = str_replace_all(text, "^[:upper:][:alpha:]{1,}\\s[:upper:][:alpha:]{1,}(?=:)", ""),
    text = tolower(text), 
    text = str_replace_all(text, "-", " "),
    text = str_replace_all(text, "'", ""),
    text = str_replace_all(text, " ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " "),
    text = str_replace_all(text, " ", " "),
    text = str_replace_all(text, "RT @[a-z,A-Z]*: ", " "),
    text = str_replace_all(text, "#[a-z,A-Z]*", " "),
    text = str_replace_all(text, "@[a-z,A-Z]*", " "),
  ) %>% 
  mutate(
    text = rm_twitter_n_url(text),
    text = gsub("&amp", "", text),
    text = gsub("@\\w+", " ", text),
    text = gsub("[[:punct:]]", "", text),
    text = gsub("[[:digit:]]", " ", text),
    text = gsub("http\\w+", " ", text),
    text = gsub("[ |\t]{2,}", " ", text),
    text = gsub("^ ", "", text),
    text = gsub(" $", "", text),
  )
```
Some tweets fetched date further back. The 3200 tweet pull per user causes this. It tells us that agencies like News24 and SABC News Tweet more daily than Daily Maverick.

```{r frequency of posts, echo=FALSE, warning=FALSE, fig.width=10, fig.height=10}
ggplot(media_agency_df, aes(x = created_at, fill = screen_name)) +
  geom_histogram(position = "identity",
                 bins = 20,
                 show.legend = FALSE) +
  facet_wrap( ~ screen_name, ncol = 1) +
  labs(title = "Post Count by Day")+
  xlab("Date") +
  ylab("Post Count") +
  theme_bw() 
```

By modeling bigrams and trigrams for our dataset we get a better understanding of what topic is being discussed with each word. 
Trigrams were modeled but were not necessary as bigrams provided enough information. We can then also see which sentiments are incorrectly labeled. "not good" gives better context of a negative sentiment, rather than it being incorrectly identified as positive good.
```{r bigrams, warning=FALSE, include=FALSE}
#bigrams
tidy_bigram_df <- media_agency_df_clean %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- tidy_bigram_df %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

#tf-idf bigrams
bigram_tf_idf <- bigrams_united %>%
  count(screen_name, bigram) %>%
  bind_tf_idf(bigram, screen_name, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf %>%
  group_by(screen_name) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ screen_name, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL) +
  theme_bw() 

# weights and graphs
# bigram_graph <- bigram_counts %>%
#   filter(n > 100) %>%
#   graph_from_data_frame()
# 
# set.seed(1234)
# a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
# ggraph(bigram_graph, layout = "fr") +
#   geom_edge_link(
#     aes(edge_alpha = n),
#     show.legend = FALSE,
#     arrow = a,
#     end_cap = circle(.07, 'inches')
#   ) +
#   geom_node_point(color = "lightblue", size = 5) +
#   geom_node_text(aes(label = name),
#                  vjust = 1,
#                  hjust = 1,
#                  repel = TRUE) +
#   theme_void()
```

We give more weight to words that appear more often with the incorrect sentiment. The graph below shows that 'no good' or 'not impressed' have the highest weight of being mis-labeled as positive, and vice-versa for negative words like guilty. We remove these words to increase the accuracy of our sentiment analysis. We can now tidy our dataset based on tidyverse's stopwords collection, and our own negation word collection.
```{r negation words, warning=FALSE, include=FALSE}
#vader lexicon imported from VADER GitHub
vader_lexicon <- read_csv2("data_in/vader_lexicon.csv") %>%
  rename("word" = TOKEN, "value" = `MEAN-SENTIMENT-RATING`)

#common negation words
negation_words <-
  c(
    "not",
    "no",
    "never",
    "without",
    "no",
    "not",
    "none",
    "no one",
    "nobody",
    "nothing",
    "neither",
    "nowhere",
    "never",
    "doesn’t",
    "isn’t",
    "wasn’t",
    "shouldn’t",
    "wouldn’t",
    "couldn’t",
    "won’t",
    "can’t",
    "don’t"
  )
negated_words <- tidy_bigram_df %>%
  filter(word1 %in% fixed(negation_words, ignore_case = TRUE)) %>%
  inner_join(vader_lexicon, by = c(word2 = "word")) %>%
  mutate(value = as.double(value)) %>%
  count(word1, word2, value, sort = TRUE) %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  mutate(word2 = reorder(word2, contribution))
negated_words %>%
  head(40) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ word1, scales = "free_y") +
  labs(x = "Sentiment value * # number of occurrences",
       y = "Words preceded by negation terms") +
  theme_bw() 

```

```{r trigrams, eval=FALSE, warning=FALSE, include=FALSE}
#trigrams
tidy_trigram_df <- media_agency_df_clean %>%
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

trigrams_filtered <- trigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word)

# new trigrams counts:
trigram_counts <- trigrams_filtered %>%
  count(word1, word2, word3, sort = TRUE)

trigrams_united <- trigrams_filtered %>%
  unite(trigram, word1, word2, word3, sep = " ")

#tf-idf trigrams
tidy_trigram_df <- trigrams_united %>%
  count(screen_name, trigram) %>%
  bind_tf_idf(trigram, screen_name, n) %>%
  arrange(desc(tf_idf))

tidy_trigram_df %>%
  group_by(screen_name) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(trigram, tf_idf), fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ screen_name, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

```

We tokenize the Tweets and remove stop words in tidytext, our own stopword dictionary, and negation word dictionary. We also use the "twitter" token to handle any left over @'s and URLS.
```{r Tidy, warning=FALSE, include=FALSE}
#tidy df and unnest
tidy_media_df <- media_agency_df_clean %>%
  unnest_tokens(word, text, token = "tweets") %>%
  filter(
    !word %in% stop_words$word,
    !word %in% negated_words$word2, 
    !word %in% "pm", 
    str_detect(word, "[a-z]")
  ) %>% 
  mutate(created_at = as.Date(created_at))

```

From our tidied Tweet dataset, we look for the top words that appear. This will give us a good idea of what topics are being discussed the most. We find that COVID-19 has been the main topic of discussion. President appears second as President Ramaphosa of South Africa usually addresses the nation regarding COVID-19 information. Additionally, Zuma also appears as he is mentioned as "former president Zuma" in most articles. Zuma appears more as his recent court avoidance and sentencing is being Tweeted. General words surrounding the COVID-19 topic as it is still the main pressure on the country, especially involving Gauteng's rise in infections. 

```{r top words, echo=FALSE, warning=FALSE}
#top words
top_words <- tidy_media_df %>%
  anti_join(stop_words, by = 'word') %>%
  count(word) %>%
  arrange(desc(n))
top_words %>%
  slice(1:20) %>%
  ggplot(aes(reorder(word,-n), n, fill = word)) +
  geom_bar(stat = "identity")  +
  theme_bw() +
  theme(
    axis.text.x = element_text(
      angle = 60,
      hjust = 1,
      size = 13
    )
  ) +
  ylab("Frequency") +
  xlab ("") +
  ggtitle("Most frequent media agency tweets") +
  guides(fill = FALSE)
```
A tf-idf is then modelled to determine which words are the most important per media agency. We also model the word importance by week and determine which media agency has the most unique topics of the week. Daily Maverick is domanting the first four weeks as they are the only user with Tweets from that time.

```{r tf-idf, echo=FALSE, warning=FALSE, fig.height = 10, fig.width = 10}
#tf-idf by user
tidy_media_tf_idf <- tidy_media_df %>%
  count(word, screen_name) %>%
  bind_tf_idf(word, screen_name, n) %>% 
    group_by(screen_name) %>%
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, tf_idf)) 

tidy_media_tf_idf %>%
  group_by(screen_name) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ screen_name, ncol = 2, scales = "free_y") +
  labs(x = "tf-idf", y = NULL) +
  theme_bw() +
  labs(
    title = "Highest tf-idf from each Media Agency"
  )
```


```{r tf-idf week, echo=FALSE, warning=FALSE, fig.height = 8, fig.width = 10}
#tf-idf week
tidy_media_tf_idf_date <- tidy_media_df %>%
  select(created_at, screen_name, word) %>%
  mutate(created_at = as.Date(created_at)) %>% 
  group_by(created_at, screen_name) %>%
  count(word, created_at) %>%
  bind_tf_idf(word, created_at, n)

tidy_media_tf_idf_date %>%
  mutate(week = week(as.Date(created_at))) %>% 
  group_by(week) %>%
  slice_max(tf_idf, n = 7) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = screen_name)) +
  geom_col(show.legend = TRUE) + 
  facet_wrap(~ week, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL) +
  theme_bw() +
  scale_fill_discrete(name = "Screen Name") +
  labs(
    title = "Highest tf-idf by Media Agency in Each Week"
  )
```

The graph below shows which words are most likely to co-occur. We use this to understand the context of words in our topic modeling. The words shown are popular words in our dataset. 

```{r Counting and correlating among sections, echo=FALSE, warning=FALSE}
media_agency_section_words <- tidy_media_df %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0)

# count words co-occuring within sections
word_pairs <- media_agency_section_words %>%
  pairwise_count(word, section, sort = TRUE)

# we need to filter for at least relatively common words first
word_cors <- media_agency_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

# Correlation of next word
# word_cors %>%
#   filter(item1 == "vaccine")

word_cors %>%
  filter(item1 %in% c("covid", "vaccine", "lockdown", "zuma", "gauteng", "ramaphosa", "police")) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation, fill = item1)) +
  geom_bar(stat = "identity") +
  facet_wrap( ~ item1, scales = "free") +
  coord_flip() +
  theme_bw() +
  xlab("Correlation Strength") +
  ylab("Preceding Word") +
  labs(title = "Words that Precede Popular Terms") +
  theme(legend.justification = c(-1,-2))

# modeled correlation map but not used.
# set.seed(1234)
# word_cors %>%
#   filter(correlation > .65) %>%
#   graph_from_data_frame() %>%
#   ggraph(layout = "fr") +
#   geom_edge_link(aes(edge_alpha = correlation),
#                  show.legend = FALSE,
#                  edge_width = 3) +
#   geom_node_point(color = "lightblue", size = 5) +
#   geom_node_text(aes(label = name), repel = TRUE) +
#   theme_void()
```

# Sentiment Analysis

```{r VADER write, eval=FALSE, warning=FALSE, include=FALSE}
vader_df <- vader_df(media_agency_df_clean$text)
write_as_csv(vader_df, "data_in/vader_tweets")
```


```{r VADER setup, warning=FALSE, include=FALSE}
vader_df <- read_csv("data_in/vader_tweets.csv")
vader_df <- vader_df %>% mutate("X1" = row_number())
media_agency_df_clean <- media_agency_df_clean %>% mutate("X1" = row_number())
media_vader_df <- media_agency_df_clean %>% left_join(vader_df, by = "X1") %>%
  mutate(compound = replace(compound, is.na(compound), 0))

```

## Sentiment over time

The general sentiment over time is mostly negative. News articles use negative headlines to get a faster reaction from people when skimming through news. Positive News usually involve sport headlines. 

```{r VADER over time, echo=FALSE, fig.width=15, warning=FALSE}
#Sentiment over time
ggplot(
  media_vader_df %>%
    mutate(created_at = as.Date(created_at)) %>%
    group_by(created_at) %>%
    summarise(sum_compound = sum(as.double(compound))),
  aes(created_at, sum_compound)
) +
  geom_line(lwd = 1.5, colour = "red") +
  geom_smooth(method = "lm", formula = "y ~ x",na.rm = TRUE, se = FALSE, linetype = "dashed") +
  theme_bw() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_date(date_breaks = "3 day", date_labels = "%m-%d", expand = c(0, 0)) +
  ylab("Total Sentiment") +
  xlab("Date") +
  labs(
    title = "Sentiment Over Time by Media Agency",
  ) +
  theme(
    axis.text.x = element_text(
      angle = 90,
      vjust = 0.5,
      hjust = 1
    ),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.background = element_blank(),
    legend.title = element_blank(),
    legend.text = element_blank(),
    legend.key = element_blank(),
    legend.justification = c(-2,-2)
  )
```

### Most Negative Tweet

An illegal gold miner who was severely injured in a clash in which four other illegal miners were killed has been charged for their murders, Mpumalanga police said on Thursday.

```{r VADER most negative, eval=FALSE, include=FALSE}
head(media_vader_df %>% select(text.y, compound) %>% 
  arrange(compound), 1)
```

### Most Positive Tweet

Daily Maverick; Food for Mzansi get the nod at the Global Media Awards: @foodformzansi gets 3rd place in Ad Campaigns; honourable mention for Best Use of Audio @dailymaverick gets honourable mentions for Reader Engagement; Best Use of Print.

```{r VADER most positive, eval=FALSE, include=FALSE}
head(media_vader_df %>% select(text.y, compound) %>% 
  arrange(desc(compound)),1)
```

## Sentiment Over Time per Agency

Daily Maverick, although decreasing, has the straightest regression line. It also has the least outlying total sentiment. We could conclude that this agency is the least bias to emotion. 
News24 has the most positive and positive daily news. 
```{r VADER per agency, echo=FALSE, fig.width=15, fig.height=10, warning=TRUE}
#Sentiment over time per agency
ggplot(
  media_vader_df %>%
    mutate(created_at = as.Date(created_at)) %>%
    group_by(created_at, screen_name) %>%
    summarise(sum_compound = sum(as.double(compound))),
  aes(created_at, sum_compound, colour = screen_name)
) +
  geom_line(lwd = 1.5) +
  geom_smooth(method = "lm", na.rm = TRUE, se = FALSE, linetype = "dashed") +
  facet_wrap( ~ screen_name, scales = "free_x", ncol = 2) +
  theme_bw() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_date(date_breaks = "3 day", date_labels = "%m-%d", expand = c(0, 0)) +
  ylab("Total Sentiment") +
  xlab("Date") +
  labs(
    title = "Sentiment Over Time by Media Agency",
  ) +
  theme(
    axis.text.x = element_text(
      angle = 90,
      vjust = 0.5,
      hjust = 1
    ),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.background = element_blank(),
    legend.title = element_blank(),
    legend.text = element_blank(),
    legend.key = element_blank(),
    legend.justification = c(-2,-2)
  )
```
\newpage

# Interations

The plot below maps the average total interactions by day. From the relevant peaks, we extract the top tweet in that day. 

```{r Interactions, echo=FALSE, warning=FALSE, fig.height=12, fig.width=20}
# media_vader_df %>% select(favorite_count, retweet_count) %>% mutate(favorite_count + retweet_count)

media_vader_df <- media_vader_df %>%
  mutate(created_at = as.Date(created_at)) %>%
  group_by(created_at) %>%
  mutate(
    retweet_daily = mean(retweet_count),
    favorite_daily = mean(favorite_count),
    total = mean(favorite_count + retweet_count)
  )
# mean for the day
interactions_plot <- ggplotly(
  ggplot(data = media_vader_df, aes(created_at)) +
    geom_ribbon(aes(
      ymin = 0, ymax = total, fill = "Total"
    )) +
    geom_ribbon(aes(
      ymin = 0, ymax = favorite_daily, fill = "Favorites"
    )) +
    geom_ribbon(aes(
      ymin = 0, ymax = retweet_daily, fill = "Retweets"
    )) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      axis.ticks = element_blank(),
      
      legend.justification = c(0, 0),
      legend.position = c(0, 0),
      legend.background = element_blank(),
      legend.key = element_blank(),
      legend.title = element_blank(),
      
      plot.title = element_text(
        size = 14,
        face = "bold",
        margin = margin(0, 0, 10, 0),
        hjust = 0
      ),
      plot.caption = element_text(face = "bold", hjust = 0),
    )
  ,
  tooltip = c("total", "favorite_daily", "retweet_daily", "created_at")
)

# peak tweets by day
# media_vader_df %>%
#   filter(created_at == "2021-05-01" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-05-23" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-05-29" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-02" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-06" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-13" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-16" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-26" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-07-03" ) %>%
#   slice_max(favorite_count + retweet_count)

# media_vader_df %>%
#   group_by(screen_name) %>%
#   slice_max(favorite_count + retweet_count)


pp <- readPNG("images/interactions_plot.png")
plot.new() 
rasterImage(pp,0,0,1,1)
```

### Peak Tweets by User   

*@eNCA*: Call for ministers over 60 to resign.

*@dailymaverick*: SCORPIO Floyd Shivambu’s brother quietly pays back R4.55m, admits he received the VBS money gratuitously.

*@TimesLIVE*:	Do you approve of Duduzane running for president?

*@News24*: Coca-Cola lost $4 billion in market value after Cristiano Ronaldo suggested people drink water instead.

*@SABCNews*: BREAKING NEWS: King of Eswatini has fled amid public violence in the country.

\newpage

# Topic Modelling

A topic model is a type of statistic model for discovering the abstract "topics" that occur in a collection of documents.
We determine the best range k-value for LDA (Latent Dirichlet Allocation). The importance of the value of 'k' is to ensure you do not over estimate or underestimate the data. If you over estimate it, you will be left with topics that carry very little meaning and if you under estimate the data, you will lose out on topics that could have been useful to your research.

By looking at the lowest minimum and the highest maximum, we can determine the 'k'. This is how we have interpreted the graphs produced. Griffiths2004 is not informative in this situation and is therefore ignored.

This method used has been proved to produce the best results of LDA without subjectively tuning the 'k' value. Our result is 12 topics.

## Gap k justification

```{r tidy matrix, warning = FALSE, include = FALSE}
tidy_matrix <-
tidy_media_df %>% count(screen_name, word) %>% cast_dfm(screen_name, word, n)
```


```{r Gap k justify, eval=FALSE, warning=FALSE, include=FALSE}
data("AssociatedPress", package = "topicmodels")
dtm <- AssociatedPress[1:10, ]

result <- FindTopicsNumber(
  tidy_matrix,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(result)
```


```{r Gap k justify image, echo=FALSE, warning=FALSE}
pp <- readPNG("images/gap_k_plot.png")
plot.new() 
rasterImage(pp,0,0,1,1)
```

## Topics found

### Beta

Our main topic influence is COVID-19, as this is what currently affects the country the most. Our words are mapped to a beta which shows the amount the word appears per topic. 

```{r Topic Modelling Beta, echo=FALSE, warning=FALSE, fig.height=12, fig.width=15}
media_lda <- LDA(tidy_matrix, k = 12, control = list(seed = 1234))


media_topics_beta <- tidy(media_lda, matrix = "beta")

media_top_terms <- media_topics_beta %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

media_top_terms <- media_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  pivot_wider(id_cols = term,
              names_from = topic,
              values_from = beta) %>%
    rename(
      "News24 - Level 4 Lockdown" = 2,
      "eNCA - Watch President Ramaphosa Speak" = 3,
      "Time Live - The Publics News" = 4,
      "eNCA - Health Covid News" = 5,
      "News24 - Cape Town, Northern & Eastern Cape, Police Updates" = 6,
      "Daily Maverick - South African Covid Vaccine" = 7,
      "News24 - Gauteng Covid Increase News" = 8,
      "SABC News - Former President Zuma in Court" = 9,
      "SABC News - President Rhamaphosa" = 10,
      "Daily Maverick - World Covid News" = 11,
      "Times Live - National Covid News" = 12,
      "Times Live - Sunday President Speach on Lockdown" = 13
    ) %>%
    pivot_longer(
      cols = c(-1),
      names_to = "topic",
      values_to = "beta"
    ) %>%
    drop_na()

ggplot(media_top_terms, aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ topic, scales = "free_y", ncol = 2) +
  scale_y_reordered() +
  labs(title = "Modeled Topics (All Tweets)")
```

### Gamma

The gamma is added to the topics modeled. We can now see which agencies are strongly associated with each topic. The higher the gamma, the more strongly the specific agency associates with the topic.

```{r Topic Modelling Gamma, echo=FALSE, warning=FALSE, fig.width=10, fig.height=7}
media_topics_gamma <-
  tidy(media_lda,
       matrix = "gamma" ,
       document_names = rownames(screen_name))

ggplot(media_topics_gamma, aes(gamma, fill = as.factor(document))) +
  geom_histogram(alpha = 0.8) +
  facet_wrap( ~ topic, ncol = 3) +
  labs(
    title = "Distribution of document probabilities for each topic",
    subtitle = "Each topic is associated with 1-5 Media Agencies",
    y = "Number of topics",
    x = expression(gamma)
  ) +
  theme_bw() +
  scale_fill_discrete(name = "Screen Names")
```


# Additional Requirements

We choose Reddit as our other data source. Facebook was considered but API needing proof of identity with an ID document seemed excessive. The 'RedditExtractoR' package is used to extract comment and post data. Search term 'Covid-19' is used. Other terms were not used as their post dates went further than 6 months.

```{r Fetch post and comments, eval=FALSE, include=FALSE}
reddit_post_df <-
  RedditExtractoR::reddit_urls(
    search_terms = "Covid-19",
    subreddit = "southafrica",
    page_threshold = 10
  )

reddit_comments_df <-
  get_reddit(
    search_terms = "Covid-19",
    subreddit = "southafrica",
    cn_threshold = 3,
    page_threshold = 10,
    sort_by = "new",
    wait_time = 2
  )

write_csv(reddit_post_df, "data_in/reddit_posts.csv")
write_csv(reddit_comment_df, "data_in/reddit_comments.csv")
```

# Comparing Comments From r/southafrica Covid-19 Posts.

We first clean the original Tweets to remove unique News based language. Media agencies often lead their Tweet about an article with the category it belongs to, eg. OPINION, BUSINESS, WATCH.
```{r Reddit Clean up, warning=FALSE, include=FALSE}
media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
media_agency_df <-
  media_agency_df[, colSums(is.na(media_agency_df)) < nrow(media_agency_df)] %>%
  select(status_id, created_at, screen_name, text, lang, favorite_count, retweet_count)

#Tweet based cleaning. Customized to fit our media sources.
rm_twitter_n_url <-
  rm_(pattern = pastex("@rm_twitter_url", "@rm_url"))
media_agency_df_clean <- media_agency_df %>%
  filter(lang == "en") %>% 
  mutate(
    #Remove news categories that appear before ':' at the start of a Tweet
    text = str_replace_all(text, "(.*?)([:upper:])(?=:)", ""),
    text = str_replace_all(text, "(.*?)([:upper:])\\s(?=|)", ""),
    text = str_replace_all(text, "^[:upper:][:alpha:]{1,}(?=:)", ""),
    text = str_replace_all(text, "^[:upper:][:alpha:]{1,}\\s[:upper:][:alpha:]{1,}(?=:)", ""),
    text = tolower(text), 
    text = str_replace_all(text, "-", " "),
    text = str_replace_all(text, "'", ""),
    text = str_replace_all(text, " ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " "),
    text = str_replace_all(text, " ", " "),
    text = str_replace_all(text, "RT @[a-z,A-Z]*: ", " "),
    text = str_replace_all(text, "#[a-z,A-Z]*", " "),
    text = str_replace_all(text, "@[a-z,A-Z]*", " "),
  ) %>% 
  mutate(
    text = rm_twitter_n_url(text),
    text = gsub("&amp", "", text),
    text = gsub("@\\w+", " ", text),
    text = gsub("[[:punct:]]", "", text),
    text = gsub("[[:digit:]]", " ", text),
    text = gsub("http\\w+", " ", text),
    text = gsub("[ |\t]{2,}", " ", text),
    text = gsub("^ ", "", text),
    text = gsub(" $", "", text),
  )
```
Some tweets fetched date further back. The 3200 tweet pull per user causes this. It tells us that agencies like News24 and SABC News Tweet more daily than Daily Maverick.

```{r Reddit frequency of posts, echo=FALSE, warning=FALSE, fig.width=10, fig.height=10}
ggplot(media_agency_df, aes(x = created_at, fill = screen_name)) +
  geom_histogram(position = "identity",
                 bins = 20,
                 show.legend = FALSE) +
  facet_wrap( ~ screen_name, ncol = 1) +
  labs(title = "Post Count by Day")+
  xlab("Date") +
  ylab("Post Count") +
  theme_bw() 
```

By modeling bigrams and trigrams for our dataset we get a better understanding of what topic is being discussed with each word. 
Trigrams were modeled but were not necessary as bigrams provided enough information. We can then also see which sentiments are incorrectly labeled. "not good" gives better context of a negative sentiment, rather than it being incorrectly identified as positive good.
```{r Reddit bigrams, warning=FALSE, include=FALSE}
#bigrams
tidy_bigram_df <- media_agency_df_clean %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- tidy_bigram_df %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

#tf-idf bigrams
bigram_tf_idf <- bigrams_united %>%
  count(screen_name, bigram) %>%
  bind_tf_idf(bigram, screen_name, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf %>%
  group_by(screen_name) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ screen_name, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL) +
  theme_bw() 

# weights and graphs
# bigram_graph <- bigram_counts %>%
#   filter(n > 100) %>%
#   graph_from_data_frame()
# 
# set.seed(1234)
# a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
# ggraph(bigram_graph, layout = "fr") +
#   geom_edge_link(
#     aes(edge_alpha = n),
#     show.legend = FALSE,
#     arrow = a,
#     end_cap = circle(.07, 'inches')
#   ) +
#   geom_node_point(color = "lightblue", size = 5) +
#   geom_node_text(aes(label = name),
#                  vjust = 1,
#                  hjust = 1,
#                  repel = TRUE) +
#   theme_void()
```

We give more weight to words that appear more often with the incorrect sentiment. The graph below shows that 'no good' or 'not impressed' have the highest weight of being mis-labeled as positive, and vice-versa for negative words like guilty. We remove these words to increase the accuracy of our sentiment analysis. We can now tidy our dataset based on tidyverse's stopwords collection, and our own negation word collection.
```{r Reddit negation words, warning=FALSE, include=FALSE}
#vader lexicon imported from VADER GitHub
vader_lexicon <- read_csv2("data_in/vader_lexicon.csv") %>%
  rename("word" = TOKEN, "value" = `MEAN-SENTIMENT-RATING`)

#common negation words
negation_words <-
  c(
    "not",
    "no",
    "never",
    "without",
    "no",
    "not",
    "none",
    "no one",
    "nobody",
    "nothing",
    "neither",
    "nowhere",
    "never",
    "doesn’t",
    "isn’t",
    "wasn’t",
    "shouldn’t",
    "wouldn’t",
    "couldn’t",
    "won’t",
    "can’t",
    "don’t"
  )
negated_words <- tidy_bigram_df %>%
  filter(word1 %in% fixed(negation_words, ignore_case = TRUE)) %>%
  inner_join(vader_lexicon, by = c(word2 = "word")) %>%
  mutate(value = as.double(value)) %>%
  count(word1, word2, value, sort = TRUE) %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  mutate(word2 = reorder(word2, contribution))
negated_words %>%
  head(40) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ word1, scales = "free_y") +
  labs(x = "Sentiment value * # number of occurrences",
       y = "Words preceded by negation terms") +
  theme_bw() 

```
```{r Reddit trigrams, eval=FALSE, warning=FALSE, include=FALSE}
#trigrams
tidy_trigram_df <- media_agency_df_clean %>%
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

trigrams_filtered <- trigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word)

# new trigrams counts:
trigram_counts <- trigrams_filtered %>%
  count(word1, word2, word3, sort = TRUE)

trigrams_united <- trigrams_filtered %>%
  unite(trigram, word1, word2, word3, sep = " ")

#tf-idf trigrams
tidy_trigram_df <- trigrams_united %>%
  count(screen_name, trigram) %>%
  bind_tf_idf(trigram, screen_name, n) %>%
  arrange(desc(tf_idf))

tidy_trigram_df %>%
  group_by(screen_name) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(trigram, tf_idf), fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ screen_name, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

```

We tokenize the Tweets and remove stop words in tidytext, our own stopword dictionary, and negation word dictionary. We also use the "twitter" token to handle any left over @'s and URLS.
```{r Reddit Tidy, warning=FALSE, include=FALSE}
#tidy df and unnest
tidy_media_df <- media_agency_df_clean %>%
  unnest_tokens(word, text, token = "tweets") %>%
  filter(
    !word %in% stop_words$word,
    !word %in% negated_words$word2, 
    !word %in% "pm", 
    str_detect(word, "[a-z]")
  ) %>% 
  mutate(created_at = as.Date(created_at))

```

From our tidied Tweet dataset, we look for the top words that appear. This will give us a good idea of what topics are being discussed the most. We find that COVID-19 has been the main topic of discussion. President appears second as President Ramaphosa of South Africa usually addresses the nation regarding COVID-19 information. Additionally, Zuma also appears as he is mentioned as "former president Zuma" in most articles. Zuma appears more as his recent court avoidance and sentencing is being Tweeted. General words surrounding the COVID-19 topic as it is still the main pressure on the country, especially involving Gauteng's rise in infections. 

```{r Reddit top words, echo=FALSE, warning=FALSE}
#top words
top_words <- tidy_media_df %>%
  anti_join(stop_words, by = 'word') %>%
  count(word) %>%
  arrange(desc(n))
top_words %>%
  slice(1:20) %>%
  ggplot(aes(reorder(word,-n), n, fill = word)) +
  geom_bar(stat = "identity")  +
  theme_bw() +
  theme(
    axis.text.x = element_text(
      angle = 60,
      hjust = 1,
      size = 13
    )
  ) +
  ylab("Frequency") +
  xlab ("") +
  ggtitle("Most frequent media agency tweets") +
  guides(fill = FALSE)
```
A tf-idf is then modelled to determine which words are the most important per media agency. We also model the word importance by week and determine which media agency has the most unique topics of the week. Daily Maverick is domanting the first four weeks as they are the only user with Tweets from that time.

```{r Reddit tf-idf, echo=FALSE, warning=FALSE, fig.height = 10, fig.width = 10}
#tf-idf by user
tidy_media_tf_idf <- tidy_media_df %>%
  count(word, screen_name) %>%
  bind_tf_idf(word, screen_name, n) %>% 
    group_by(screen_name) %>%
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, tf_idf)) 

tidy_media_tf_idf %>%
  group_by(screen_name) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ screen_name, ncol = 2, scales = "free_y") +
  labs(x = "tf-idf", y = NULL) +
  theme_bw() +
  labs(
    title = "Highest tf-idf from each Media Agency"
  )
```


```{r Reddit tf-idf week, echo=FALSE, warning=FALSE, fig.height = 8, fig.width = 10}
#tf-idf week
tidy_media_tf_idf_date <- tidy_media_df %>%
  select(created_at, screen_name, word) %>%
  mutate(created_at = as.Date(created_at)) %>% 
  group_by(created_at, screen_name) %>%
  count(word, created_at) %>%
  bind_tf_idf(word, created_at, n)

tidy_media_tf_idf_date %>%
  mutate(week = week(as.Date(created_at))) %>% 
  group_by(week) %>%
  slice_max(tf_idf, n = 7) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = screen_name)) +
  geom_col(show.legend = TRUE) + 
  facet_wrap(~ week, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL) +
  theme_bw() +
  scale_fill_discrete(name = "Screen Name") +
  labs(
    title = "Highest tf-idf by Media Agency in Each Week"
  )
```

The graph below shows which words are most likely to co-occur. We use this to understand the context of words in our topic modeling. The words shown are popular words in our dataset. 

```{r Reddit Counting and correlating among sections, echo=FALSE, warning=FALSE}
media_agency_section_words <- tidy_media_df %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0)

# count words co-occuring within sections
word_pairs <- media_agency_section_words %>%
  pairwise_count(word, section, sort = TRUE)

# we need to filter for at least relatively common words first
word_cors <- media_agency_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

# Correlation of next word
# word_cors %>%
#   filter(item1 == "vaccine")

word_cors %>%
  filter(item1 %in% c("covid", "vaccine", "lockdown", "zuma", "gauteng", "ramaphosa", "police")) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation, fill = item1)) +
  geom_bar(stat = "identity") +
  facet_wrap( ~ item1, scales = "free") +
  coord_flip() +
  theme_bw() +
  xlab("Correlation Strength") +
  ylab("Preceding Word") +
  labs(title = "Words that Precede Popular Terms") +
  theme(legend.justification = c(-1,-2))

# modeled correlation map but not used.
# set.seed(1234)
# word_cors %>%
#   filter(correlation > .65) %>%
#   graph_from_data_frame() %>%
#   ggraph(layout = "fr") +
#   geom_edge_link(aes(edge_alpha = correlation),
#                  show.legend = FALSE,
#                  edge_width = 3) +
#   geom_node_point(color = "lightblue", size = 5) +
#   geom_node_text(aes(label = name), repel = TRUE) +
#   theme_void()
```

# Sentiment Analysis

```{r Reddit VADER setup, warning=FALSE, include=FALSE}
# vader_df <- vader_df(media_agency_df_clean$text)
# write_as_csv(vader_df, "data_in/vader_tweets")

vader_df <- read_csv("data_in/vader_tweets.csv")
vader_df <- vader_df %>% mutate("X1" = row_number())
media_agency_df_clean <- media_agency_df_clean %>% mutate("X1" = row_number())
media_vader_df <- media_agency_df_clean %>% left_join(vader_df, by = "X1") %>%
  mutate(compound = replace(compound, is.na(compound), 0))

```

## Sentiment over time

The general sentiment over time is mostly negative. News articles use negative headlines to get a faster reaction from people when skimming through news. Positive News usually involve sport headlines. 

```{r Reddit VADER over time, echo=FALSE, fig.width=15, warning=FALSE}
#Sentiment over time
ggplot(
  media_vader_df %>%
    mutate(created_at = as.Date(created_at)) %>%
    group_by(created_at) %>%
    summarise(sum_compound = sum(as.double(compound))),
  aes(created_at, sum_compound)
) +
  geom_line(lwd = 1.5, colour = "red") +
  geom_smooth(method = "lm", formula = "y ~ x",na.rm = TRUE, se = FALSE, linetype = "dashed") +
  theme_bw() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_date(date_breaks = "3 day", date_labels = "%m-%d") +
  ylab("Total Sentiment") +
  xlab("Date") +
  labs(
    title = "Sentiment Over Time by Media Agency",
  ) +
  theme(
    axis.text.x = element_text(
      angle = 90,
      vjust = 0.5,
      hjust = 1
    ),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.background = element_blank(),
    legend.title = element_blank(),
    legend.text = element_blank(),
    legend.key = element_blank(),
    legend.justification = c(-2,-2)
  )
```

### Most Negative Tweet

An illegal gold miner who was severely injured in a clash in which four other illegal miners were killed has been charged for their murders, Mpumalanga police said on Thursday.

```{r Reddit VADER most negative, eval=FALSE, include=FALSE}
head(media_vader_df %>% select(text.y, compound) %>% 
  arrange(compound), 1)
```

### Most Positive Tweet

Daily Maverick; Food for Mzansi get the nod at the Global Media Awards: @foodformzansi gets 3rd place in Ad Campaigns; honourable mention for Best Use of Audio @dailymaverick gets honourable mentions for Reader Engagement; Best Use of Print.

```{r Reddit VADER most positive, eval=FALSE, include=FALSE}
head(media_vader_df %>% select(text.y, compound) %>% 
  arrange(desc(compound)),1)
```

## Sentiment Over Time per Agency

Daily Maverick, although decreasing, has the straightest regression line. It also has the least outlying total sentiment. We could conclude that this agency is the least bias to emotion. 
News24 has the most positive and positive daily news. 
```{r Reddit VADER per agency, echo=FALSE, fig.width=15, fig.height=10, warning=TRUE}
#Sentiment over time per agency
ggplot(
  media_vader_df %>%
    mutate(created_at = as.Date(created_at)) %>%
    group_by(created_at, screen_name) %>%
    summarise(sum_compound = sum(as.double(compound))),
  aes(created_at, sum_compound, colour = screen_name)
) +
  geom_line(lwd = 1.5) +
  geom_smooth(method = "lm", na.rm = TRUE, se = FALSE, linetype = "dashed") +
  facet_wrap( ~ screen_name, scales = "free_x", ncol = 2) +
  theme_bw() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_date(date_breaks = "3 day", date_labels = "%m-%d") +
  ylab("Total Sentiment") +
  xlab("Date") +
  labs(
    title = "Sentiment Over Time by Media Agency",
  ) +
  theme(
    axis.text.x = element_text(
      angle = 90,
      vjust = 0.5,
      hjust = 1
    ),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.background = element_blank(),
    legend.title = element_blank(),
    legend.text = element_blank(),
    legend.key = element_blank(),
    legend.justification = c(-2,-2)
  )
```
\newpage

# Interations

The plot below maps the average total interactions by day. From the relevant peaks, we extract the top tweet in that day. 

```{r Reddit Interactions, echo=FALSE, warning=FALSE, fig.height=12, fig.width=20}
# media_vader_df %>% select(favorite_count, retweet_count) %>% mutate(favorite_count + retweet_count)

media_vader_df <- media_vader_df %>%
  mutate(created_at = as.Date(created_at)) %>%
  group_by(created_at) %>%
  mutate(
    retweet_daily = mean(retweet_count),
    favorite_daily = mean(favorite_count),
    total = mean(favorite_count + retweet_count)
  )
# mean for the day
interactions_plot <- ggplotly(
  ggplot(data = media_vader_df, aes(created_at)) +
    geom_ribbon(aes(
      ymin = 0, ymax = total, fill = "Total"
    )) +
    geom_ribbon(aes(
      ymin = 0, ymax = favorite_daily, fill = "Favorites"
    )) +
    geom_ribbon(aes(
      ymin = 0, ymax = retweet_daily, fill = "Retweets"
    )) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      axis.ticks = element_blank(),
      
      legend.justification = c(0, 0),
      legend.position = c(0, 0),
      legend.background = element_blank(),
      legend.key = element_blank(),
      legend.title = element_blank(),
      
      plot.title = element_text(
        size = 14,
        face = "bold",
        margin = margin(0, 0, 10, 0),
        hjust = 0
      ),
      plot.caption = element_text(face = "bold", hjust = 0),
    )
  ,
  tooltip = c("total", "favorite_daily", "retweet_daily", "created_at")
)

# peak tweets by day
# media_vader_df %>%
#   filter(created_at == "2021-05-01" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-05-23" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-05-29" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-02" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-06" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-13" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-16" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-26" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-07-03" ) %>%
#   slice_max(favorite_count + retweet_count)

# media_vader_df %>%
#   group_by(screen_name) %>%
#   slice_max(favorite_count + retweet_count)


pp <- readPNG("images/interactions_plot.png")
plot.new() 
rasterImage(pp,0,0,1,1)
```

### Peak Tweets by User   

*@eNCA*: Call for ministers over 60 to resign.

*@dailymaverick*: SCORPIO Floyd Shivambu’s brother quietly pays back R4.55m, admits he received the VBS money gratuitously.

*@TimesLIVE*:	Do you approve of Duduzane running for president?

*@News24*: Coca-Cola lost $4 billion in market value after Cristiano Ronaldo suggested people drink water instead.

*@SABCNews*: BREAKING NEWS: King of Eswatini has fled amid public violence in the country.

\newpage

# Topic Modelling

A topic model is a type of statistic model for discovering the abstract "topics" that occur in a collection of documents.
We determine the best range k-value for LDA (Latent Dirichlet Allocation). The importance of the value of 'k' is to ensure you do not over estimate or underestimate the data. If you over estimate it, you will be left with topics that carry very little meaning and if you under estimate the data, you will lose out on topics that could have been useful to your research.

By looking at the lowest minimum and the highest maximum, we can determine the 'k'. This is how we have interpreted the graphs produced. Griffiths2004 is not informative in this situation and is therefore ignored.

This method used has been proved to produce the best results of LDA without subjectively tuning the 'k' value. Our result is 12 topics.

## Gap k justification

```{r Reddit tidy matrix, warning = FALSE, include = FALSE}
tidy_matrix <-
tidy_media_df %>% count(screen_name, word) %>% cast_dfm(screen_name, word, n)
```


```{r Reddit Gap k justify, eval=FALSE, warning=FALSE, include=FALSE}
data("AssociatedPress", package = "topicmodels")
dtm <- AssociatedPress[1:10, ]

result <- FindTopicsNumber(
  tidy_matrix,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(result)
```


```{r Reddit Gap k justify image, echo=FALSE, warning=FALSE}
pp <- readPNG("images/gap_k_plot.png")
plot.new() 
rasterImage(pp,0,0,1,1)
```

## Topics found

### Beta

Our main topic influence is COVID-19, as this is what currently affects the country the most. Our words are mapped to a beta which shows the amount the word appears per topic. 

```{r Reddit Topic Modelling Beta, echo=FALSE, warning=FALSE, fig.height=12, fig.width=15}
media_lda <- LDA(tidy_matrix, k = 12, control = list(seed = 1234))


media_topics_beta <- tidy(media_lda, matrix = "beta")

media_top_terms <- media_topics_beta %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

media_top_terms <- media_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  pivot_wider(id_cols = term,
              names_from = topic,
              values_from = beta) %>%
    rename(
      "News24 - Level 4 Lockdown" = 2,
      "eNCA - Watch President Ramaphosa Speak" = 3,
      "Time Live - The Publics News" = 4,
      "eNCA - Health Covid News" = 5,
      "News24 - Cape Town, Northern & Eastern Cape, Police Updates" = 6,
      "Daily Maverick - South African Covid Vaccine" = 7,
      "News24 - Gauteng Covid Increase News" = 8,
      "SABC News - Former President Zuma in Court" = 9,
      "SABC News - President Rhamaphosa" = 10,
      "Daily Maverick - World Covid News" = 11,
      "Times Live - National Covid News" = 12,
      "Times Live - Sunday President Speach on Lockdown" = 13
    ) %>%
    pivot_longer(
      cols = c(-1),
      names_to = "topic",
      values_to = "beta"
    ) %>%
    drop_na()

ggplot(media_top_terms, aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ topic, scales = "free_y", ncol = 2) +
  scale_y_reordered() +
  labs(title = "Modeled Topics (All Tweets)")
```

### Gamma

The gamma is added to the topics modeled. We can now see which agencies are strongly associated with each topic. The higher the gamma, the more strongly the specific agency associates with the topic.

```{r Reddit Topic Modelling Gamma, echo=FALSE, warning=FALSE, fig.width=10, fig.height=7}
media_topics_gamma <-
  tidy(media_lda,
       matrix = "gamma" ,
       document_names = rownames(screen_name))

ggplot(media_topics_gamma, aes(gamma, fill = as.factor(document))) +
  geom_histogram(alpha = 0.8) +
  facet_wrap( ~ topic, ncol = 3) +
  labs(
    title = "Distribution of document probabilities for each topic",
    subtitle = "Each topic is associated with 1-5 Media Agencies",
    y = "Number of topics",
    x = expression(gamma)
  ) +
  theme_bw() +
  scale_fill_discrete(name = "Screen Names")
```