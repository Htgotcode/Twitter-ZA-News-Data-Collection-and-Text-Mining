---
title: "Assignment 2: South African Media Agency Twitter: Sentiment & Topic Modeling"
subtitle: "21013527 (K. van Antwerpen), 21670897 (T. Luyt), 20073445 (F. Cilliers)"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: true
---

\newpage

# GitHub

[GitHub Repository Link](https://github.com/Htgotcode/Twitter-ZA-News-Data-Collection-and-Text-Mining)

# 1. Background

South Africa has seen an increase in COVID-19 recently and that has been labelled the third wave. The statistic of a third wave has moved the country into another level 4 lockdown. The news on statistics and general COVID-19 related happenings in South Africa are usually reported by their media agencies. The media agencies use many platforms online to repost their articles such as websites and mobile apps. Twitter is a platform where they can post their headline with a link to the article. Using sentiment analysis and topic modelling, we will compare how sentiment and topics have changed over time and how they compare to other media agencies.

# 2. Analysis Process

Twitter posts will be extracted using the “rtweet” package for the programming language “R”. By using the get_timeline method in the package we can extract the latest 3200 Tweets from a specific user without premium. In this case, the users will be a selection of top South African media agencies. 3200 Tweets will give us 2 months’ worth of data per media agency. Relevant COVID-19 Tweets will be extracted from that data. We use VADER to conduct sentiment analysis on the Tweets extracted. We choose VADER over sentimentR, as VADER has been academically proven to provide a more accurate sentiment on Tweets. VADER will give us a positive, negative, neutral, and compound metric on the Tweet. Compound is the Tweets overall sentiment. We then conducted topic modeling using LDA. 
Most of our process comes from working through [Text Mining with R](https://www.tidytextmining.com/).

## 2.1. Media Agency Reasoning

After research on what makes a news source trustworth and unbias, we chose a mix of them. 

*News24:* Recognized by APP Annie (App Annie is the standard in app analytics and app market data) as the most known South African internet media source.

*Times Live:* Claim to be South Africa's second-biggest news website, published by Arena Holdings (Times Live website). No evidence found to disprove this claim. In top 10 of most visited publication websites for South Africa.

*Daily Maverick:* Boasts free, fair, and fearless reporting. 

*eNCA:* In top 10 of most visited publication websites for South Africa.

*SABC News:* National news company with government ties. Reaches a wide variety of viewers in different languages. The company is both state owned and a public broadcaster company.

\newpage

```{r SETUP, warning=FALSE, include=FALSE}
#AVAILABLE ON GITHUB
library(rtweet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(tidytext)
library(stringr)
library(scales)
library(readr)
library(lubridate)
library(vader)
library(quanteda)
library(lubridate)
library(zoo)
library(plotly)
library(forcats)
library(igraph)
library(ggraph)
library(widyr)
library(qdapRegex)
library(png)
library(lubridate)
library(forecast)
library(ldatuning)
library(topicmodels)
library(lda)
library(stm)
library(RedditExtractoR)
library(gt)
library(tinytex)
```

```{r Fetching Tweets, eval=FALSE, warning=FALSE, include=FALSE}
media_agency_df <- get_timeline("News24", n = 3200)
media_agency_df <- media_agency_df %>%
  bind_rows(get_timeline("eNCA", n = 3200)) %>%
  bind_rows(get_timeline("TimesLIVE", n = 3200)) %>%
  bind_rows(get_timeline("SABCNews", n = 3200)) %>%
  bind_rows(get_timeline("dailymaverick", n = 3200))

write_as_csv(media_agency_df, "data_in/media_agency_tweets")
```

# 3. Giving Context to our Data and Tidying.

We first clean the original Tweets to remove unique News based language. Media agencies often lead their Tweet about an article with the category it belongs to, eg. OPINION, BUSINESS, WATCH.

```{r Clean up, warning=FALSE, include=FALSE}
# created if more Tweets could be accessed to be more covid topic specific.
# covid_dictionary <- c("herd", "immunity", "incubation", "job", "loss", "Kits", "lockdown", "mask", "N95", "outbreak", "pandemic", "quarantine", "recovery", "sanitiser", "transmission", "Underlying", "conditions", "Ventilators", "WHO", "xenophobia", "youTube", "zoonotic", "stay-at-home", "covid", "coronavirus", "hyrdoxychloroquine", "asymptomatic", "frontline", "virus", "self-isolation", "disinfectant", "shelter-in-place", "masks", "SARS-CoV-2", "ICU", "corona", "reopen", "distancing", "covering", "furlough", "tracer", "easing", "remdesivir", "mail-in", "hornet", "antibody", "in-person", "defund", "racism", "looting", "loot", "reopen", "two-metre", "pandemic", "looter", "distancing", "dexamethasone", "racial", "vaccine", "curfew","johnssons", "astrazeneca", "hospitals", "social-distance", "social-distancing", "police", "regulations", "symptoms", "testing", "positive-tests", "negative-tests" , "confirmed-cases", "restrictions", "deaths", "infected", "recoveries", "level", "jobs", "unemployed", "doctors", "infections", "sanitise", "sanitiser", "sanitisation", "containment")

media_agency_df <- read_csv("data_in/media_agency_tweets.csv")
media_agency_df <-
  media_agency_df[, colSums(is.na(media_agency_df)) < nrow(media_agency_df)] %>%
  select(status_id, created_at, screen_name, text, lang, favorite_count, retweet_count)

#Tweet based cleaning. Customized to fit our media sources.
rm_twitter_n_url <-
  rm_(pattern = pastex("@rm_twitter_url", "@rm_url"))
media_agency_df_clean <- media_agency_df %>%
  filter(lang == "en") %>% 
  mutate(
    #Remove news categories that appear before ':' at the start of a Tweet
    text = str_replace_all(text, "(.*?)([:upper:])(?=:)", ""),
    text = str_replace_all(text, "(.*?)([:upper:])\\s(?=|)", ""),
    text = str_replace_all(text, "^[:upper:][:alpha:]{1,}(?=:)", ""),
    text = str_replace_all(text, "^[:upper:][:alpha:]{1,}\\s[:upper:][:alpha:]{1,}(?=:)", ""),
    text = tolower(text), 
    text = str_replace_all(text, "-", " "),
    text = str_replace_all(text, "'", ""),
    text = str_replace_all(text, " ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " "),
    text = str_replace_all(text, " ", " "),
    text = str_replace_all(text, "RT @[a-z,A-Z]*: ", " "),
    text = str_replace_all(text, "#[a-z,A-Z]*", " "),
    text = str_replace_all(text, "@[a-z,A-Z]*", " "),
  ) %>% 
  mutate(
    text = rm_twitter_n_url(text),
    text = gsub("&amp", "", text),
    text = gsub("@\\w+", " ", text),
    text = gsub("[[:punct:]]", "", text),
    text = gsub("[[:digit:]]", " ", text),
    text = gsub("http\\w+", " ", text),
    text = gsub("[ |\t]{2,}", " ", text),
    text = gsub("^ ", "", text),
    text = gsub(" $", "", text),
  )
```

Some tweets fetched date further back, as seen in figure 4.1. The 3200 tweet pull per user causes this. It tells us that agencies like News24 and SABC News Tweet more daily than Daily Maverick.

```{r frequency of posts, echo=FALSE, warning=FALSE, fig.width=10, fig.height=10}
ggplot(media_agency_df, aes(x = created_at, fill = screen_name)) +
  geom_histogram(position = "identity",
                 bins = 20,
                 show.legend = FALSE) +
  facet_wrap( ~ screen_name, ncol = 1) +
  labs(x = "Date", y = "Post Count", caption = "Figure 3.1: Post Count by Day") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1.2)))
```

\pagebreak

## 3.1. Bigrams
  
Next, by modeling bigrams and trigrams for our dataset, we get a better understanding of what topic is being discussed with each word, as shown in figure 3.2. Trigrams were modeled but were not necessary as bigrams provided enough information. We can then also see which sentiments are incorrectly labeled. "not good" gives better context of a negative sentiment, rather than it being incorrectly identified as positive good, as shown in figure 3.3.

```{r bigrams, echo=FALSE, warning=FALSE, fig.height=7.5}
#bigrams
tidy_bigram_df <- media_agency_df_clean %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- tidy_bigram_df %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

#tf-idf bigrams
bigram_tf_idf <- bigrams_united %>%
  count(screen_name, bigram) %>%
  bind_tf_idf(bigram, screen_name, n) %>%
  arrange(desc(tf_idf))

# weights and graphs
# bigram_graph <- bigram_counts %>%
#   filter(n > 100) %>%
#   graph_from_data_frame()
# 
# set.seed(1234)
# a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
# ggraph(bigram_graph, layout = "fr") +
#   geom_edge_link(
#     aes(edge_alpha = n),
#     show.legend = FALSE,
#     arrow = a,
#     end_cap = circle(.07, 'inches')
#   ) +
#   geom_node_point(color = "lightblue", size = 5) +
#   geom_node_text(aes(label = name),
#                  vjust = 1,
#                  hjust = 1,
#                  repel = TRUE) +
#   theme_void()

bigram_tf_idf %>%
  group_by(screen_name) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ screen_name, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL, caption = "Figure 3.2: Bigrams") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1.2)))
```

## 3.2. Negation Words

We give more weight to words that appear more often with the incorrect sentiment. The graph below (Figure 3.3) shows that 'no good' or 'not impressed' have the highest weight of being mislabeled as positive, and vice-versa for negative words like guilty. We remove these words to increase the accuracy of our sentiment analysis. We can now tidy our dataset based on tidyverse's stopwords collection, and our own negation word collection.

```{r negation words, echo=FALSE, message=FALSE, warning=FALSE, fig.height=7.5}
#vader lexicon imported from VADER GitHub
vader_lexicon <- read_csv2("data_in/vader_lexicon.csv") %>%
  rename("word" = TOKEN, "value" = `MEAN-SENTIMENT-RATING`)

#common negation words
negation_words <-
  c(
    "not",
    "no",
    "never",
    "without",
    "no",
    "not",
    "none",
    "no one",
    "nobody",
    "nothing",
    "neither",
    "nowhere",
    "never",
    "doesn’t",
    "isn’t",
    "wasn’t",
    "shouldn’t",
    "wouldn’t",
    "couldn’t",
    "won’t",
    "can’t",
    "don’t"
  )
negated_words <- tidy_bigram_df %>%
  filter(word1 %in% fixed(negation_words, ignore_case = TRUE)) %>%
  inner_join(vader_lexicon, by = c(word2 = "word")) %>%
  mutate(value = as.double(value)) %>%
  count(word1, word2, value, sort = TRUE) %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  mutate(word2 = reorder(word2, contribution))

negated_words %>%
  head(40) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ word1, scales = "free_y") +
  labs(x = "Sentiment value * # number of occurrences", y = "Words preceded by negation terms", caption = "Figure 3.3: Negation Words") +
  theme_bw() +
  theme(plot.caption = element_text(hjust = 0.5, size = rel(1.2)))

```

```{r trigrams, eval=FALSE, warning=FALSE, include=FALSE}
#trigrams
tidy_trigram_df <- media_agency_df_clean %>%
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", text)) %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

trigrams_filtered <- trigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word)

# new trigrams counts:
trigram_counts <- trigrams_filtered %>%
  count(word1, word2, word3, sort = TRUE)

trigrams_united <- trigrams_filtered %>%
  unite(trigram, word1, word2, word3, sep = " ")

#tf-idf trigrams
tidy_trigram_df <- trigrams_united %>%
  count(screen_name, trigram) %>%
  bind_tf_idf(trigram, screen_name, n) %>%
  arrange(desc(tf_idf))

tidy_trigram_df %>%
  group_by(screen_name) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(trigram, tf_idf), fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ screen_name, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

```

```{r Tidy, warning=FALSE, include=FALSE}
#tidy df and unnest
tidy_media_df <- media_agency_df_clean %>%
  unnest_tokens(word, text, token = "tweets") %>%
  filter(
    !word %in% stop_words$word,
    !word %in% negated_words$word2, 
    !word %in% "pm", 
    str_detect(word, "[a-z]")
  ) %>% 
  mutate(created_at = as.Date(created_at))

```

## 3.3. Top Words

From our tidied Tweet dataset, we look for the top words that appear (Figure 3.4). It gives us a good idea of what topics are being discussed the most. We find that COVID-19 has been the main topic of discussion. President appears second as President Ramaphosa of South Africa usually addresses the nation regarding COVID-19 information. Additionally, Zuma also appears as he is mentioned as "former president Zuma" in most articles. Zuma appears more as his recent court avoidance and sentencing is being Tweeted. General words surrounding the COVID-19 topic as it is still the main pressure on the country, especially involving Gauteng's rise in infections. 

```{r top words, echo=FALSE, message=FALSE, warning=FALSE}
#top words
top_words <- tidy_media_df %>%
  anti_join(stop_words, by = 'word') %>%
  count(word) %>%
  arrange(desc(n))
top_words %>%
  slice(1:20) %>%
  ggplot(aes(reorder(word, -n), n, fill = word)) +
  geom_bar(stat = "identity")  +
  theme_bw() +
  theme(
    plot.caption = element_text(hjust = 0.5, size = rel(1.2)),
    axis.text.x = element_text(
      angle = 60,
      hjust = 1,
      size = 13
      )
    ) +
  labs(x = NULL, y = "Frequency", caption = "Figure 3.4: Most Frequent Media Agency Words") +
  guides(fill = FALSE)
```

\pagebreak

## 3.4. Analyzing Words and User Frequency

## 3.4.1. Term Frequency Distribution

What we see from figure 3.6 is each media agency's use of uncommon words. The more right-skewed the data is, the less unique the news articles are. Times Live is shown to have the most diversity.

```{r Unique words, echo=FALSE, message=FALSE, warning=FALSE}
tweet_words <- media_agency_df_clean %>%
  unnest_tokens(word, text) %>%
  count(screen_name, word, sort = TRUE)

total_words <- tweet_words %>% 
  group_by(screen_name) %>% 
  summarize(total = sum(n))

tweet_words <- left_join(tweet_words, total_words)

ggplot(tweet_words, aes(n/total, fill = screen_name)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~screen_name, ncol = 2, scales = "free_y") +
  labs(x = "Word Count", y = "Term Frequency (n/total)", caption = "Figure 3.5: Distribution of Term Frequency") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1.2)))
```

\pagebreak

## 3.4.2. Zipf's Law

Our interpretation of Zipf's Law (Figure 3.6) on our dataset shows that the media agencies tend to use the same words often. News on different categories in the world will generally be of the same topic, but with different subjects. Eg. "Business News today for Amazon is x" vs. "Business News today for Microsoft is x". 

```{r Zipfs law, echo=FALSE, message=FALSE, warning=FALSE}
freq_by_rank <- tweet_words %>% 
  group_by(screen_name) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup()

# rank_subset <- freq_by_rank %>% 
#   filter(rank < 500,
#          rank > 10)
# lm(log10(`term frequency`) ~   log10(rank), data = rank_subset)

# Call:
# lm(formula = log10(`term frequency`) ~ log10(rank), data = rank_subset)
# 
# Coefficients:
# (Intercept)  log10(rank)  
#     -1.2136      -0.8917  

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = screen_name)) + 
  geom_abline(intercept = -1.2136, slope = -0.8917, 
              color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "Rank", y = "Term Frequency", caption = "Figure 3.6: Zipf's Law with a Broken Power Law") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1.2)))
```
  
## 3.5. TF-IDF

A tf-idf is then modeled to determine which words are the most important per media agency (Figure 3.7). We also model the word importance by week and determine which media agency has the most unique topics of the week. Daily Maverick is dominating the first four weeks as they are the only user with Tweets from that time.

```{r tf-idf, echo=FALSE, fig.height=7.5, message=FALSE, warning=FALSE}
#tf-idf by user
tidy_media_tf_idf <- tidy_media_df %>%
  count(word, screen_name) %>%
  bind_tf_idf(word, screen_name, n) %>% 
    group_by(screen_name) %>%
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, tf_idf)) 

tidy_media_tf_idf %>%
  group_by(screen_name) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ screen_name, ncol = 2, scales = "free_y") +
  labs(x = "tf-idf", y = NULL, caption = "Figure 3.7: Highest tf-idf from each Media Agency") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1.2)))
```

Tweet topics involved with the tf-idf's listed in the weeks collected (Figure 3.8):

- 18. Nazanin's struggles to breathe while Narenda mMdi's government abdicates all responsibility.
- 19. Newspaper is on sale now in and free to loyalty card holders so go grab a copy henni has movies for everyone so maybe ill see you there. extinction rebellion stop putting activists on trial it isn’t in the public interest.
- 20. Covid ready for liftoff SA's vaccine mission impossible.
- 21. Springboks and Bafana Bafana face loss of players due to injury and covid. cabinet approves bill to strengthen sabcs finances management.
- 22. Expenditure uncovered through onfidential internal audit. exposed digital vibes bankrolled maintenance work paid to minister’s son.
- 23. Concern hit sprinboks due to injury leading up to british and irish lions series. F1 grand prix endured four red flags due to a large amount of crashes around the track.
- 24. Danish footballer Christian Eriksen had a heart attack on the field in a recent Euros match. Inter Millan team mate, Lukaku, send out message of support. Youth day keynote address delivered by the president, cyril ramaphosa.
- 25. Dr. Mary Kawonga on hostipal capacity and Eskom's electric grid failures.
- 26. Student found death outside Walter Sisulu University. Rape survivor, Andile Gaelesiwe, has released a new book with her foundation.
- 27. The appearance of Jacob Zuma in the constitutional court. Jacob Zuma supporters move in a motorcar from Durban to Nkandla to offer support to the former president for contempt in the constitutional court, ahead of his arrest.

```{r tf-idf week, echo=FALSE, warning=FALSE, fig.height = 8, fig.width = 10}
#tf-idf week
tidy_media_tf_idf_date <- tidy_media_df %>%
  select(created_at, screen_name, word) %>%
  mutate(created_at = as.Date(created_at)) %>% 
  group_by(created_at, screen_name) %>%
  count(word, created_at) %>%
  bind_tf_idf(word, created_at, n)

tidy_media_tf_idf_date %>%
  mutate(week = week(as.Date(created_at))) %>% 
  group_by(week) %>%
  slice_max(tf_idf, n = 7) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = screen_name)) +
  geom_col(show.legend = TRUE) + 
  facet_wrap(~ week, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL, caption = "Figure 3.8: Highest tf-idf by Media Agency in Each Week") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1.2))) +
  scale_fill_discrete(name = "Screen Name") 

```

## 3.6. Word Correlation Among Users


The figure (3.9) below shows which words are most likely to co-occur. We use this to understand the context of words in our topic modeling. The words shown are popular words in our dataset. 

```{r Counting and correlating among sections, echo=FALSE, warning=FALSE}
media_agency_section_words <- tidy_media_df %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0)

# count words co-occuring within sections
word_pairs <- media_agency_section_words %>%
  pairwise_count(word, section, sort = TRUE)

# we need to filter for at least relatively common words first
word_cors <- media_agency_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

# Correlation of next word
# word_cors %>%
#   filter(item1 == "vaccine")

word_cors %>%
  filter(item1 %in% c("covid", "vaccine", "lockdown", "zuma", "gauteng", "ramaphosa", "police", "people", "minister")) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation, fill = item1)) +
  geom_bar(stat = "identity") +
  facet_wrap( ~ item1, scales = "free") +
  coord_flip() +
  labs(x = "Correlation Strength", y = "Preceding Word", caption = "Figure 3.9: Words that Precede Popular Terms") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1.2))) +
  theme(legend.justification = c(-10,-10))

# modeled correlation map but not used.
# set.seed(1234)
# word_cors %>%
#   filter(correlation > .65) %>%
#   graph_from_data_frame() %>%
#   ggraph(layout = "fr") +
#   geom_edge_link(aes(edge_alpha = correlation),
#                  show.legend = FALSE,
#                  edge_width = 3) +
#   geom_node_point(color = "lightblue", size = 5) +
#   geom_node_text(aes(label = name), repel = TRUE) +
#   theme_void()
```

# 4. Sentiment Analysis

```{r VADER write, eval=FALSE, warning=FALSE, include=FALSE}
vader_df <- vader_df(media_agency_df_clean$text)
write_as_csv(vader_df, "data_in/vader_tweets")
```

```{r VADER setup, warning=FALSE, include=FALSE}
vader_df <- read_csv("data_in/vader_tweets.csv")
vader_df <- vader_df %>% mutate("X1" = row_number())
media_agency_df_clean <- media_agency_df_clean %>% mutate("X1" = row_number())
media_vader_df <- media_agency_df_clean %>% left_join(vader_df, by = "X1") %>%
  mutate(compound = replace(compound, is.na(compound), 0))

```

## 4.1. Sentiment over time

The general sentiment over time is mostly negative (Figure 4.1). News articles use negative headlines to get a faster reaction from people when skimming through news. Positive News usually involve sport headlines. 

```{r VADER over time, echo=FALSE, fig.width=15, warning=FALSE}
#Sentiment over time
ggplot(
  media_vader_df %>%
    mutate(created_at = as.Date(created_at)) %>%
    group_by(created_at) %>%
    summarise(sum_compound = sum(as.double(compound))),
  aes(created_at, sum_compound)
) +
  geom_line(lwd = 1.5, colour = "red") +
  geom_smooth(method = "lm", formula = "y ~ x",na.rm = TRUE, se = FALSE, linetype = "dashed") +
  theme_bw() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_date(date_breaks = "3 day", date_labels = "%m-%d", expand = c(0, 0)) +
  labs(x = "Date", y = "Total Sentiment", caption = "Figure 4.1: Sentiment Over Time by Media Agency") +
  theme_bw() +
  theme(
    axis.text.x = element_text(
      angle = 90,
      vjust = 0.5,
      hjust = 1
    ),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.background = element_blank(),
    legend.title = element_blank(),
    legend.text = element_blank(),
    legend.key = element_blank(),
    legend.justification = c(-2,-2),
    plot.caption = element_text(hjust=0.5, size=rel(1.2))
  )
```

### 4.1.2. Most Negative Tweet

Times Live: An illegal gold miner who was severely injured in a clash in which four other illegal miners were killed has been charged for their murders, Mpumalanga police said on Thursday.

```{r VADER most negative, eval=FALSE, include=FALSE}
head(media_vader_df %>% select(text.y, compound) %>% 
  arrange(compound), 1)
```

### 4.1.3 Most Positive Tweet

Daily Maverick; Food for Mzansi get the nod at the Global Media Awards: @foodformzansi gets 3rd place in Ad Campaigns; honourable mention for Best Use of Audio @dailymaverick gets honourable mentions for Reader Engagement; Best Use of Print.

```{r VADER most positive, eval=FALSE, include=FALSE}
head(media_vader_df %>% select(text.y, compound) %>% 
  arrange(desc(compound)),1)
```

## 4.2. Sentiment Over Time per Agency

Daily Maverick, although decreasing, has the straightest regression line. It also has the least outlying total sentiment. We could conclude that this agency is the least bias to emotion. 
News24 has the most positive and negative daily news. 

```{r VADER per agency, echo=FALSE, fig.height=10, fig.width=15, message=FALSE, warning=TRUE}
#Sentiment over time per agency
ggplot(
  media_vader_df %>%
    mutate(created_at = as.Date(created_at)) %>%
    group_by(created_at, screen_name) %>%
    summarise(sum_compound = sum(as.double(compound))),
  aes(created_at, sum_compound, colour = screen_name)
) +
  geom_line(lwd = 1.5) +
  geom_smooth(method = "lm", na.rm = TRUE, se = FALSE, linetype = "dashed") +
  facet_wrap( ~ screen_name, scales = "free_x", ncol = 2) +
  theme_bw() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_date(date_breaks = "3 day", date_labels = "%m-%d", expand = c(0, 0)) +
  labs(x = "Total Sentiment", y = "Date", caption = "Figure 4.2: Sentiment Over Time by Media Agency") +
  theme_bw() +
  theme(
    axis.text.x = element_text(
      angle = 90,
      vjust = 0.5,
      hjust = 1
    ),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.background = element_blank(),
    legend.title = element_blank(),
    legend.text = element_blank(),
    legend.key = element_blank(),
    legend.justification = c(-2,-2),
    plot.caption = element_text(hjust=0.5, size=rel(1.2))
  )
```

\newpage

# 5. Interactions

The plot below maps the average total interactions by day. From the relevant peaks, we extract the top tweet in that day. 

```{r Interactions, eval=FALSE, fig.height=12, fig.width=20, warning=FALSE, include=FALSE}
media_vader_df <- media_vader_df %>%
  mutate(created_at = as.Date(created_at)) %>%
  group_by(created_at) %>%
  mutate(
    retweet_daily = mean(retweet_count),
    favorite_daily = mean(favorite_count),
    total = mean(favorite_count + retweet_count)
  )
# mean for the day
interactions_plot <- ggplotly(
  ggplot(data = media_vader_df, aes(created_at)) +
    geom_ribbon(aes(
      ymin = 0, ymax = total, fill = "Total"
    )) +
    geom_ribbon(aes(
      ymin = 0, ymax = favorite_daily, fill = "Favorites"
    )) +
    geom_ribbon(aes(
      ymin = 0, ymax = retweet_daily, fill = "Retweets"
    )) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      axis.ticks = element_blank(),
      
      legend.justification = c(0, 0),
      legend.position = c(0, 0),
      legend.background = element_blank(),
      legend.key = element_blank(),
      legend.title = element_blank(),
      
      plot.title = element_text(
        size = 14,
        face = "bold",
        margin = margin(0, 0, 10, 0),
        hjust = 0
      ),
      plot.caption = element_text(face = "bold", hjust = 0),
    )
  ,
  tooltip = c("total", "favorite_daily", "retweet_daily", "created_at")
)

# peak tweets by day
# media_vader_df %>%
#   filter(created_at == "2021-05-01" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-05-23" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-05-29" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-02" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-06" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-13" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-16" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-06-26" ) %>%
#   slice_max(favorite_count + retweet_count)
# media_vader_df %>%
#   filter(created_at == "2021-07-03" ) %>%
#   slice_max(favorite_count + retweet_count)

# media_vader_df %>%
#   group_by(screen_name) %>%
#   slice_max(favorite_count + retweet_count)
```

```{r Interactions image, echo=FALSE, fig.height=12, fig.width=20, warning=FALSE}
pp <- readPNG("images/interactions_plot.png")
plot.new() 
rasterImage(pp,0,0,1,1)
```

## 5.1. Peak Tweets by User   

*@eNCA:* Call for ministers over 60 to resign.

*@dailymaverick:* SCORPIO Floyd Shivambu’s brother quietly pays back R4.55m, admits he received the VBS money gratuitously.

*@TimesLIVE:*	Do you approve of Duduzane running for president?

*@News24:* Coca-Cola lost $4 billion in market value after Cristiano Ronaldo suggested people drink water instead.

*@SABCNews:* BREAKING NEWS: King of Eswatini has fled amid public violence in the country.

\newpage

# 6. Topic Modelling

A topic model is a type of statistic model for discovering the abstract "topics" that occur in a collection of documents.
We determine the best range k-value for LDA (Latent Dirichlet Allocation). The importance of the value of 'k' is to ensure you do not over estimate or underestimate the data. If you over estimate it, you will be left with topics that carry very little meaning and if you under estimate the data, you will lose out on topics that could have been useful to your research.

By looking at the lowest minimum and the highest maximum, we can determine the 'k'. This is how we have interpreted the graphs produced. Griffiths2004 is not informative in this situation and is therefore ignored.

This method used has been proved to produce the best results of LDA without subjectively tuning the 'k' value. Our result is 12 topics (Figure 6.1).

## 6.1. Gap k justification

```{r tidy matrix, warning = FALSE, include = FALSE}
tidy_matrix <-
tidy_media_df %>% count(screen_name, word) %>% cast_dfm(screen_name, word, n)
```


```{r Gap k justify, eval=FALSE, warning=FALSE, include=FALSE}
data("AssociatedPress", package = "topicmodels")
dtm <- AssociatedPress[1:10, ]

result <- FindTopicsNumber(
  tidy_matrix,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(result)
```


```{r Gap k justify image, echo=FALSE, warning=FALSE}
pp <- readPNG("images/gap_k_plot.png")
plot.new() 
rasterImage(pp,0,0,1,1)
```
\newpage

## 6.2. Topics found

### 6.2.1. Beta

Our main topic influence is COVID-19, as this is what currently affects the country the most. Our words are mapped to a beta which shows the amount the word appears per topic. 

```{r Topic Modelling Beta, echo=FALSE, warning=FALSE, fig.height=12, fig.width=15}
media_lda <- LDA(tidy_matrix, k = 12, control = list(seed = 1234))


media_topics_beta <- tidy(media_lda, matrix = "beta")

media_top_terms <- media_topics_beta %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

media_top_terms <- media_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  pivot_wider(id_cols = term,
              names_from = topic,
              values_from = beta) %>%
    rename(
      "News24 - Level 4 Lockdown" = 2,
      "eNCA - Watch President Ramaphosa Speak" = 3,
      "Time Live - The Publics News" = 4,
      "eNCA - Health Covid News" = 5,
      "News24 - Cape Town, Northern & Eastern Cape, Police Updates" = 6,
      "Daily Maverick - South African Covid Vaccine" = 7,
      "News24 - Gauteng Covid Increase News" = 8,
      "SABC News - Former President Zuma in Court" = 9,
      "SABC News - President Rhamaphosa" = 10,
      "Daily Maverick - World Covid News" = 11,
      "Times Live - National Covid News" = 12,
      "Times Live - Sunday President Speach on Lockdown" = 13
    ) %>%
    pivot_longer(
      cols = c(-1),
      names_to = "topic",
      values_to = "beta"
    ) %>%
    drop_na()

ggplot(media_top_terms, aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ topic, scales = "free_y", ncol = 2) +
  scale_y_reordered() +
  labs(x = "Beta", y = "Term", caption = "Figure 6.2: Modeled Topics (All Tweets)") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1.2))) 
```

\pagebreak

### 6.2.2. Gamma

The gamma is added to the topics modeled. We can now see which agencies are strongly associated with each topic. The higher the gamma, the more strongly the specific agency associates with the topic.

```{r Topic Modelling Gamma, echo=FALSE, fig.height=7, fig.width=10, message=FALSE, warning=FALSE}
media_topics_gamma <-
  tidy(media_lda,
       matrix = "gamma" ,
       document_names = rownames(screen_name))

ggplot(media_topics_gamma, aes(gamma, fill = as.factor(document))) +
  geom_histogram(alpha = 0.8) +
  facet_wrap( ~ topic, ncol = 3) +
  labs(
    title = "Distribution of document probabilities for each topic",
    subtitle = "Each topic is associated with 1-5 Media Agencies",
    y = "Number of topics",
    x = expression(gamma)
  ) +
  theme_bw() +
  scale_fill_discrete(name = "Screen Names")
```


# 7. Additional Requirements

We choose Reddit as our other data source. Facebook was considered but API needing proof of identity with an ID document seemed excessive. The 'ReddicommentractoR' package is used to extract comment and post data. Search term 'Covid-19' is used. Other terms were not used as their post dates went further than 6 months. We follow most of the same analysis for Reddit data as Twitter.

```{r Fetch post and comments, eval=FALSE, message=FALSE, include=FALSE}
reddit_comments_df <-
  get_reddit(
    search_terms = "Covid-19",
    subreddit = "southafrica",
    cn_threshold = 3,
    page_threshold = 10,
    sort_by = "new",
    wait_time = 2
  )

write_csv(reddit_comments_df, "data_in/reddit_comments.csv")
```

## 7.1. Reddit Comparing Comments From r/southafrica Covid-19 Posts.

We clean the data as we did before. Controversial and foul language is left in to not affect sentiment. The worst language is usually moderated within the Reddit communities before it is seen by the public.

```{r Reddit Clean up, message=FALSE, warning=FALSE, include=FALSE}
reddit_comments_df <- read_csv("data_in/reddit_comments.csv")
reddit_comments_df <-
  reddit_comments_df[, colSums(is.na(reddit_comments_df)) < nrow(reddit_comments_df)] %>% 
  mutate(comm_date = dmy(comm_date), post_date = dmy(post_date))

reddit_comments_df_clean <- reddit_comments_df %>%
  mutate(
    comment = tolower(comment), 
    comment = str_replace_all(comment, "-", " "),
    comment = str_replace_all(comment, "'", ""),
    comment = str_replace_all(comment, " ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " "),
    comment = str_replace_all(comment, " ", " "),
    comment = str_replace_all(comment, "#[a-z,A-Z]*", " "),
    comment = str_replace_all(comment, "@[a-z,A-Z]*", " "),
  ) %>% 
  mutate(
    comment = rm_twitter_n_url(comment),
    comment = gsub("&amp", "", comment),
    comment = gsub("J&amp;J", " ", comment),
    comment = gsub("@\\w+", " ", comment),
    comment = gsub("[[:punct:]]", "", comment),
    comment = gsub("[[:digit:]]", " ", comment),
    comment = gsub("http\\w+", " ", comment),
    comment = gsub("[ |\t]{2,}", " ", comment),
    comment = gsub("^ ", "", comment),
    comment = gsub(" $", "", comment),
  )
```


```{r Reddit bigrams, fig.height=20, message=FALSE, warning=FALSE, include=FALSE}
#bigrams
tidy_bigram_df <- reddit_comments_df_clean %>%
  unnest_tokens(bigram, comment, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- tidy_bigram_df %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

#tf-idf bigrams
bigram_tf_idf <- bigrams_united %>%
  count(title, bigram) %>%
  bind_tf_idf(bigram, title, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf %>%
  group_by(title) %>%
  slice_max(tf_idf, n = 1) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ title, ncol = 3, scales = "free_y") +
  labs(x = "tf-idf", y = NULL, caption = "Figure 7.1: Reddit Bigrams") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1.2))) 

# weights and graphs
# bigram_graph <- bigram_counts %>%
#   filter(n > 100) %>%
#   graph_from_data_frame()
# 
# set.seed(1234)
# a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
# ggraph(bigram_graph, layout = "fr") +
#   geom_edge_link(
#     aes(edge_alpha = n),
#     show.legend = FALSE,
#     arrow = a,
#     end_cap = circle(.07, 'inches')
#   ) +
#   geom_node_point(color = "lightblue", size = 5) +
#   geom_node_comment(aes(label = name),
#                  vjust = 1,
#                  hjust = 1,
#                  repel = TRUE) +
#   theme_void()
```


```{r Reddit negation words, echo=FALSE, warning=FALSE}
#vader lexicon imported from VADER GitHub
vader_lexicon <- read_csv2("data_in/vader_lexicon.csv") %>%
  rename("word" = TOKEN, "value" = `MEAN-SENTIMENT-RATING`)

#common negation words
negation_words <-
  c(
    "not",
    "no",
    "never",
    "without",
    "no",
    "not",
    "none",
    "no one",
    "nobody",
    "nothing",
    "neither",
    "nowhere",
    "never",
    "doesn’t",
    "isn’t",
    "wasn’t",
    "shouldn’t",
    "wouldn’t",
    "couldn’t",
    "won’t",
    "can’t",
    "don’t"
  )
negated_words <- tidy_bigram_df %>%
  filter(word1 %in% fixed(negation_words, ignore_case = TRUE)) %>%
  inner_join(vader_lexicon, by = c(word2 = "word")) %>%
  mutate(value = as.double(value)) %>%
  count(word1, word2, value, sort = TRUE) %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  mutate(word2 = reorder(word2, contribution))
negated_words %>%
  head(40) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ word1, scales = "free_y") +
  labs(x = "Sentiment value * # number of occurrences",
       y = "Words preceded by negation terms",
       caption = "Reddit Negation Words") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1.2))) 

```


```{r Reddit Tidy, warning=FALSE, include=FALSE}
#tidy df and unnest
tidy_reddit_df <- reddit_comments_df_clean %>%
  unnest_tokens(word, comment, token = "words") %>%
  filter(
    !word %in% stop_words$word,
    !word %in% negated_words$word2, 
    !word %in% "pm", 
    str_detect(word, "[a-z]")
  ) %>% 
  mutate(post_date = as.Date(post_date))

```

From our tidied Reddit dataset (Figure 7.2), we look for the top words that appear. This will give us a good idea of what topics are being discussed the most. We find that with COVID-19 comments being pulled, the comments discuss people's general interaction with the virus. The comments come from regular people and this shows what topics are discussed among people.

```{r Reddit top words, echo=FALSE, warning=FALSE}
#top words
top_words <- tidy_reddit_df %>%
  anti_join(stop_words, by = 'word') %>%
  count(word) %>%
  arrange(desc(n))
top_words %>%
  slice(1:20) %>%
  ggplot(aes(reorder(word,-n), n, fill = word)) +
  geom_bar(stat = "identity")  +
  theme_bw() +
  theme(
    axis.text.x = element_text(
      angle = 60,
      hjust = 1,
      size = 13
    ),
    plot.caption = element_text(hjust=0.5, size=rel(1.2))
  ) +
  labs(x = "tf-idf", y = "Frequency", caption = "Figure 7.2: Most Frequent Reddit Comment Words") +
  guides(fill = FALSE)
```

```{r Reddit Unique words, message=FALSE, warning=FALSE, include=FALSE}
comment_words <- reddit_comments_df_clean %>%
  unnest_tokens(word, comment) %>%
  count(title, word, sort = TRUE)

total_words <- comment_words %>% 
  group_by(title) %>% 
  summarize(total = sum(n))

comment_words <- left_join(comment_words, total_words)

ggplot(comment_words, aes(n/total, fill = title)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~title, ncol = 2, scales = "free_y") +
  labs(x = "Word Count", y = "Term Frequency (n/total)", caption = "Figure 3.5: Distribution of Term Frequency") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1.2)))
```

\pagebreak

Our interpretation of Zipf's Law (Figure 7.3) on Reddit comments show that commenters use a lot of text slang, as they do not follow the broken power law of regular language. 

```{r Reddit Zipfs law, echo=FALSE, message=FALSE, warning=FALSE}
freq_by_rank <- comment_words %>% 
  group_by(title) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup()

# rank_subset <- freq_by_rank %>%
#   filter(rank < 500,
#          rank > 10)
# lm(log10(`term frequency`) ~   log10(rank), data = rank_subset)
#
# Call:
# lm(formula = log10(`term frequency`) ~ log10(rank), data = rank_subset)
# 
# Coefficients:
# (Intercept)  log10(rank)  
#     -0.7917      -0.8907  


freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = title)) + 
  geom_abline(intercept = -0.7917, slope = -0.8907, 
              color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "Rank", y = "Term Frequency", caption = "Figure 7.3: Reddit Zipf's Law with a Broken Power Law") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1.2)))
```

```{r Reddit tf-idf, echo=FALSE, fig.height=20, fig.width=15, message=FALSE, warning=FALSE}
#tf-idf by user
tidy_reddit_tf_idf <- tidy_reddit_df %>%
  count(word, title) %>%
  bind_tf_idf(word, title, n) %>% 
    group_by(title) %>%
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, tf_idf)) 

tidy_reddit_tf_idf %>%
  group_by(title) %>%
  slice_max(tf_idf, n = 5) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ title, ncol = 2, scales = "free_y") +
  labs(x = "tf-idf", y = NULL, caption = "Figure 7.3: Highest tf-idf from Title") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1.2))) 
```


```{r Reddit tf-idf week, echo=FALSE, warning=FALSE, fig.height = 9, fig.width = 13}
#tf-idf week
tidy_reddit_tf_idf_date <- tidy_reddit_df %>%
  select(post_date, title, word) %>%
  mutate(post_date = as.Date(post_date)) %>% 
  group_by(post_date, title) %>%
  count(word, post_date) %>%
  bind_tf_idf(word, post_date, n)

tidy_reddit_tf_idf_date %>%
  mutate(week = week(as.Date(post_date))) %>% 
  group_by(week) %>%
  slice_max(tf_idf, n = 7) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title)) +
  geom_col(show.legend = TRUE) + 
  facet_wrap(~ week, ncol = 2, scales = "free") +
  scale_fill_discrete(name = "Screen Name") +
  labs(x = "tf-idf", y = NULL, caption = "Figure 7.4: Highest tf-idf by Title in Each Week") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1.2))) 
```

The graph below shows which words are most likely to co-occur, or not. We use this to understand the context of words in our topic modeling. The words shown are popular words in our dataset. 

```{r Reddit Counting and correlating among sections, echo=FALSE, fig.width=12, message=FALSE, warning=FALSE}
reddit_section_words <- tidy_reddit_df %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0)

# count words co-occuring within sections
word_pairs <- reddit_section_words %>%
  pairwise_count(word, section, sort = TRUE)

# we need to filter for at least relatively common words first
word_cors <- reddit_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

# Correlation of next word
# word_cors %>%
#   filter(item1 == "vaccine")

word_cors %>%
  filter(item1 %in% c("covid", "vaccine", "lockdown", "gauteng", "ramaphosa", "police")) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation, fill = item1)) +
  geom_bar(stat = "identity") +
  facet_wrap( ~ item1, scales = "free") +
  coord_flip() +
  labs(x = "Correlation Strength", y = "Preceding Word", caption = "Figure 7.5: Words that Precede Popular Terms") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1.2)),
        legend.justification = c(-10,-20)
        )

# modeled correlation map but not used.
# set.seed(1234)
# word_cors %>%
#   filter(correlation > .65) %>%
#   graph_from_data_frame() %>%
#   ggraph(layout = "fr") +
#   geom_edge_link(aes(edge_alpha = correlation),
#                  show.legend = FALSE,
#                  edge_width = 3) +
#   geom_node_point(color = "lightblue", size = 5) +
#   geom_node_comment(aes(label = name), repel = TRUE) +
#   theme_void()
```

\pagebreak

# 8. Reddit Sentiment Analysis

```{r Reddit VADER write, eval=FALSE, warning=FALSE, include=FALSE}
vader_df <- vader_df(reddit_comments_df_clean$comment)
write_as_csv(vader_df, "data_in/vader_reddit_comments")
```


```{r Reddit VADER setup, warning=FALSE, include=FALSE}
vader_df <- read_csv("data_in/vader_reddit_comments.csv")
vader_df <- vader_df %>% mutate("X1" = row_number())
reddit_comments_df_clean <- reddit_comments_df_clean %>% mutate("X1" = row_number())
reddit_vader_df <- reddit_comments_df_clean %>% left_join(vader_df, by = "X1") %>%
  mutate(compound = replace(compound, is.na(compound), 0))

```

## 8.1. Sentiment over time

Reddit sentiment is sporadic. Everyone has their own opinion on a post title whether it is a positive or negative title.

```{r Reddit VADER over time, echo=FALSE, fig.width=15, warning=FALSE}
#Sentiment over time
ggplot(
  reddit_vader_df %>%
    mutate(post_date = as.Date(post_date)) %>%
    group_by(post_date) %>%
    summarise(sum_compound = sum(as.double(compound))),
  aes(post_date, sum_compound)
) +
  geom_line(lwd = 1.5, colour = "red") +
  geom_smooth(method = "lm", formula = "y ~ x",na.rm = TRUE, se = FALSE, linetype = "dashed") +
  theme_bw() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_date(date_breaks = "3 day", date_labels = "%m-%d", expand = c(0, 0)) +
  labs(x = "Date", y = "Total Sentiment", caption = "Figure 8.1: Sentiment Over Time by Comments") +
  theme_bw() +
  theme(
    axis.text.x = element_text(
      angle = 90,
      vjust = 0.5,
      hjust = 1
    ),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.background = element_blank(),
    legend.title = element_blank(),
    legend.text = element_blank(),
    legend.key = element_blank(),
    legend.justification = c(-2,-2),
    plot.caption = element_text(hjust=0.5, size=rel(1.2))
  )
```

### 8.1.1. Most Negative Comment

Lambpanties: it does but sheesh the poor people my dad rents property to a restuarant owner and the poor guy hasnt been able to pay his rent about or different months now one month my parents even pitched in to pay his staffs wages there is no way the poor guy is not going to go under from this cherry from hell on top he has covid right now to boot	

```{r Reddit VADER most negative, eval=FALSE, include=FALSE}
head(reddit_vader_df %>% select(comment, compound) %>% 
  arrange(compound), 1)
```

### 8.1.2. Most Positive Comment

babufrikhasaposse: but arts is a strong driver of an improved society you dont want to live in a society without arts and demanding other people meet a standard you set while pretending they dont contribute positively to society is a bit absurd not everyone is interested in science and the aim of education isnt economic value exclusively even if everything you said is true that doesnt make the comment i replied to mot just an asshole thing to say

```{r Reddit VADER most positive, eval=FALSE, include=FALSE}
head(reddit_vader_df %>% select(comment, compound) %>% 
  arrange(desc(compound)),1)
```

# 9. Reddit Interactions

### 9.1. Highest Upvoted Comment from each Post

```{r Reddit Interactions Highest, echo=FALSE, warning=FALSE}
# reddit_vader_df <- reddit_vader_df[!(reddit_vader_df$comment=="removed" | reddit_vader_df$title=="[deleted]"),]
# reddit_comments_df %>% 
#   group_by(title) %>% 
#   slice_max(order_by = comment_score) %>% 
#   select(title, user, comment, comment_score) %>% 
#   mutate(comment = str_replace_all(comment, "", "")) %>% 
#   mutate(comment = str_replace_all(comment, "=", "")) %>% 
#   mutate(comment = str_replace_all(comment, "", "")) %>% 
#   mutate(title = str_replace_all(title, "", "")) %>% 
#   mutate(comment = str_replace_all(comment, "removed", "")) %>% 
#   mutate(user = str_replace_all(user, "[deleted]", "")) %>% 
#   filter(comment_score > 10) %>% 
#   gt() %>% 
#   tab_options(table.layout = "auto")

pp <- readPNG("images/reddit_interactions_tbl.png")
plot.new() 
rasterImage(pp,0,0,1,1)
```

\pagebreak

### 9.2. Controversial Comments

Usually met with less comment score.

```{r Reddit Interactions Controversial, echo=FALSE, warning=FALSE}
# reddit_vader_df %>% 
#   group_by(title) %>% 
#   select(title, user, comment, comment_score, controversiality) %>%
#   filter(controversiality == 1) %>%
#   mutate(comment = str_replace_all(comment, "", "")) %>% 
#   mutate(comment = str_replace_all(comment, "=", "")) %>% 
#   mutate(comment = str_replace_all(comment, "removed", "N/A")) %>% 
#   gt() %>% 
#   tab_options(table.layout = "auto") 
pp <- readPNG("images/reddit_controversial_tbl.png")
plot.new() 
rasterImage(pp,0,0,1,1)
```
  
\pagebreak

# 10. Reddit Topic Modelling

## 10.1. Gap k justification

Our result is 9-14 topics. From this we found 12 topics to be accurate.

```{r Reddit tidy matrix, message=FALSE, warning=FALSE, include=FALSE}
tidy_matrix <-
tidy_reddit_df %>% count(title, word) %>% cast_dfm(title, word, n)
```


```{r Reddit Gap k justify, echo=FALSE, message=FALSE, warning=FALSE}
data("AssociatedPress", package = "topicmodels")
dtm <- AssociatedPress[1:10, ]

result <- FindTopicsNumber(
  tidy_matrix,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(result)
```

\newpage

## 10.2. Topics found

Topics of discussion in r/southafrica in the past 6 months.

```{r Reddit Topic Modelling Beta, echo=FALSE, warning=FALSE, fig.height=12, fig.width=15}
reddit_lda <- LDA(tidy_matrix, k = 12, control = list(seed = 1234))


reddit_topics_beta <- tidy(reddit_lda, matrix = "beta")

reddit_top_terms <- reddit_topics_beta %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>%
  ungroup() %>%
  arrange(topic, -beta)

reddit_top_terms <- reddit_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  pivot_wider(id_cols = term,
              names_from = topic,
              values_from = beta) %>%
    rename(
      "Russian Vaccine Data" = 2,
      "Religious People on Vaccination" = 3,
      "Lockdown Discussions" = 4,
      "Covid Deaths" = 5,
      "Time Based Discussion in Lockdown" = 6,
      "Vaccine Discussions" = 7,
      "Sale of Alcohol and the Truama it Causes" = 8,
      "Zuma's Prison Time " = 9,
      "Gauteng in Need of More Hospital Equipment" = 10,
      "People's Interaction with Covid Laws" = 11,
      "People Wearing Masks" = 12,
      "Covid Reported Death Data" = 13
    ) %>%
    pivot_longer(
      cols = c(-1),
      names_to = "topic",
      values_to = "beta"
    ) %>%
    drop_na()

ggplot(reddit_top_terms, aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ topic, scales = "free_y", ncol = 2) +
  scale_y_reordered() +
  labs(x = "Beta", y = "Term", caption = "Figure 9.1: Modeled Topics (All Tweets)") +
    theme_bw() +
    theme(plot.caption = element_text(hjust=0.5, size=rel(1.2)))
```
